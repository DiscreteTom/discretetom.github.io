(window.webpackJsonp=window.webpackJsonp||[]).push([[124],{1157:function(t,e){const n={render:function(){var t=this;t.$createElement;return t._self._c,t._m(0)},staticRenderFns:[function(){var t=this,e=t.$createElement,n=t._self._c||e;return n("div",{staticClass:"frontmatter-markdown"},[n("h2",{attrs:{id:"前言"}},[t._v("前言")]),t._v(" "),n("p",[t._v("本文是观看"),n("a",{attrs:{href:"https://www.bilibili.com/video/BV11A411L7CK"}},[t._v("此视频")]),t._v("时的笔记")]),t._v(" "),n("h2",{attrs:{id:"spark概述"}},[t._v("Spark概述")]),t._v(" "),n("ul",[n("li",[n("strong",[t._v("基于内存")]),t._v("、快速、通用、可扩展的大数据分析计算引擎")]),t._v(" "),n("li",[t._v("使用Scala语言开发，"),n("strong",[t._v("支持流处理")])]),t._v(" "),n("li",[t._v("解决MapReduce处理慢的问题，被认为是MapReduce框架的升级版\n"),n("ul",[n("li",[t._v("MapReduce关注一次性数据计算。读数据-计算-写数据，只提供了map-reduce两种操作，所以复杂的业务逻辑需要手动串联map-reduce操作。而单次map-reduce会产生大量的磁盘IO，导致效率低下")]),t._v(" "),n("li",[t._v("Spark提供了更多的计算模型，且基于内存，不同任务串联时使用内存保存数据，所以更方便更快")]),t._v(" "),n("li",[t._v("如果生产环境内存不足，那么还是要用MapReduce基于硬盘做数据处理")]),t._v(" "),n("li",[t._v("总结：Spark和MapReduce的根本差异是"),n("strong",[t._v("多个作业之间的数据通信方式")]),t._v("。Spark基于内存，MapReduce基于磁盘")])])]),t._v(" "),n("li",[t._v("模块\n"),n("ul",[n("li",[t._v("核心模块:\n"),n("ul",[n("li",[t._v("Spark Core\n"),n("ul",[n("li",[t._v("最基础、最核心的功能。高层模块都是基于Spark Core实现的")])])])])]),t._v(" "),n("li",[t._v("高层模块：\n"),n("ul",[n("li",[t._v("Spark SQL\n"),n("ul",[n("li",[t._v("操作结构化数据")]),t._v(" "),n("li",[t._v("使用SQL或者HQL（Hive版本的SQL）")])])]),t._v(" "),n("li",[t._v("Spark Streaming\n"),n("ul",[n("li",[t._v("操作流式数据")])])]),t._v(" "),n("li",[t._v("Spark MLlib\n"),n("ul",[n("li",[t._v("机器学习")]),t._v(" "),n("li",[t._v("提供算法库、模型评估、数据导入等功能")])])]),t._v(" "),n("li",[t._v("Spark GraphX\n"),n("ul",[n("li",[t._v("图形挖掘计算")])])])])])])])]),t._v(" "),n("h2",{attrs:{id:"spark-core"}},[t._v("Spark Core")]),t._v(" "),n("h3",{attrs:{id:"spark环境和运行模式"}},[t._v("Spark环境和运行模式")]),t._v(" "),n("ul",[n("li",[t._v("Spark框架是一个运行的环境\n"),n("ul",[n("li",[t._v("就像我们连接到MySQL然后执行SQL命令一样，我们需要连接到Spark环境，然后调用Spark API")]),t._v(" "),n("li",[t._v("Spark目前提供了Scala/Java/Python的SDK")]),t._v(" "),n("li",[t._v("代码逻辑：\n"),n("ul",[n("li",[t._v("建立连接")]),t._v(" "),n("li",[t._v("提交/执行任务")]),t._v(" "),n("li",[t._v("断开连接")])])])])]),t._v(" "),n("li",[t._v("Spark被设计为可以在所有常见集群环境中执行\n"),n("ul",[n("li",[t._v("比如本地、容器、Yarn等。国内主流环境为Yarn，容器正在逐渐变得流行")]),t._v(" "),n("li",[t._v("本地模式(Local)\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("/bin/spark-shell")]),t._v("可以启动一个scala shell，在里面可以写scala代码操作spark\n"),n("ul",[n("li",[t._v("启动shell的时候会在本地启动一个spark服务端")]),t._v(" "),n("li",[t._v("shell里面已经存在一个sc对象(SparkContext)和一个spark对象（Spark会话）")]),t._v(" "),n("li",[t._v("可以使用4040端口访问spark web管理界面")])])]),t._v(" "),n("li",[t._v("也可以直接在用SDK写代码的时候把"),n("code",{pre:!0},[t._v("master")]),t._v("设置为"),n("code",{pre:!0},[t._v("local")]),t._v("或"),n("code",{pre:!0},[t._v("local[*]")])])])]),t._v(" "),n("li",[t._v("Standalone模式（独立部署模式）\n"),n("ul",[n("li",[t._v("master/slave模式。比如3台机器，一台master负责管理，两台slave。3台机器都有worker可以跑任务")]),t._v(" "),n("li",[t._v("和MR一样，Spark也可以配置历史任务服务器(event log)。略")]),t._v(" "),n("li",[t._v("可以配置高可用，实现主备master。通常使用zookeeper配置")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("master")]),t._v("中设置多个endpoint")])])]),t._v(" "),n("li",[t._v("Yarn模式\n"),n("ul",[n("li",[t._v("Spark主要负责计算而不是资源调度。可以使用Yarn实现资源调度")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("master")]),t._v("需要设置为"),n("code",{pre:!0},[t._v("yarn")])]),t._v(" "),n("li",[t._v("需要启动YARN/HDFS")])])]),t._v(" "),n("li",[t._v("容器模式，略")])])])]),t._v(" "),n("h3",{attrs:{id:"端口号"}},[t._v("端口号")]),t._v(" "),n("ul",[n("li",[t._v("Spark Local: 4040")]),t._v(" "),n("li",[t._v("Spark Master内部通信：7077")]),t._v(" "),n("li",[t._v("Standalone模式下SparkMaster:8080")]),t._v(" "),n("li",[t._v("历史服务：18080")]),t._v(" "),n("li",[t._v("使用YARN查看任务：8088")])]),t._v(" "),n("h3",{attrs:{id:"sparkcore核心概念"}},[t._v("SparkCore核心概念")]),t._v(" "),n("ul",[n("li",[t._v("Executor\n"),n("ul",[n("li",[t._v("是worker中的一个JVM进程")]),t._v(" "),n("li",[t._v("提交app的时候可以提供参数指定executor数量、每个executor的内存大小、每个executor的CPU数量")])])]),t._v(" "),n("li",[t._v("并行度\n"),n("ul",[n("li",[t._v("整个集群并行执行任务的数量")]),t._v(" "),n("li",[t._v("并行和并发的区别：并行(parallelism)使用的是独立的CPU核心，并发(concurrency)可以复用单个CPU核心")])])]),t._v(" "),n("li",[t._v("有向无环图(DAG)")])]),t._v(" "),n("blockquote",[n("p",[t._v("大数据计算引擎的迭代：第一代是MapReduce，仅提供map和reduce两种阶段，那么上层应用就必须把算法拆分为map/reduce，且需要多个job串联。难以迭代计算。")]),t._v(" "),n("p",[t._v("因为这个弊端，催生了DAG框架，被视为第二代计算引擎。比如Tez和更上层的Oozie。但是它们仍然只能执行批处理任务。")]),t._v(" "),n("p",[t._v("接下来就是以Spark为代表的第三代计算引擎，主要特点是job内部支持DAG(DAG不会跨job)，以及支持准实时计算。")])]),t._v(" "),n("h3",{attrs:{id:"sparkcore架构"}},[t._v("SparkCore架构")]),t._v(" "),n("ul",[n("li",[t._v("使用master-slave架构")]),t._v(" "),n("li",[t._v("客户端有一个driver program，里面使用spark context连接欸到spark集群的cluster manager\n"),n("ul",[n("li",[t._v("driver负责\n"),n("ul",[n("li",[t._v("把用户程序转换为job")]),t._v(" "),n("li",[t._v("在executor之间调度task")]),t._v(" "),n("li",[t._v("跟踪executor的执行情况")])])])])]),t._v(" "),n("li",[t._v("每个worker node上面都有一个cache和一个executor")]),t._v(" "),n("li",[t._v("executor负责执行多个task，并把结果返回给driver\n"),n("ul",[n("li",[t._v("executor自带block manager，把RDD缓存在executor进程的内存中实现加速计算")])])]),t._v(" "),n("li",[t._v("如果使用了Yarn进行调度，driver会请求Yarn Application Master，AM会请求Spark Master")])]),t._v(" "),n("h3",{attrs:{id:"核心编程"}},[t._v("核心编程")]),t._v(" "),n("h4",{attrs:{id:"流程"}},[t._v("流程")]),t._v(" "),n("p",[t._v("以使用Yarn调度为例")]),t._v(" "),n("ol",[n("li",[t._v("启动Yarn环境（ResourceManager, NodeManager, ApplicationManager）")]),t._v(" "),n("li",[t._v("Spark通过申请资源来创建调度节点和计算节点\n"),n("ol",[n("li",[t._v("有些Node上面跑Driver(调度节点)")]),t._v(" "),n("li",[t._v("有些Node上面跑Executor(计算节点)")])])]),t._v(" "),n("li",[t._v("Driver根据需求，把计算逻辑根据分区划分成不同的任务，放到任务池(Task Pool)中")]),t._v(" "),n("li",[t._v("调度节点根据计算节点的状态，从任务池中取任务，发送到计算节点进行计算")])]),t._v(" "),n("h4",{attrs:{id:"数据结构"}},[t._v("数据结构")]),t._v(" "),n("p",[t._v("为了实现高并发/高吞吐的数据处理，Spark封装了三大数据结构：")]),t._v(" "),n("ul",[n("li",[t._v("RDD(Resilient Distributed Dataset) - 弹性分布式数据集\n"),n("ul",[n("li",[t._v("是Spark中最基础的数据处理模型")]),t._v(" "),n("li",[t._v("封装计算逻辑，并不保存数据")]),t._v(" "),n("li",[t._v("是一个抽象类")]),t._v(" "),n("li",[t._v("弹性，不可变，可分区，内部元素可以并行计算的集合\n"),n("ul",[n("li",[t._v("存储的弹性：内存/磁盘自动切换")]),t._v(" "),n("li",[t._v("容错的弹性：数据丢失可以自动恢复")]),t._v(" "),n("li",[t._v("计算的弹性：出错重试")]),t._v(" "),n("li",[t._v("分片的弹性：根据需求重新分片，以便Executor被充分利用")]),t._v(" "),n("li",[t._v("不可变：对RDD的操作只能生成新的RDD，不会修改原RDD")])])]),t._v(" "),n("li",[t._v("可以被拆解为Task之后被序列化，发送给Executor。Task中包含数据和操作")]),t._v(" "),n("li",[t._v("通过对RDD操作得到新的RDD，其实是在使用装饰者设计模式，给原RDD添加功能。直到执行collect，才会真正执行任务，之前都是在定义操作逻辑")]),t._v(" "),n("li",[t._v("执行操作的时候，为了可以把任务发到不同的Executor，RDD会对数据进行分区")]),t._v(" "),n("li",[t._v("五大核心属性\n"),n("ul",[n("li",[t._v("分区列表(a list of partitions)")]),t._v(" "),n("li",[t._v("分区计算函数(a function for computing each split)")]),t._v(" "),n("li",[t._v("RDD之间的依赖关系(a list of dependencies on other RDDs)")]),t._v(" "),n("li",[t._v("分区器(optionally, a Partitioner for k-v RDDs)")]),t._v(" "),n("li",[t._v("计算每个分区的首选位置(optionally, a list of preferred locations to compute each split on)\n"),n("ul",[n("li",[t._v("尽量把计算任务放到保存文件的节点上，以免产生网络传输")])])])])]),t._v(" "),n("li",[t._v("创建方式\n"),n("ul",[n("li",[t._v("从内存创建，即利用已有的数组"),n("code",{pre:!0},[t._v("Seq")]),t._v("创建。\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("val rdd: RDD[Int] = sc.parallelize(seq)")])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("val rdd: RDD[Int] = sc.makeRDD(seq)")])]),t._v(" "),n("li",[t._v("可以使用参数"),n("code",{pre:!0},[t._v("numSlices")]),t._v("指定分区数量。默认分区数量从SparkConf中获取，如果没有，则等于totalCores（当前环境最大可用核数）。Spark会自动根据数据长度，尽可能地进行均匀分片")])])]),t._v(" "),n("li",[t._v("从外部存储创建（比如HDFS, HBase，本地文件系统）\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("val rdd: RDD[String] = sc.textFile(path)")]),t._v(" "),n("ul",[n("li",[t._v("以行为单位读取数据")]),t._v(" "),n("li",[t._v("path可以是单个文件也可以是目录，支持通配符")]),t._v(" "),n("li",[t._v("也可以是HDFS路径："),n("code",{pre:!0},[t._v("hdfs://xxx:xxx/xxx")])]),t._v(" "),n("li",[t._v("使用"),n("code",{pre:!0},[t._v("minPartitions")]),t._v("指定最小分区数量")]),t._v(" "),n("li",[t._v("读取HDFS的文件，底层使用的是Hadoop")])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("sc.wholeTextFiles(path)")]),t._v("以整个文件为单位读取文件")])])]),t._v(" "),n("li",[t._v("从其他RDD创建，即为已有的RDD添加操作")]),t._v(" "),n("li",[t._v("直接创建(new)，通常由Spark框架自身使用")])])]),t._v(" "),n("li",[t._v("两大类方法（算子）\n"),n("ul",[n("li",[t._v("转换\n"),n("ul",[n("li",[t._v("对RDD进行操作，生成新的RDD")]),t._v(" "),n("li",[t._v("比如："),n("code",{pre:!0},[t._v("map/flatMap/reduce/groupBy/reduceByKey")])]),t._v(" "),n("li",[t._v("分类\n"),n("ul",[n("li",[t._v("单Value类型，操作单个RDD\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("map")]),t._v("逐条转换数据")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("mapValues")]),t._v("仅转换k-v里面的v，不改变k")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("mapPartitions")]),t._v("逐个分区操作数据，性能比"),n("code",{pre:!0},[t._v("map")]),t._v("高\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("rdd.mapPartitions(_.map(_*2))")])]),t._v(" "),n("li",[t._v("会把整个分区的数据加载到内存")]),t._v(" "),n("li",[t._v("处理完的数据不会被释放掉")]),t._v(" "),n("li",[t._v("如果数据量大，可能会内存溢出。"),n("code",{pre:!0},[t._v("map")]),t._v("慢一些，但是不会溢出")])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("mapPartitionsWithIndex")]),t._v("逐个分区操作数据，并且可以得到分区的编号")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("flatMap")]),t._v("扁平映射")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("glom")]),t._v("把一个分区的数据转换为数组。分区不变")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("groupBy")]),t._v("分组\n"),n("ul",[n("li",[t._v("分组和分区没有必然的关系")]),t._v(" "),n("li",[t._v("数据会根据分组，被发送到不同的分区。这个过程称为shuffle")]),t._v(" "),n("li",[t._v("shuffle比较消耗性能。shuffle越多，性能越差。所以一个优化程序的思路就是通过改变算子，减少shuffle")])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("filter")]),t._v("过滤。分区数量不变，但是可能数据倾斜")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("sample")]),t._v("抽样")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("distinct")]),t._v("去重")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("coalesce")]),t._v("修改分区数量，提升效率\n"),n("ul",[n("li",[t._v("通常用来减少分区")]),t._v(" "),n("li",[t._v("如果目标分区数量大于原分区，需要把第二个参数shuffle设置为true")])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("repartition")]),t._v("扩大分区，提升效率。底层使用的就是"),n("code",{pre:!0},[t._v("coalesce")])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("sortBy")]),t._v("排序")])])]),t._v(" "),n("li",[t._v("双Value类型，操作两个RDD\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("intersection")]),t._v("交集")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("union")]),t._v("并集")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("subtract")]),t._v("差集")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("zip")]),t._v("拉链\n"),n("ul",[n("li",[t._v("合并相同index的值")]),t._v(" "),n("li",[t._v("要求两个RDD的分区数量一致、分区内元素数量一致")])])])])]),t._v(" "),n("li",[t._v("Key-Value类型，RDD的数据类型是有两个元素的tuple\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("partitionBy")]),t._v("根据传入的Partitioner进行分区。默认分区器是"),n("code",{pre:!0},[t._v("HashPartitioner")]),t._v(" "),n("ul",[n("li",[t._v("如果新的分区器和之前一样，则不会重新分区")])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("reduceByKey")]),t._v("把相同的key的数据进行聚合\n"),n("ul",[n("li",[t._v("如果某个key对应的元素只有一个，则结果中不会出现这个key。因为reduceByKey是成对运算的")]),t._v(" "),n("li",[t._v("执行速度比"),n("code",{pre:!0},[t._v("groupByKey")]),t._v("快，因为"),n("code",{pre:!0},[t._v("reduceByKey")]),t._v("会先在当前分区进行操作（预聚合），然后再落盘（因为shuffle），所以落盘数据量会少一些")])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("groupByKey")]),t._v("根据key进行分组\n"),n("ul",[n("li",[t._v("和"),n("code",{pre:!0},[t._v("groupBy")]),t._v("的区别："),n("code",{pre:!0},[t._v("groupBy")]),t._v("的值中会包含"),n("code",{pre:!0},[t._v("key")]),t._v("，而"),n("code",{pre:!0},[t._v("groupByKey")]),t._v("的值不会包含"),n("code",{pre:!0},[t._v("key")])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("groupByKey")]),t._v("会导致shuffle把数据重组。Spark在处理shuffle的时候会落盘以避免OOM，所以"),n("code",{pre:!0},[t._v("groupByKey")]),t._v("会比较慢")])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("aggregateByKey")]),t._v("分区内聚合，然后分区间聚合\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("reduceByKey")]),t._v("的分区内聚合函数，与分区间聚合函数相同")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("aggregateByKey")]),t._v("可以像"),n("code",{pre:!0},[t._v("reduceByKey")]),t._v("一样实现分区内聚合，然后再使用不同的函数分区间聚合")]),t._v(" "),n("li",[t._v("比如：求所有分区的最大值，然后求和")])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("foldByKey")]),t._v("对分区内和分区间使用相同的函数聚合\n"),n("ul",[n("li",[t._v("相比于"),n("code",{pre:!0},[t._v("reduceByKey")]),t._v("，"),n("code",{pre:!0},[t._v("foldByKey")]),t._v("可以设置初始值")])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("combineByKey")]),t._v("先对每个key对应的第一个元素进行转换，然后再进行分区内、分区间聚合")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("reduceByKey")]),t._v(" vs "),n("code",{pre:!0},[t._v("aggregateByKey")]),t._v(" vs "),n("code",{pre:!0},[t._v("foldByKey")]),t._v(" vs "),n("code",{pre:!0},[t._v("combineByKey")]),t._v(" "),n("ul",[n("li",[t._v("底层使用了相同的逻辑")]),t._v(" "),n("li",[t._v("初始值的取值不同、分区内和分区间的函数不同，导致上述函数的不同")])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("join")]),t._v("连接相同key的value\n"),n("ul",[n("li",[t._v("如果没有匹配的key，则结果中不会包含这个key")]),t._v(" "),n("li",[t._v("如果一个RDD中出现重复的key，则每个item都会join一次。这会导致笛卡尔乘积的存在，导致数据量暴增，导致OOM")])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("leftOuterJoin")]),t._v(" & "),n("code",{pre:!0},[t._v("rightOuterJoin")]),t._v("允许结果存在None")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("cogroup")]),t._v(" = group + connect\n"),n("ul",[n("li",[t._v("先分别把两个RDD中相同key的value进行group，然后把两个RDD相同的key对应的两个group给连接起来，变成"),n("code",{pre:!0},[t._v("(key, list, list)")])]),t._v(" "),n("li",[t._v("可以一次连接最多3个RDD")])])])])])])])])]),t._v(" "),n("li",[t._v("行动\n"),n("ul",[n("li",[t._v("触发作业的执行")]),t._v(" "),n("li",[t._v("底层调用"),n("code",{pre:!0},[t._v("sc.runJob")]),t._v("创建"),n("code",{pre:!0},[t._v("ActiveJob")]),t._v("并执行作业")]),t._v(" "),n("li",[t._v("包括\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("reduce")]),t._v("直接获得reduce的结果")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("collect")]),t._v("把数据按照分区顺序采集到driver的内存，得到数组")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("count")]),t._v("获取数据个数")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("first")]),t._v("获取第一个数据")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("take")]),t._v("取指定数量的数据")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("takeOrdered")]),t._v("排序，再取指定数量的数据\n"),n("ul",[n("li",[t._v("支持自定义排序逻辑。默认数值升序")])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("aggregate")]),t._v("数据进行分区内聚合（+初始值），然后进行分区间的聚合（再+初始值）\n"),n("ul",[n("li",[t._v("所以初始值一共被使用了【分区数量+1】次")])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("fold")]),t._v("类似"),n("code",{pre:!0},[t._v("aggregate")]),t._v("，只不过分区内和分区间使用相同的聚合逻辑")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("countByKey")]),t._v("根据Key，计算数量。RDD的数据需要是tuple list，根据k-v的Key计算")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("countByValue")]),t._v("根据值，计算数量。RDD的数据需要是一个list。不是根据k-v的value进行计算")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("saveAsTextFile")]),t._v(" & "),n("code",{pre:!0},[t._v("saveAsObjectFile")]),t._v(" & "),n("code",{pre:!0},[t._v("saveAsSequenceFile")]),t._v("保存为文件")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("foreach")]),t._v("分布式执行。可能无序")])])])])])])]),t._v(" "),n("li",[t._v("可序列化。RDD会被分发到不同的Executor进行处理，所以里面定义的处理逻辑（算子，函数）都是可序列化的\n"),n("ul",[n("li",[t._v("闭包检测：Spark会检测算子的参数（函数）里面的对象是否可序列化。所有需要被executor使用的对象都可以序列化，才可以成功执行函数。而在driver端使用的对象可以不支持序列化")]),t._v(" "),n("li",[t._v("Java自带的序列化可以序列化任何类，但是比较重。Spark 2.0开始支持使用Kryo序列化框架进行序列化，速度是Java Serializable的10倍。\n"),n("ul",[n("li",[t._v("RDD在shuffle数据的时候，简单数据类型、数组、字符串默认使用Kryo进行序列化")]),t._v(" "),n("li",[t._v("Kryo会忽略Java的transient关键字，即使是transient变量也会被序列化")])])])])]),t._v(" "),n("li",[t._v("RDD的依赖\n"),n("ul",[n("li",[t._v("如果两个RDD直接依赖，则就称为【依赖关系】。如果两个RDD之间间接依赖，则称为【血缘关系】")]),t._v(" "),n("li",[t._v("每个RDD都会保存依赖/血缘关系。如果某个计算出现异常，就可以根据血缘进行重新计算")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("RDD.toDebugString")]),t._v("可以看到血缘关系和shuffle")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("RDD.dependencies")]),t._v("可以看到依赖关系")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("OneToOneDependency/NarrowDependency")]),t._v("一个分区对应一个分区的依赖/窄依赖，比如"),n("code",{pre:!0},[t._v("map")]),t._v(" "),n("ul",[n("li",[t._v("上游RDD的每个分区最多被一个下游RDD使用。独生子女")])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("ShuffleDependency")]),t._v("打乱重组依赖/宽依赖。比如"),n("code",{pre:!0},[t._v("groupBy")]),t._v(" "),n("ul",[n("li",[t._v("上游RDD的每个分区被多个下游RDD使用")])])]),t._v(" "),n("li",[t._v("窄依赖只需要在父依赖执行完毕后就可以直接向下执行。宽依赖需要等待所有父任务执行完毕才能继续执行，需要划分阶段")]),t._v(" "),n("li",[t._v("阶段划分\n"),n("ul",[n("li",[t._v("每个job的最后一个task是一个阶段(result stage)")]),t._v(" "),n("li",[t._v("宽依赖会导致阶段的生成")]),t._v(" "),n("li",[t._v("阶段用来进行任务调度")])])]),t._v(" "),n("li",[t._v("任务划分\n"),n("ul",[n("li",[t._v("核心概念\n"),n("ul",[n("li",[t._v("Application\n"),n("ul",[n("li",[t._v("初始化一个SparkContext就会生成一个Application")])])]),t._v(" "),n("li",[t._v("Job\n"),n("ul",[n("li",[t._v("一个Action算子就会生成一个Job")])])]),t._v(" "),n("li",[t._v("Stage\n"),n("ul",[n("li",[t._v("等于宽依赖的个数+1(result stage)")])])]),t._v(" "),n("li",[t._v("Task\n"),n("ul",[n("li",[t._v("一个Stage阶段中，最后一个RDD的分区个数就是此stage的Task的个数")])])]),t._v(" "),n("li",[t._v("Application-Job-Stage-Task每一层都是一对多的关系")])])]),t._v(" "),n("li",[t._v("根据RDD生成DAG，然后根据DAG划分任务，最后交给worker执行")]),t._v(" "),n("li",[t._v("Stage和Task基本上名字的对应的，比如"),n("code",{pre:!0},[t._v("ShuffleMapStage")]),t._v("对应"),n("code",{pre:!0},[t._v("ShuffleMapTask")])])])])])]),t._v(" "),n("li",[t._v("RDD持久化/缓存/复用\n"),n("ul",[n("li",[t._v("假设多个任务共享了步骤，如果我们仅仅是重用了RDD对象，并不能实现RDD复用，因为RDD不保存数据。操作还是执行了两遍")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("RDD.cache()")]),t._v("可以缓存对象在内存。速度快但是可能不安全")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("RDD.persist()")]),t._v("同上")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("RDD.persist(StorageLevel.XXX)")]),t._v("持久化到某个存储中\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("MEMORY_ONLY")]),t._v("仅内存。默认。内存不够了丢数据")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("DISK_ONLY")]),t._v("仅硬盘，不需要指定文件名，保存为临时文件。Application结束后丢失。速度比内存慢")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("DISK_ONLY_2")]),t._v("仅硬盘，备份2份")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("MEMORY_AND_DISK")]),t._v("内存不够了写磁盘")])])]),t._v(" "),n("li",[t._v("持久化操作必须在action算子执行的时候才会缓存")]),t._v(" "),n("li",[t._v("也可以用来缓存一些时间很长的操作的结果")]),t._v(" "),n("li",[t._v("通过在血缘关系中添加一个阶段的方式，使后面的操作优先从cache中取数据，而不是重新计算")])])]),t._v(" "),n("li",[t._v("RDD检查点(checkpoint)\n"),n("ul",[n("li",[t._v("落盘，必须指定路径/文件名，Application结束后也不会丢失")]),t._v(" "),n("li",[t._v("通常保存在HDFS上")]),t._v(" "),n("li",[t._v("为了数据安全，通常会重新执行一遍作业\n"),n("ul",[n("li",[t._v("所以通常和cache结合使用，先cache再checkpoint，就不会重新执行作业了")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("RDD.cache()")])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("RDD.checkpoint()")])])])]),t._v(" "),n("li",[t._v("checkpoint会直接修改血缘关系，把之前的所有依赖/血缘改为自身")])])]),t._v(" "),n("li",[t._v("checkpoint vs cache vs persist\n"),n("ul",[n("li",[t._v("cache是临时存储")]),t._v(" "),n("li",[t._v("persist比cache的数据更安全，但是不能跨作业共享")]),t._v(" "),n("li",[t._v("checkpoint永久保存数据，可以跨作业")])])]),t._v(" "),n("li",[t._v("分区器\n"),n("ul",[n("li",[t._v("内置哈希分区器、范围分区器、python分区器，支持自定义")]),t._v(" "),n("li",[t._v("自定义分区器需要集成"),n("code",{pre:!0},[t._v("Partitioner")])]),t._v(" "),n("li",[t._v("使用"),n("code",{pre:!0},[t._v("RDD.partitionBy(xxx)")]),t._v("来应用自定义分区")])])]),t._v(" "),n("li",[t._v("文件保存和读取\n"),n("ul",[n("li",[t._v("保存\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("saveAsTextFile")])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("saveAsObjectFile")])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("saveAsSequenceFile")]),t._v("仅支持键值数据")])])]),t._v(" "),n("li",[t._v("读取\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("RDD.textFile")]),t._v(" & "),n("code",{pre:!0},[t._v("RDD.objectFile")]),t._v(" & "),n("code",{pre:!0},[t._v("RDD.sequenceFile")])])])])])])])]),t._v(" "),n("li",[t._v("累加器 - 分布式共享"),n("strong",[t._v("只写")]),t._v("变量\n"),n("ul",[n("li",[t._v("思想\n"),n("ul",[n("li",[t._v("正常情况下，driver端的变量在传送给executor的时候，每个executor都会收到一个全新的副本。如果executor对变量进行了修改，driver也无法获得这些变量")]),t._v(" "),n("li",[t._v("累加器可以把driver端定义的变量，在executor端进行修改之后，传回driver进行merge")]),t._v(" "),n("li",[t._v("一些简单的操作，不需要在executor端进行（不需要分片、shuffle等操作），就可以传回driver进行合并")])])]),t._v(" "),n("li",[t._v("示例\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v('sumAcc = sc.longAccumulator("some-name")')])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("rdd.foreach(num => sumAcc.add(num))")])])])]),t._v(" "),n("li",[t._v("系统自带的累加器类型\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("longAccumulator")])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("doubleAccumulator")])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("collectAccumulator")]),t._v("集合类型")])])]),t._v(" "),n("li",[t._v("支持自定义累加器\n"),n("ul",[n("li",[t._v("继承"),n("code",{pre:!0},[t._v("AccumulatorV2")]),t._v("，指定输入类型和输出类型")]),t._v(" "),n("li",[t._v("需要实现如下方法\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("add")]),t._v("在executor端添加新的值")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("isZero")]),t._v("判断累加器是否为初始状态")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("copy")]),t._v("复制累加器")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("reset")]),t._v("重置累加器")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("merge")]),t._v("在driver端合并多个累加器")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("value")]),t._v("在driver端求值")])])])])])])]),t._v(" "),n("li",[t._v("广播变量 - 分布式共享"),n("strong",[t._v("只读")]),t._v("变量\n"),n("ul",[n("li",[t._v("思想\n"),n("ul",[n("li",[t._v("一个Executor里面可能执行多个任务，但是这些任务可能会共享一些变量")]),t._v(" "),n("li",[t._v("从【每个任务传输一次变量】改为【每个Executor传输一次变量】")]),t._v(" "),n("li",[t._v("使用广播变量降低网络传输")])])]),t._v(" "),n("li",[t._v("示例\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("bc = sc.broadcast(xx)")])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("bc.value")]),t._v(" => "),n("code",{pre:!0},[t._v("xx")])])])])])])]),t._v(" "),n("h3",{attrs:{id:"wordcount"}},[t._v("WordCount")]),t._v(" "),n("pre",{staticClass:"language-scala"},[n("code",{pre:!0,attrs:{class:"language-scala"}},[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// init connection")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" sparkConf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" SparkConf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("setMaster"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"local"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("setAppName"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"WordCount"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" sc "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" SparkContext"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sparkConf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// read data by line")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" lines"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" RDD"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("textFile"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"data"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// `data` is a folder")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// split each line to words")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" words"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" RDD"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" lines"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flatMap"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('" "')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// transform `word` to `(word, 1)`")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" wordToOne "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" words"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("map "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  word "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("=>")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("word"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// group by word")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// `(word, 1)` to `(word, (word, 1))`")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" wordGroup"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" RDD"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Iterable"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("Int")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" wordToOne"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("groupBy"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  t "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("=>")]),t._v(" t"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_1\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// calculate word count")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" wordToCount "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" wordGroup"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("map "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("case")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("word"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" list"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("=>")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    list"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reduce"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("t1"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" t2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("=>")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("t1"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_1"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" t1"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_2 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" t2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// or, use `reduceByKey`")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// val wordToCount = wordToOne.reduceByKey(_+_)")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// collect result")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" array"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" Array"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("Int")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" wordToCount"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("collect"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// print result")]),t._v("\narray"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("foreach"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("println"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// close spark connection")]),t._v("\nsc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),n("p",[t._v("上述代码也可以简化为一行：")]),t._v(" "),n("pre",{staticClass:"language-scala"},[n("code",{pre:!0,attrs:{class:"language-scala"}},[t._v("sc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("textFile"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"data"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flatMap"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('" "')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("map"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reduceByKey"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("_"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("collect\n")])]),t._v(" "),n("h3",{attrs:{id:"命令"}},[t._v("命令")]),t._v(" "),n("ul",[n("li",[n("code",{pre:!0},[t._v("bin/spark-shell")]),t._v("进入交互式命令行\n"),n("ul",[n("li",[t._v("使用scala语言")]),t._v(" "),n("li",[t._v("自动提供"),n("code",{pre:!0},[t._v("sc")]),t._v("对象（spark context）")])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("bin/spark-submit <jar> <params>")]),t._v("提交应用\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("--class xxx")]),t._v("应用程序的主类名，比如"),n("code",{pre:!0},[t._v("org.apache.spark.examples.SparkPi")])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("--master xxx")]),t._v("指定主节点\n"),n("ul",[n("li",[t._v("本地模式："),n("code",{pre:!0},[t._v("local")]),t._v("（单线程）或者"),n("code",{pre:!0},[t._v("local[*]")]),t._v("（所有核）")]),t._v(" "),n("li",[t._v("Standalone，指定集群master："),n("code",{pre:!0},[t._v("spark://<host>:<port>")]),t._v("。默认端口7077")]),t._v(" "),n("li",[t._v("Yarn模式，填写"),n("code",{pre:!0},[t._v("Yarn")])])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("--executor-memory 1G")]),t._v("每个executor可用的内存")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("--total-executor-cores 2")]),t._v("所有executor一共使用2个CPU线程")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("--executor-cores 1")]),t._v("每个executor使用的CPU线程数量为1")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("jar")]),t._v("是主类jar包所在位置")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("params")]),t._v("是主程序参数")])])])]),t._v(" "),n("h3",{attrs:{id:"工程化代码"}},[t._v("工程化代码")]),t._v(" "),n("p",[t._v("Java程序三层架构：")]),t._v(" "),n("ul",[n("li",[t._v("Controller - 控制层\n"),n("ul",[n("li",[t._v("接受请求，调度服务")]),t._v(" "),n("li",[t._v("Controller类中保存很多Service的实例")])])]),t._v(" "),n("li",[t._v("Service - 服务层\n"),n("ul",[n("li",[t._v("处理业务逻辑，和持久化层交换数据")])])]),t._v(" "),n("li",[t._v("DAO - 持久化层\n"),n("ul",[n("li",[t._v("访问数据库，维护数据对象模型")])])])]),t._v(" "),n("blockquote",[n("p",[t._v("如果程序有界面（客户端程序/网页），则通常使用MVC三层架构（model/view/controller）。服务端程序通常使用上述三层架构")])]),t._v(" "),n("blockquote",[n("p",[t._v("可以把代码组织到不同的文件夹中：controller/service/dao/util/common/bean。其中util指所有包都可能调用的东西，比如字符串处理函数。common是把一些复用的代码抽象为函数。bean放置一些实体类")])]),t._v(" "),n("p",[t._v("以下为使用python写的工程化伪代码。"),n("strong",[t._v("仅供参考")])]),t._v(" "),n("pre",{staticClass:"language-python"},[n("code",{pre:!0,attrs:{class:"language-python"}},[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# file: util/EnvUtil")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("EnvUtil")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  scLocal "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" threading"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("local"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("put")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("k"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    scLocal"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("k"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" v\n\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("take")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("k"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" scLocal"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("k"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n  \n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("clear")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("k"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("del")]),t._v(" scLocal"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("k"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\nenvUtil "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" EnvUtil"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# file: common/TApplication")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" EnvUtil "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" envUtil\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("TApplication")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("start")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("f"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" name"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" master"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'local[*]'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    sparkConf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SparkConf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("setMaster"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("master"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("setAppName"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("name"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    sc "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SparkContext"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sparkConf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    envUtil"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("put"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'sc'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" sc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    f"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    sc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    envUtil"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("clear"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'sc'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# file: common/TController")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("TController")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("dispatch")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("pass")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# file: common/TService")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("TService")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("dataAnalysis")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("pass")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# file: common/TDao")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" EnvUtil "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" envUtil\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("TDao")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("readFile")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    sc "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" envUtil"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("take"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'sc'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    sc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("textFile"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# file: application/WordCountApplication")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("f")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  controller "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" WordCountController"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  controller"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dispatch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("WordCountApplication")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("TApplication"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("pass")]),t._v("\n\nWordCountApplication"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("start"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("f"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'wc'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# file: controller/WordCountController")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("WordCountController")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("TController"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  wcService "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" WordCountService"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("dispatch")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    result "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" wcService"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataAnalysis"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("result"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# file: service/WordCountService")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("WordCountService")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("TService"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("dataAnalysis")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# file: dao/WordCountDao")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("WordCountDao")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("TDao"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("pass")]),t._v("\n")])]),t._v(" "),n("blockquote",[n("p",[t._v("ThreadLocal负责实现线程之间数据的隔离。它并不能解决线程安全问题，因为它的数据根本不跨线程，不涉及多个线程修改数据的情况。ThreadLocal的主要目的是单个线程的数据共享")])]),t._v(" "),n("h3",{attrs:{id:"源码分析略"}},[t._v("源码分析（略")]),t._v(" "),n("h2",{attrs:{id:"sparksql"}},[t._v("SparkSQL")]),t._v(" "),n("h3",{attrs:{id:"sparksql概述"}},[t._v("SparkSQL概述")]),t._v(" "),n("ul",[n("li",[t._v("用来处理结构化数据")]),t._v(" "),n("li",[t._v("前身是Shark")]),t._v(" "),n("li",[t._v("提供了新的数据模型：DataFrame & DataSet")]),t._v(" "),n("li",[t._v("兼容Hive/HQL")]),t._v(" "),n("li",[t._v("可以直接连接到各种数据库，也可以被JDBC/ODBC连接")]),t._v(" "),n("li",[t._v("支持处理各种类型的数据\n"),n("ul",[n("li",[t._v("CSV/format/JSON/JDBC/text/parquet/orc/…")]),t._v(" "),n("li",[t._v("以JSON为例，每一行都要是一个合法的JSON，而不是整个文档是一个JSON")])])]),t._v(" "),n("li",[t._v("使用"),n("code",{pre:!0},[t._v("import spark.implicits._")]),t._v("引入spark隐式类型转换，比如字符串转数字\n"),n("ul",[n("li",[t._v("此处的"),n("code",{pre:!0},[t._v("spark")]),t._v("不是包名，而是"),n("code",{pre:!0},[t._v("SparkSession")]),t._v("的成员")]),t._v(" "),n("li",[t._v("这条语句通常出现在初始化"),n("code",{pre:!0},[t._v("SparkSession")]),t._v("之后")])])])]),t._v(" "),n("h3",{attrs:{id:"sparksql数据模型"}},[t._v("SparkSQL数据模型")]),t._v(" "),n("ul",[n("li",[t._v("DataFrame\n"),n("ul",[n("li",[t._v("概述\n"),n("ul",[n("li",[t._v("以RDD为基础的分布式数据集")]),t._v(" "),n("li",[t._v("相比于RDD，DataFrame自身就有schema/元数据信息，每一列都有名称和数据类型")]),t._v(" "),n("li",[t._v("懒执行（触发时执行），但是性能比RDD高（因为可以进行各种底层过程优化）")]),t._v(" "),n("li",[t._v("支持使用DSL(domain-specific language)\n"),n("ul",[n("li",[t._v("直接使用"),n("code",{pre:!0},[t._v("df")]),t._v("的方法来访问数据，从而避免创建临时视图之后用"),n("code",{pre:!0},[t._v("spark.sql")]),t._v("执行SQL命令")])])])])]),t._v(" "),n("li",[t._v("创建\n"),n("ul",[n("li",[t._v("从Spark数据源创建\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("spark.read.xxx")])]),t._v(" "),n("li",[t._v("从文件读取的时候，数值类型会默认视为"),n("code",{pre:!0},[t._v("bigint")]),t._v("，可以和"),n("code",{pre:!0},[t._v("long")]),t._v("转换但是不能转换为"),n("code",{pre:!0},[t._v("int")])])])]),t._v(" "),n("li",[t._v("从RDD转换\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("rdd.toDF('name')")]),t._v("RDD变DF")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("df.rdd")]),t._v("DF变RDD")])])]),t._v(" "),n("li",[t._v("从Hive查询")])])]),t._v(" "),n("li",[t._v("API\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("df.show")]),t._v("查看DF内容")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("df.createOrReplaceTempView('xxx')")]),t._v("/"),n("code",{pre:!0},[t._v("df.createTempView('xxx')")]),t._v("创建临时视图\n"),n("ul",[n("li",[t._v("视图只能查询不能修改（就像SQL一样）")]),t._v(" "),n("li",[t._v("生命周期为session会话。在session外查不到。比如"),n("code",{pre:!0},[t._v("spark.newSession.sql()")]),t._v("就查不到")])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("spark.sql('select * from xxx')")]),t._v("执行SQL")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("df.createOrReplaceGlobalTempView('xxx')")]),t._v("/"),n("code",{pre:!0},[t._v("df.createGlobalTempView('xxx')")]),t._v("创建全局临时视图\n"),n("ul",[n("li",[t._v("使用"),n("code",{pre:!0},[t._v("global_temp.xxx")]),t._v("访问视图")])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("df.printSchema")]),t._v("查看表结构")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("df.select('xxx').show()")]),t._v("查询某列\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v('df.select($"xxx" + 1).show')]),t._v("查询某列+1。使用此语法时，每列都要使用"),n("code",{pre:!0},[t._v("$")]),t._v("（Scala）")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("df.select('xx + 1).show")]),t._v("查询某列+1。使用此语法时，每列都要使用"),n("code",{pre:!0},[t._v("'")]),t._v("（Scala）")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v('df.select(\'xxx + 1 as "xxx2")')]),t._v("列的别名")])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v('df.filter($"xxx" > 30)')]),t._v("条件过滤（Scala)\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("df.filter(df.xx > 30)")]),t._v("python语法")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("df.filter('xx > 30')")]),t._v("python语法")])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("df.groupBy")]),t._v("分组")])])])])]),t._v(" "),n("li",[t._v("DataSet\n"),n("ul",[n("li",[t._v("概述\n"),n("ul",[n("li",[t._v("是Spark 1.6添加的新的抽象")]),t._v(" "),n("li",[t._v("是DataFrame的扩展\n"),n("ul",[n("li",[t._v("DataFrame是DataSet的特例")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("type DataFrame = DataSet[Row]")])]),t._v(" "),n("li",[t._v("所以所有DataFrame的API，DataSet都支持")])])]),t._v(" "),n("li",[t._v("把一行数据视为一个对象")]),t._v(" "),n("li",[t._v("提供了RDD的优势（强类型，支持lambda函数）")]),t._v(" "),n("li",[t._v("未来可能会逐步取代RDD和DataFrame成为唯一接口")])])]),t._v(" "),n("li",[t._v("创建\n"),n("ul",[n("li",[t._v("从List直接创建\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("list.toDS")])])])]),t._v(" "),n("li",[t._v("从DataFrame创建（给DataFrame赋予类型）\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("df.as[SomeType]")])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("ds.toDF")]),t._v("还原为DataFrame")])])]),t._v(" "),n("li",[t._v("从RDD创建，RDD里面的元素必须是一个类的对象\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("rdd.toDS")])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("ds.rdd")]),t._v("转换为RDD")])])])])]),t._v(" "),n("li",[t._v("API\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("ds.show()")]),t._v("查看数据")])])])])])]),t._v(" "),n("h3",{attrs:{id:"sparksql环境对象"}},[t._v("SparkSQL环境对象")]),t._v(" "),n("ul",[n("li",[t._v("老版本中\n"),n("ul",[n("li",[t._v("使用SQLConext对象进行SparkSQL查询")]),t._v(" "),n("li",[t._v("使用HiveContext进行Hive查询")])])]),t._v(" "),n("li",[t._v("新版本\n"),n("ul",[n("li",[t._v("使用SparkSesion进行查询")]),t._v(" "),n("li",[t._v("是SQLContext和HiveContext的组合，兼容二者的API")]),t._v(" "),n("li",[t._v("底层是SparkContext("),n("code",{pre:!0},[t._v("sc")]),t._v(")")]),t._v(" "),n("li",[t._v("默认环境对象为"),n("code",{pre:!0},[t._v("spark")])]),t._v(" "),n("li",[t._v("构造环境："),n("code",{pre:!0},[t._v("SparkSession.builder().config(sparkConf).getOrCreate()")])])])])]),t._v(" "),n("h3",{attrs:{id:"udf"}},[t._v("UDF")]),t._v(" "),n("ul",[n("li",[t._v("SQL里面的自定义函数")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v('spark.udf.register("name", f)')])]),t._v(" "),n("li",[t._v("定义UDAF（自定义聚合函数）（老版本）\n"),n("ul",[n("li",[t._v("需要定义一个类，继承"),n("code",{pre:!0},[t._v("UserDefinedAggregateFunction")]),t._v("（此类在新版本已经不推荐使用）")]),t._v(" "),n("li",[t._v("实现如下成员\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("inputSchema")]),t._v("，即输入数据的数据类型")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("bufferSchema")]),t._v("，即中间结果的数据类型")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("dataType")]),t._v("，即输出/最终结果的数据类型")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("deterministic")]),t._v("，函数的稳定性，即输入相同的参数，输出是否相同")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("initialize")]),t._v("，如何初始化缓冲区")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("update")]),t._v("，如何更新缓冲区")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("merge")]),t._v("，如何合并缓冲区")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("evaluate")]),t._v("，如何（聚合）得到最终结果")])])]),t._v(" "),n("li",[t._v("使用"),n("code",{pre:!0},[t._v('spark.udf.register("name", f)')]),t._v("进行注册，然后就可以在"),n("code",{pre:!0},[t._v("spark.sql")]),t._v("的SQL语句中调用")])])]),t._v(" "),n("li",[t._v("定义UDAF（from Spark 3.0）\n"),n("ul",[n("li",[t._v("定义一个类，实现"),n("code",{pre:!0},[t._v("Aggregator")]),t._v("类\n"),n("ul",[n("li",[t._v("Aggregator是模板类，可以指定输入/输出/缓冲区的类型，就可以使用强类型的特性来指定数据，而不用像UserDefinedAggregateFunction一样使用索引来指定数据")])])]),t._v(" "),n("li",[t._v("重写方法\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("zero")]),t._v("，初始化缓冲区")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("reduce")]),t._v("，更新缓冲区")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("merge")]),t._v("，合并缓冲区")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("finish")]),t._v("，计算最终结果")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("bufferEncoder")]),t._v("，如何编码缓冲区以便网络传输\n"),n("ul",[n("li",[t._v("如果是自定义类，则使用"),n("code",{pre:!0},[t._v("Encoders.product")])]),t._v(" "),n("li",[t._v("如果是scala内置类（比如long），则使用"),n("code",{pre:!0},[t._v("Encoders.scalaLong")])])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("outputEncoder")]),t._v("，如何编码结果以便网络传输")])])]),t._v(" "),n("li",[t._v("使用"),n("code",{pre:!0},[t._v('spark.udf.register("name", functions.udaf(new MyAggregator()))')]),t._v("的方式注册")]),t._v(" "),n("li",[t._v("早期spark版本中也有Aggregator，但是不能用在SQL中，只能用在DSL中\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("udafColumn = new MyAggregator().toColumn")])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("ds.select(udafColumn)")])])])]),t._v(" "),n("li",[t._v("DataFrame/DataSet自带一些聚合函数，比如"),n("code",{pre:!0},[t._v("count/countDistinct/avt/max/min")])]),t._v(" "),n("li",[t._v("Aggregator和UserDefinedAggregateFunction的最大区别是：前者是强类型，后者是弱类型")])])])]),t._v(" "),n("h3",{attrs:{id:"数据的读取和保存"}},[t._v("数据的读取和保存")]),t._v(" "),n("ul",[n("li",[t._v("默认文件格式为Parquet（读取和写入）。可以根据"),n("code",{pre:!0},[t._v("spark.sql.sources.default")]),t._v("修改默认数据格式\n"),n("ul",[n("li",[t._v("Parquet是一种能够有效存储嵌套数据的列式存储格式")]),t._v(" "),n("li",[t._v("读取JSON的时候，每一行都要是一个合法的JSON，而不是整个文档是一个JSON\n"),n("ul",[n("li",[t._v("JSON会被解析为"),n("code",{pre:!0},[t._v("Dataset[Row]")])])])])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("spark.read.load")]),t._v("是通用的数据加载方法。默认加载Parquet\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v('spark.read.format("json").load("path")')]),t._v("加载其他格式的数据")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v('spark.read.json("path")')])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v('spark.sql("select * from json.`path"`").show()')]),t._v("直接在SQL语句中读数据")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v('spark.read.format("csv").option("sep", ";").option("inferSchema", "true").option("header", "true").load("path")')])])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("df.write.save('path')")]),t._v("写文件。默认写Parquet\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v('df.write.format("json").save("path")')]),t._v("写JSON")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v('df.write.mode("append").json("path")')]),t._v("设置模式\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("error")]),t._v("如果文件已经存在则抛出异常（默认）")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("append")]),t._v("如果文件已经存在则追加")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("overwrite")]),t._v("如果文件已经存在则覆盖")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("ignore")]),t._v("如果文件已经存在则忽略")])])])])]),t._v(" "),n("li",[t._v("接入Hive\n"),n("ul",[n("li",[t._v("SparkSQL原生就支持Hive。也可以在编译SparkSQL的时候就加上Hive的支持。可以使用Hive的元数据")]),t._v(" "),n("li",[t._v("使用内置的Hive（不连接到Hive服务）\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v('spark.sql("show tables").show()')]),t._v("查看Hive里面的元数据\n"),n("ul",[n("li",[t._v("数据保存在"),n("code",{pre:!0},[t._v("/opt/module/spark-local/metastore_db")]),t._v("目录下")])])]),t._v(" "),n("li",[t._v("使用"),n("code",{pre:!0},[t._v("df.createOrReplaceTempView")]),t._v("会创建临时元数据")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v('spark.sql("create table user(id int)")')]),t._v("创建表")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("spark.sql(\"load data local inpath 'path' into table user\")")]),t._v("把数据加载到表\n"),n("ul",[n("li",[t._v("数据加载在"),n("code",{pre:!0},[t._v("/opt/module/spark-local/spark-warehouse/user")]),t._v("目录下")])])])])]),t._v(" "),n("li",[t._v("使用外部Hive服务\n"),n("ul",[n("li",[t._v("需要把"),n("code",{pre:!0},[t._v("hive-site.xml")]),t._v("放到spark的"),n("code",{pre:!0},[t._v("conf")]),t._v("目录下")]),t._v(" "),n("li",[t._v("需要把mysql驱动拷贝到"),n("code",{pre:!0},[t._v("jars")]),t._v("目录下")]),t._v(" "),n("li",[t._v("如果访问不到hdfs，需要把"),n("code",{pre:!0},[t._v("core-site.xml")]),t._v(" & "),n("code",{pre:!0},[t._v("hdfs-site.xml")]),t._v("拷贝到"),n("code",{pre:!0},[t._v("conf")]),t._v("目录下")])])]),t._v(" "),n("li",[t._v("使用代码访问Hive\n"),n("ul",[n("li",[t._v("把"),n("code",{pre:!0},[t._v("hive-site.xml")]),t._v("拷贝到classpath下")]),t._v(" "),n("li",[t._v("创建支持Hive的SparkSession: "),n("code",{pre:!0},[t._v("spark = SparkSession.builder().enableHiveSupport().config(sparkConf).getOrCreate()")])]),t._v(" "),n("li",[t._v("添加依赖，比如MySQL的驱动")])])]),t._v(" "),n("li",[t._v("使用命令行工具"),n("code",{pre:!0},[t._v("spark-sql")]),t._v("可以直接访问到Hive，执行各种SQL命令。类似于Hive的CLI")]),t._v(" "),n("li",[t._v("使用"),n("code",{pre:!0},[t._v("beeline")]),t._v(" "),n("ul",[n("li",[t._v("Spark Thrift Server是Spark社区实现的一个Thrift服务，目标是无缝兼容HiveServer2")]),t._v(" "),n("li",[t._v("部署Spark Thrift Server后，可以直接使用hive的beeline访问Spark Thrift Server执行操作。也就是说Spark Thrift Server可以视为Hive服务端")]),t._v(" "),n("li",[t._v("Spark Thrift Server的目标是取代HiveServer2。它可以和HDFS里面的Hive Metastore交互，获取元数据")])])])])])]),t._v(" "),n("h2",{attrs:{id:"sparkstreaming"}},[t._v("SparkStreaming")]),t._v(" "),n("h3",{attrs:{id:"sparkstreaming概述"}},[t._v("SparkStreaming概述")]),t._v(" "),n("ul",[n("li",[t._v("处理准实时流式数据\n"),n("ul",[n("li",[t._v("通常把毫秒级处理视为实时处理，把小时/天级别处理视为离线处理")]),t._v(" "),n("li",[t._v("Spark的架构决定了它只能实现批处理。SparkStreaming只是使用微批(micro-batch)的概念来加速处理，但是仍然无法实现毫秒级别，只能实现准实时处理，秒级/分钟级延迟")]),t._v(" "),n("li",[t._v("真正的实时处理，可以使用Flink/Storm")])])]),t._v(" "),n("li",[t._v("多种数据源，比如Kafka/Flume/Twitter/MQ/socket")]),t._v(" "),n("li",[t._v("使用Spark的算子进行计算，比如map/reduce/join/window")]),t._v(" "),n("li",[t._v("数据模型为【离散化流(discretized stream, DStream)】\n"),n("ul",[n("li",[t._v("可以把DStream视为很多的RDD。根据DStream根据时间间隔从源获取数据形成RDD并发送给SparkCore进行逻辑处理")])])]),t._v(" "),n("li",[t._v("背压机制(Spark Streaming Backpressure)\n"),n("ul",[n("li",[t._v("根据JobScheduler反馈的作业信息，动态调整Receiver接收数据的速率")]),t._v(" "),n("li",[t._v("默认不启用。需要修改"),n("code",{pre:!0},[t._v("spark.streaming.backpressure.enabled")])])])])]),t._v(" "),n("h3",{attrs:{id:"stream-word-count"}},[t._v("Stream Word Count")]),t._v(" "),n("blockquote",[n("p",[t._v("使用netcat工具向9999端口写入数据："),n("code",{pre:!0},[t._v("netcat -lp 9999")])])]),t._v(" "),n("pre",{staticClass:"language-scala"},[n("code",{pre:!0,attrs:{class:"language-scala"}},[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 初始化环境")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" sparkConf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" SparkConf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("setMaster"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"local[*]"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("setAppName"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"SparkStreaming"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" ssc "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" StreamingContext"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sparkConf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Seconds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 采集周期为3秒")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 设置数据源")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" lines "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ssc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("socketTextStream"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"localhost"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("9999")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 定义WC逻辑")]),t._v("\nlines"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flatMap"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('" "')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("map"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reduceByKey"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("_"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("print"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 开始任务并持续等待")]),t._v("\nssc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("start"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nssc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("awaitTermination"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 关闭环境")]),t._v("\nssc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),n("h3",{attrs:{id:"dstream数据源"}},[t._v("DStream数据源")]),t._v(" "),n("ul",[n("li",[n("code",{pre:!0},[t._v("ssc.queueStream(new mutable.Queue[RDD[Int]](), oneAtTime = false)")]),t._v(" "),n("ul",[n("li",[t._v("从RDD Queue创建DStream")]),t._v(" "),n("li",[t._v("通常用来调试")])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v('ssc.socketTextStream("localhost", 9999)')]),t._v(" "),n("ul",[n("li",[t._v("socket数据源")])])]),t._v(" "),n("li",[t._v("自定义数据源\n"),n("ul",[n("li",[t._v("继承"),n("code",{pre:!0},[t._v("Receiver")]),t._v("类\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("class MyReceiver extends Receiver[String](StorageLevel.MEMORY_ONLY)")])])])]),t._v(" "),n("li",[t._v("重写方法\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("onStart")]),t._v(" "),n("ul",[n("li",[t._v("创建一个新线程，死循环采集数据并使用"),n("code",{pre:!0},[t._v("store")]),t._v("方法保存数据到流")])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("onStop")]),t._v(" "),n("ul",[n("li",[t._v("停止"),n("code",{pre:!0},[t._v("onStart")]),t._v("里面的死循环")])])])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("ssc.receiverStream(new MyReceiver())")]),t._v("创建自定义数据源DStream")])])]),t._v(" "),n("li",[t._v("Kafka数据源\n"),n("ul",[n("li",[t._v("老版本\n"),n("ul",[n("li",[t._v("使用ReceiverAPI，需要一个专门的Executor收集数据，然后发给其他Executor进行计算。通常会导致接收数据的Receiver内存溢出（因为收集数据比处理数据快）。目前版本已经无法使用此方式")])])]),t._v(" "),n("li",[t._v("当前版本\n"),n("ul",[n("li",[t._v("使用DirectAPI，由负责计算的Executor直接消费Kafka的数据，速率由Executor自身控制")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v('KafkaUtils.createDirectStream[String, String](ssc, LocationStrategies.PreferConsistent, ConsumerStrategy.Subscribe[String, String](Set("xxx"), kafkaPara))')]),t._v(" "),n("ul",[n("li",[n("code",{pre:!0},[t._v("createDirectStream[String, String]")]),t._v("是在分别指定Kafka中k-v的数据类型")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("LocationStrategies.PreferConsistent")]),t._v("由引擎决定从哪个Broker读取数据")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("ConsumerStrategy")]),t._v("指定消费者的详情\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v('Set("xxx")')]),t._v("指定Topic")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("KafkaPara")]),t._v("一个保存Kafka连接信息的变量")])])]),t._v(" "),n("li",[t._v("返回一个InputDStream")])])])])])])])]),t._v(" "),n("h3",{attrs:{id:"dstream转换操作"}},[t._v("DStream转换操作")]),t._v(" "),n("ul",[n("li",[t._v("类似于RDD，DStream也是定义一系列的转换操作，然后run程序")]),t._v(" "),n("li",[t._v("支持有状态转换。用来在微批之间传递状态")]),t._v(" "),n("li",[t._v("无状态操作\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("transform")]),t._v("允许DStream执行任何RDD-to-RDD的转换")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("join")]),t._v("两个DStream之间的join\n"),n("ul",[n("li",[t._v("其实就是join两个微批的RDD")])])])])]),t._v(" "),n("li",[t._v("有状态操作\n"),n("ul",[n("li",[n("code",{pre:!0},[t._v("UpdateStateByKey")]),t._v("根据key更新状态\n"),n("ul",[n("li",[t._v("首先根据key对数据进行分组")]),t._v(" "),n("li",[t._v("然后根据key得到已有的数据"),n("code",{pre:!0},[t._v("opt")]),t._v("和当前微批中的数据"),n("code",{pre:!0},[t._v("seq")])]),t._v(" "),n("li",[t._v("合并"),n("code",{pre:!0},[t._v("seq")]),t._v("和"),n("code",{pre:!0},[t._v("opt")]),t._v("得到新的"),n("code",{pre:!0},[t._v("opt")])]),t._v(" "),n("li",[t._v("需要提前设置checkpoint以防状态数据丢失")])])])])]),t._v(" "),n("li",[t._v("窗口操作\n"),n("ul",[n("li",[t._v("把多个满足条件的微批当作一个微批，并且支持滑动，使一个微批被多次引用")]),t._v(" "),n("li",[t._v("窗口的范围必须是采集周期的整数倍")]),t._v(" "),n("li",[t._v("默认每个采集周期进行一次计算（滑动距离/步长为一个采集周期）")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("stream.window(Seconds(6), Seconds(6)).reduceByKey(_+_)")]),t._v(" "),n("ul",[n("li",[t._v("第一个Seconds表示窗口范围，第二个Seconds表示滑动距离（步长）")])])]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("countByWindow")]),t._v("返回滑动窗口中的元素个数")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("reduceByWindow")]),t._v("整合窗口中的流元素，得到一个新的单元素流")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("reduceByKeyAndWindow")]),t._v("根据key和windows整合元素\n"),n("ul",[n("li",[t._v("有两个版本。一个版本只需要传入一个reduce函数，另一个版本可以传入两个reduce函数，后者用来优化window中重复数据被重复计算的问题")])])])])])]),t._v(" "),n("pre",{staticClass:"language-scala"},[n("code",{pre:!0,attrs:{class:"language-scala"}},[t._v("stream"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("updateStateByKey"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("seq"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" Seq"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("Int")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" opt"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" Option"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("Int")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("=>")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    Option"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("opt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("getOrElse"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" seq"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("num"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),n("h3",{attrs:{id:"dstream输出操作"}},[t._v("DStream输出操作")]),t._v(" "),n("ul",[n("li",[t._v("Spark Streaming如果没有输出操作，会报错（视为丢失流数据）")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("print")]),t._v("输出数据和时间戳")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("saveAsTextFiles/ObjectFiles/HadoopFiles")]),t._v("保存文件")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("foreachRDD")]),t._v("执行任意RDD操作，默认不输出时间戳")])]),t._v(" "),n("h3",{attrs:{id:"优雅结束流任务"}},[t._v("优雅结束流任务")]),t._v(" "),n("ul",[n("li",[t._v("流任务是一直执行的分布式的任务。一个一个关闭任务太不优雅了，而且可能有数据不一致的问题")]),t._v(" "),n("li",[n("code",{pre:!0},[t._v("ssc.stop(true, true)")]),t._v("优雅关闭\n"),n("ul",[n("li",[t._v("第一个true表示停止SparkContext")]),t._v(" "),n("li",[t._v("第二个true表示启用优雅关闭，即计算节点不再接收新的数据，但是会把当前数据处理完毕")]),t._v(" "),n("li",[t._v("通常在另一个线程中调用此函数。主线程仍然是"),n("code",{pre:!0},[t._v("ssc.start(); ssc.awaitTermination();")])]),t._v(" "),n("li",[t._v("通常由一个外部存储保存状态（比如数据库、redis等）。线程会轮询，或者被事件触发，从而优雅关闭流任务")]),t._v(" "),n("li",[t._v("停止之前通常会使用"),n("code",{pre:!0},[t._v("ssc.getState()")]),t._v("获取当前状态，防止关闭已经被关闭的流")])])])]),t._v(" "),n("pre",{staticClass:"language-scala"},[n("code",{pre:!0,attrs:{class:"language-scala"}},[t._v("ssc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("start"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" Thread"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" Runnable "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("override")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" run"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("Unit")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("someCondition"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" state "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ssc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("getState"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("state "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" StreamingContextState"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ACTIVE"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            ssc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            System"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("exit"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n        Thread"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sleep"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5000")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nssc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("awaitTermination"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),n("h3",{attrs:{id:"恢复数据"}},[t._v("恢复数据")]),t._v(" "),n("ul",[n("li",[t._v("优雅结束之后，需要根据检查点恢复数据并重新启动流")]),t._v(" "),n("li",[t._v("关键是在"),n("code",{pre:!0},[t._v("StreamingContext.getActiveOrCreate")]),t._v("的时候，传入检查点路径")])]),t._v(" "),n("pre",{staticClass:"language-scala"},[n("code",{pre:!0,attrs:{class:"language-scala"}},[t._v("ssc "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" StreamingContext"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("getActiveOrCreate"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"checkpoint-path"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("=>")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// create ssc")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" sparkConf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" SparkConf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("setMaster"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"xxx"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("setAppName"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"xxx"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" ssc "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" StreamingContext"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sparkConf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Seconds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// define operations")]),t._v("\n  ssc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("socketTextStream"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"localhost"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("9999")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("map"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("print"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n  ssc\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nssc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("checkpoint"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"checkpoint-path"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nssc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("start"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// graceful stop")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" Thread"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n\nssc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("awaitTermination"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),n("h3",{attrs:{id:"示例流处理场景"}},[t._v("示例流处理场景")]),t._v(" "),n("ul",[n("li",[t._v("把短时间点击某个广告超过100次的用户放入黑名单\n"),n("ul",[n("li",[t._v("屏蔽恶意广告点击")])])]),t._v(" "),n("li",[t._v("实时统计某些数据，比如广告点击总流量")])])])}]};t.exports={attributes:{title:"Big Data(Part 8)",description:"Spark"},vue:{render:n.render,staticRenderFns:n.staticRenderFns,component:{data:function(){return{templateRender:null}},render:function(t){return this.templateRender?this.templateRender():t("div","Rendering")},created:function(){this.templateRender=n.render,this.$options.staticRenderFns=n.staticRenderFns}}}}}}]);
(window.webpackJsonp=window.webpackJsonp||[]).push([[115],{1147:function(t,n){const e={render:function(){var t=this;t.$createElement;return t._self._c,t._m(0)},staticRenderFns:[function(){var t=this,n=t.$createElement,e=t._self._c||n;return e("div",{staticClass:"frontmatter-markdown"},[e("h2",{attrs:{id:"前言"}},[t._v("前言")]),t._v(" "),e("p",[t._v("本文是观看"),e("a",{attrs:{href:"https://www.bilibili.com/video/BV1cW411r7c5"}},[t._v("此视频")]),t._v("时的笔记")]),t._v(" "),e("h2",{attrs:{id:"hadoop概述"}},[t._v("Hadoop概述")]),t._v(" "),e("p",[t._v("Hadoop是由Apache基金会开发的分布式系统基础架构，主要解决海量数据的存储与计算问题")]),t._v(" "),e("p",[t._v("由于很多大数据框架都基于Hadoop，所以【Hadoop】这个名词有时候也代指Hadoop生态圈")]),t._v(" "),e("p",[t._v("基于Google在大数据方面的三篇论文，Hadoop有如下三个核心组件")]),t._v(" "),e("ul",[e("li",[t._v("HDFS\n"),e("ul",[e("li",[t._v("参考Google GFS的论文")])])]),t._v(" "),e("li",[t._v("MR\n"),e("ul",[e("li",[t._v("参考Google Map-Reduce的论文")])])]),t._v(" "),e("li",[t._v("HBase\n"),e("ul",[e("li",[t._v("参考Google BigTable的论文")])])])]),t._v(" "),e("p",[t._v("三大发行版本：")]),t._v(" "),e("ul",[e("li",[t._v("Apache\n"),e("ul",[e("li",[t._v("原生版本，适合入门学习")])])]),t._v(" "),e("li",[t._v("CDH\n"),e("ul",[e("li",[t._v("Cloudera的Hadoop发行版")]),t._v(" "),e("li",[t._v("商业支持")]),t._v(" "),e("li",[t._v("兼容性、安全性、稳定性更强")]),t._v(" "),e("li",[t._v("在大型互联网企业中用的较多")])])]),t._v(" "),e("li",[t._v("Hortonworks发行版\n"),e("ul",[e("li",[t._v("Hortonworks的主打产品是HDP(Hortonworks Data Platform)")]),t._v(" "),e("li",[t._v("文档较好")])])])]),t._v(" "),e("p",[t._v("Hadoop的优势：")]),t._v(" "),e("ul",[e("li",[t._v("高可靠\n"),e("ul",[e("li",[t._v("底层维护多个数据副本（默认3个）")])])]),t._v(" "),e("li",[t._v("高扩展\n"),e("ul",[e("li",[t._v("任务分配给数以千计的节点，轻松扩展")])])]),t._v(" "),e("li",[t._v("高效\n"),e("ul",[e("li",[t._v("基于MapReduce并行工作")])])]),t._v(" "),e("li",[t._v("高容错\n"),e("ul",[e("li",[t._v("自动把失败的任务重新分配")])])])]),t._v(" "),e("h2",{attrs:{id:"核心组件"}},[t._v("核心组件")]),t._v(" "),e("h3",{attrs:{id:"版本概述"}},[t._v("版本概述")]),t._v(" "),e("ul",[e("li",[t._v("1.x\n"),e("ul",[e("li",[t._v("Common\n"),e("ul",[e("li",[t._v("辅助工具集")])])]),t._v(" "),e("li",[t._v("HDFS\n"),e("ul",[e("li",[t._v("数据存储")])])]),t._v(" "),e("li",[t._v("MapReduce\n"),e("ul",[e("li",[t._v("计算+资源调度")])])])])]),t._v(" "),e("li",[t._v("2.x（解耦了计算和资源调度）\n"),e("ul",[e("li",[t._v("Common\n"),e("ul",[e("li",[t._v("辅助工具集")])])]),t._v(" "),e("li",[t._v("HDFS\n"),e("ul",[e("li",[t._v("数据存储")])])]),t._v(" "),e("li",[t._v("MapReduce\n"),e("ul",[e("li",[t._v("计算")])])]),t._v(" "),e("li",[t._v("Yarn\n"),e("ul",[e("li",[t._v("资源调度")])])])])])]),t._v(" "),e("h3",{attrs:{id:"hdfs"}},[t._v("HDFS")]),t._v(" "),e("blockquote",[e("p",[t._v("类似于MFS那样的分布式存储/对象存储")])]),t._v(" "),e("blockquote",[e("p",[t._v("Hadoop Distributed File System")])]),t._v(" "),e("h4",{attrs:{id:"hdfs组件"}},[t._v("HDFS组件")]),t._v(" "),e("ul",[e("li",[t._v("NameNode(nn)，就是HDFS集群的master node\n"),e("ul",[e("li",[t._v("存储文件的元数据\n"),e("ul",[e("li",[t._v("文件名")]),t._v(" "),e("li",[t._v("目录结构")]),t._v(" "),e("li",[t._v("生成时间")]),t._v(" "),e("li",[t._v("权限")]),t._v(" "),e("li",[t._v("副本数")]),t._v(" "),e("li",[t._v("文件数据块索引")])])]),t._v(" "),e("li",[t._v("配置副本策略")]),t._v(" "),e("li",[t._v("处理客户端读写请求（类似于MFS，仅处理文件元数据的请求）")]),t._v(" "),e("li",[t._v("数据保存在内存，并持久化到磁盘(FsImage)\n"),e("ul",[e("li",[t._v("所有更新操作都先以追加的形式写入到Edits文件（相当于日志），而不是修改FsImage，从而保证效率（【追加】操作效率较高）")]),t._v(" "),e("li",[t._v("为了避免Edits文件过大，会定期合并Edits和FsImage。如果有2NN，则由2NN执行此操作以减轻NN压力")])])]),t._v(" "),e("li",[e("strong",[t._v("NN不持久化保存数据块的位置")]),t._v("，而是DN进行维护，并周期性向NN汇报，NN把这些信息保存在内存")]),t._v(" "),e("li",[t._v("支持【多目录】，把文件保存在多个本地目录中，每个目录的内容"),e("strong",[t._v("相同")]),t._v("。增加了可靠性")])])]),t._v(" "),e("li",[t._v("DataNode(dn)\n"),e("ul",[e("li",[t._v("存储文件的数据和部分元数据\n"),e("ul",[e("li",[t._v("数据长度")]),t._v(" "),e("li",[t._v("校验和（CRC校验）")]),t._v(" "),e("li",[t._v("时间戳")])])]),t._v(" "),e("li",[t._v("DN启动后向NN注册，告诉NN自己有哪些文件")]),t._v(" "),e("li",[t._v("启动后，默认每个小时向NN汇报所有信息")]),t._v(" "),e("li",[t._v("心跳\n"),e("ul",[e("li",[t._v("每隔3秒，NN向DN发送请求进行健康检查")]),t._v(" "),e("li",[t._v("如果超过10分钟+30秒，NN没有收到DN的心跳，则判定节点不可用")]),t._v(" "),e("li",[t._v("超时时长计算公式："),e("code",{pre:!0},[t._v("2 * dfs.namenode.heartbeat.recheck-interval + 10 * dfs.heartbeat.interval")]),t._v(" "),e("ul",[e("li",[e("code",{pre:!0},[t._v("dfs.namenode.heartbeat.recheck-interval")]),t._v("默认为5分钟")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("dfs.heartbeat.interval")]),t._v("默认为3秒")])])])])]),t._v(" "),e("li",[t._v("支持【多目录】，每个目录存储的数据"),e("strong",[t._v("不同")]),t._v("，各个目录的数据加起来是节点的所有数据。可以用来把不同数据放在不同磁盘")])])]),t._v(" "),e("li",[t._v("Secondary NameNode(2nn)\n"),e("ul",[e("li",[t._v("监控HDFS状态的辅助程序，周期性获取HDFS元数据快照")]),t._v(" "),e("li",[t._v("不是热备，NN挂掉的时候，2NN并不能代替NN提供服务")]),t._v(" "),e("li",[t._v("分担NN的工作量，比如定期合并FsImage和Edits，然后推送给NN")]),t._v(" "),e("li",[t._v("紧急情况下可以用2NN恢复NN的数据，但是会丢一部分数据")])])]),t._v(" "),e("li",[t._v("Client，客户端，是java包\n"),e("ul",[e("li",[t._v("文件上传的时候会把文件切块然后上传，默认每块128MB")]),t._v(" "),e("li",[t._v("和NameNode交互元数据，和DataNode交互数据")]),t._v(" "),e("li",[t._v("提供一些管理HDFS的命令，比如初始化NameNode，也可以增删改查")])])])]),t._v(" "),e("h4",{attrs:{id:"其他细节"}},[t._v("其他细节")]),t._v(" "),e("ul",[e("li",[t._v("HDFS是一个文件系统")]),t._v(" "),e("li",[t._v("通过目录树来定位文件")]),t._v(" "),e("li",[t._v("分布式")]),t._v(" "),e("li",[t._v("适合一次写入，多次读出，不支持修改")]),t._v(" "),e("li",[t._v("常用来做离线数据分析，无法做实时、低延迟数据分析")]),t._v(" "),e("li",[t._v("优点\n"),e("ul",[e("li",[t._v("多副本（默认3），高容错，自动恢复副本数量\n"),e("ul",[e("li",[t._v("如果副本数量为3，则第一个副本在本地机架的某一个节点，第二个副本和第一个副本位于同一个机架的不同节点，第三个副本位于不同机架的节点上，从而同时保证冗余和速度")])])]),t._v(" "),e("li",[t._v("PB级数据存储，百万级文件数量")])])]),t._v(" "),e("li",[t._v("缺点\n"),e("ul",[e("li",[t._v("不适合实时数据访问")]),t._v(" "),e("li",[t._v("无法高效存储大量小文件\n"),e("ul",[e("li",[t._v("因为小文件会浪费NameNode的内存")]),t._v(" "),e("li",[t._v("小文件的寻址时间比读取时间还长")])])]),t._v(" "),e("li",[t._v("不支持文件随机访问/修改，仅支持末尾追加")]),t._v(" "),e("li",[t._v("不支持并发写入，一个文件只能有一个写进程")])])]),t._v(" "),e("li",[t._v("HDFS在同一个物理机上仅保存一个备份，即使replication数量大于物理机数量")]),t._v(" "),e("li",[t._v("HDFS NameNode默认有一个WEB UI，可以用来进行数据管理\n"),e("ul",[e("li",[t._v("HTTP端口：50070")]),t._v(" "),e("li",[t._v("HTTPS端口：50470")])])]),t._v(" "),e("li",[t._v("NameNode的IPC端口：8020/9000")]),t._v(" "),e("li",[t._v("数据块的大小默认为128MB，文件大于128MB时会拆分，文件小于128MB时并不意味着这个数据块会占128MB的空间\n"),e("ul",[e("li",[t._v("如何确定数据块大小：建议寻址时间是传输时间的1%。比如寻址时间是10ms，硬盘速度100MB/s，那么数据块大小建议为100MB")])])]),t._v(" "),e("li",[t._v("每个数据块都会有一个元数据，用来校验")]),t._v(" "),e("li",[t._v("支持流IO上传/下载，支持定位读取")])]),t._v(" "),e("h4",{attrs:{id:"数据流"}},[t._v("数据流")]),t._v(" "),e("ul",[e("li",[t._v("写数据\n"),e("ul",[e("li",[t._v("客户端向NN发上传请求，NN返回允许")]),t._v(" "),e("li",[t._v("客户端请求上传第一个数据块，NN根据DN的负载、DN和客户端的距离、数据副本数量等因素，返回多个DN地址\n"),e("ul",[e("li",[t._v("距离的计算\n"),e("ul",[e("li",[t._v("同一个节点上的进程，距离为0")]),t._v(" "),e("li",[t._v("节点距离为两个节点到达共同祖先的距离之和，比如，同一个机架上的两个节点，共同祖先是机架，则距离为2")])])])])]),t._v(" "),e("li",[t._v("假设副本数量为3。客户端向DN1发请求，DN1向DN2发请求，DN2向DN3发请求，然后逆向响应，最后响应到客户端，建立通道")]),t._v(" "),e("li",[t._v("客户端流式上传数据，DN1/2/3之间按顺序流式复制")]),t._v(" "),e("li",[t._v("客户端通知NN，传输完毕")])])]),t._v(" "),e("li",[t._v("读数据\n"),e("ul",[e("li",[t._v("客户端向NN发读取请求，NN返回文件的元数据，包括数据在哪个DN上")]),t._v(" "),e("li",[t._v("客户端从最近的DN节点"),e("strong",[t._v("依次")]),t._v("请求数据的每个块（不是并行）")])])]),t._v(" "),e("li",[t._v("更新NN元数据\n"),e("ul",[e("li",[t._v("客户端发起修改请求")]),t._v(" "),e("li",[t._v("NN修改Edits文件，然后修改元数据")]),t._v(" "),e("li",[t._v("如果定时时间到了（默认1小时），或者Edits中的数据满了（2NN会不断请求NN以检查Edits是否满了，默认每分钟一次，默认100w条记录算满），2NN会请求执行CheckPoint，即帮助NN合并Edits & FsImage")]),t._v(" "),e("li",[t._v("NN滚动Edits文件（比如把文件后缀从001变为002）")]),t._v(" "),e("li",[t._v("2NN把需要的FsImage & Edits文件拷贝到本机，然后加载到内存进行合并")]),t._v(" "),e("li",[t._v("把合并结束的FsImage拷贝回NN")])])])]),t._v(" "),e("h4",{attrs:{id:"nn故障恢复"}},[t._v("NN故障恢复")]),t._v(" "),e("p",[t._v("有如下几个方案：")]),t._v(" "),e("ul",[e("li",[t._v("把2NN的数据拷贝到NN")]),t._v(" "),e("li",[t._v("从2NN导入checkpoint")])]),t._v(" "),e("h4",{attrs:{id:"安全模式"}},[t._v("安全模式")]),t._v(" "),e("ul",[e("li",[t._v("NN启动时，先把FsImage加载到内存，然后根据Edits文件更新内存，然后创建新的FsImage & Edits，然后开始监听请求。这段时间内NN无法接收请求，处于安全模式，只读，不可写\n"),e("ul",[e("li",[t._v("刚刚格式化的NN由于没有保存任何数据块，所以不会进入安全模式")])])]),t._v(" "),e("li",[t._v("DN启动时会向NN发送自身的块列表信息，从而报告数据块保存的位置。这个期间处于安全模式")]),t._v(" "),e("li",[t._v("如果满足【最小副本条件】，则NN会在30秒内退出安全模式\n"),e("ul",[e("li",[t._v("最小副本条件：整个文件系统中99.9%的块满足【最小副本级别】（默认为1）")])])]),t._v(" "),e("li",[t._v("可以手动进入安全模式，从而使HDFS只读")])]),t._v(" "),e("h4",{attrs:{id:"访问控制"}},[t._v("访问控制")]),t._v(" "),e("p",[t._v("可以使用白名单/黑名单来控制哪些DN可以注册到NN，防止外界恶意注册到自己的内部集群，也可以用来退役某些节点")]),t._v(" "),e("ul",[e("li",[t._v("白名单\n"),e("ul",[e("li",[t._v("配置NN的"),e("code",{pre:!0},[t._v("/opt/module/hadoop-x.x.x/etc/hadoop/dfs.hosts")]),t._v("文件（文件名不一定是"),e("code",{pre:!0},[t._v("dfs.hosts")]),t._v("，官方推荐"),e("code",{pre:!0},[t._v("dfs.hosts")]),t._v("）\n"),e("ul",[e("li",[t._v("每一行是一个主机名，表示允许访问的主机")])])]),t._v(" "),e("li",[t._v("配置"),e("code",{pre:!0},[t._v("hdfs-site.xml")]),t._v("，并同步到所有节点\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("<name>dfs.hosts</name> <value>/opt/module/hadoop-x.x.x/etc/hadoop/dfs.hosts</value>")])])])]),t._v(" "),e("li",[t._v("刷新NN："),e("code",{pre:!0},[t._v("hdfs dfsadmin -refreshNodes")])]),t._v(" "),e("li",[t._v("更新ResourceManager节点："),e("code",{pre:!0},[t._v("yarn rmadmin -refreshNodes")])]),t._v(" "),e("li",[t._v("数据重平衡："),e("code",{pre:!0},[t._v("start-balancer.sh")])])])]),t._v(" "),e("li",[t._v("黑名单\n"),e("ul",[e("li",[t._v("配置NN的"),e("code",{pre:!0},[t._v("dfs.hosts.exclude")]),t._v("文件")]),t._v(" "),e("li",[t._v("更新"),e("code",{pre:!0},[t._v("hdfs-site.xml")]),t._v("，并同步到所有节点\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("<name>dfs.hosts.exclude</name> <value>/path/to/dfs.hosts.exclude</value>")])])])]),t._v(" "),e("li",[t._v("刷新NN："),e("code",{pre:!0},[t._v("hdfs dfsadmin -refreshNodes")])]),t._v(" "),e("li",[t._v("更新ResourceManager节点："),e("code",{pre:!0},[t._v("yarn rmadmin -refreshNodes")])]),t._v(" "),e("li",[t._v("数据重平衡："),e("code",{pre:!0},[t._v("start-balancer.sh")])])])])]),t._v(" "),e("h4",{attrs:{id:"hdfs-2x新特性"}},[t._v("HDFS 2.x新特性")]),t._v(" "),e("ol",[e("li",[t._v("跨集群数据拷贝："),e("code",{pre:!0},[t._v("hadoop distcp <src> <dst>")])]),t._v(" "),e("li",[t._v("小文件存档：HAR类型文件，减少NN内存占用，且直接使用har协议就可以查看内容\n"),e("ol",[e("li",[t._v("存档："),e("code",{pre:!0},[t._v("hadoop archive -archiveName xxx.har -p <input-path> <output-path>")])]),t._v(" "),e("li",[t._v("查看存档："),e("code",{pre:!0},[t._v("hadoop fs -lsr har:///xxx.har")])]),t._v(" "),e("li",[t._v("解压："),e("code",{pre:!0},[t._v("hadoop fs -cp har:///xxx.har/* <path>")])])])]),t._v(" "),e("li",[t._v("回收站，防止误删除。默认关闭\n"),e("ol",[e("li",[e("code",{pre:!0},[t._v("fs.trash.interval")]),t._v("默认值为0表示禁用，其他值表示文件存活时间（分钟）")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("fs.trash.checkpoint.interval")]),t._v("表示检查回收站的间隔。如果为0则此值和"),e("code",{pre:!0},[t._v("fs.trash.interval")]),t._v("相等")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("fs.trash.checkpoint.interval")]),t._v("应不大于"),e("code",{pre:!0},[t._v("fs.trash.interval")])]),t._v(" "),e("li",[t._v("默认回收站用户是"),e("code",{pre:!0},[t._v("dr.who")])]),t._v(" "),e("li",[t._v("回收站路径："),e("code",{pre:!0},[t._v("/user/xxx/.Trash")]),t._v("，会按照日期把文件进行归类")]),t._v(" "),e("li",[t._v("使用命令行"),e("code",{pre:!0},[t._v("rm")]),t._v("删除文件会自动进入回收站。编程调用API时，需要使用"),e("code",{pre:!0},[t._v("moveToTrash")]),t._v("才会放入回收站")]),t._v(" "),e("li",[t._v("清空回收站："),e("code",{pre:!0},[t._v("hadoop fs -expunge")]),t._v("。建议看下文档")])])]),t._v(" "),e("li",[t._v("快照管理\n"),e("ol",[e("li",[t._v("备份目录。增量快照，并不会复制所有文件")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs dfsadmin -allowSnapshot <path>")]),t._v("为指定目录启用快照（默认所有目录禁用）（启用后才可以创建快照）")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs dfsadmin -disallowSnapshot <path>")]),t._v("为指定目录禁用快照")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs dfs -createSnapshot <path>")]),t._v("对目录创建快照")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs dfs -createSnapshot <path> <name>")]),t._v("对目录创建带名称的快照")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs dfs -renameSnapshot <path> <old-name> <new-name>")]),t._v("重命名快照")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs lsSnapshottableDir")]),t._v("列出当前用户所有可以被创建快照的目录")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs snapshotDiff <path> <from> <to>")]),t._v("比较两个快照目录的不同。"),e("strong",[t._v("重要")]),t._v("。快照就是用来进行文件比较的")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs dfs -deleteSnapshot <path> <snapshot-name>")]),t._v("删除快照")])])])]),t._v(" "),e("h3",{attrs:{id:"mapreduce"}},[t._v("MapReduce")]),t._v(" "),e("h4",{attrs:{id:"mr概述"}},[t._v("MR概述")]),t._v(" "),e("p",[t._v("组件：")]),t._v(" "),e("ul",[e("li",[t._v("Map\n"),e("ul",[e("li",[t._v("并行处理数据")]),t._v(" "),e("li",[t._v("读数据，默认按行进行处理，得到k-v对（行数-字符串）")]),t._v(" "),e("li",[t._v("对每一对k-v数据执行map方法（MapTask进程），输出格式也是k-v，我们假设输出为ko-vo")])])]),t._v(" "),e("li",[t._v("Reduce\n"),e("ul",[e("li",[t._v("reduce的输入数据类型对应map的输出数据类型")]),t._v(" "),e("li",[t._v("对Map的结果进行汇总，得到"),e("code",{pre:!0},[t._v("ko -> [vo1, vo2, ...]")])]),t._v(" "),e("li",[t._v("reducer对每一组相同的k-v调用一次reduce方法")])])]),t._v(" "),e("li",[t._v("Driver\n"),e("ul",[e("li",[t._v("配置MR任务信息")])])])]),t._v(" "),e("p",[t._v("其他细节：")]),t._v(" "),e("ul",[e("li",[t._v("是一个分布式计算编程框架")]),t._v(" "),e("li",[t._v("优点\n"),e("ul",[e("li",[t._v("易于编程，提供众多接口，程序是串行逻辑，MR负责并行执行")]),t._v(" "),e("li",[t._v("易于扩展，增减节点")]),t._v(" "),e("li",[t._v("高容错，节点down的时候自动把任务重新分配到新的节点")]),t._v(" "),e("li",[t._v("适合PB级数据离线处理")])])]),t._v(" "),e("li",[t._v("缺点\n"),e("ul",[e("li",[t._v("不擅长实时计算，返回结果速度很慢")]),t._v(" "),e("li",[t._v("不擅长流式计算，只能输入静态数据")]),t._v(" "),e("li",[t._v("不擅长DAG（有向无环图）计算，即，前一个运算输出作为后一个运算的输入。因为每个MR作业的输出结果会写入磁盘造成大量IO，导致性能低下")])])])]),t._v(" "),e("h4",{attrs:{id:"mr原理"}},[t._v("MR原理")]),t._v(" "),e("h5",{attrs:{id:"切片与mr并行度"}},[t._v("切片与MR并行度")]),t._v(" "),e("ul",[e("li",[t._v("数据块\n"),e("ul",[e("li",[t._v("HDFS从物理上把数据分成的块")])])]),t._v(" "),e("li",[t._v("数据切片\n"),e("ul",[e("li",[t._v("逻辑分片，并不会根据切片把数据存储在磁盘上")]),t._v(" "),e("li",[t._v("每个MapTask处理一个切片，可能需要跨数据块/跨DN读取数据分片")]),t._v(" "),e("li",[t._v("每个文件都会单独进行切片（如果单个文件大小大于数据块大小）")]),t._v(" "),e("li",[t._v("切片完毕后，把切片信息保存在一个切片规划文件中，并提交给YARN，由YARN的MrAppMaster开启对应的MapTask")]),t._v(" "),e("li",[t._v("切片机制\n"),e("ul",[e("li",[t._v("FileInputFormat\n"),e("ul",[e("li",[t._v("根据文件大小进行切片，不考虑文件内容")]),t._v(" "),e("li",[t._v("默认切片大小等于数据块的大小，避免传输时对数据块进行拆分或跨数据块读取\n"),e("ul",[e("li",[t._v("本地运行时切片默认32MB")]),t._v(" "),e("li",[t._v("集群模式下（由YARN调度），1.x默认64MB，2.x默认128MB")]),t._v(" "),e("li",[t._v("如果最后一个切片的长度不大于数据块大小的1.1倍，则不切片（尽量避免过小的切片）\n"),e("ul",[e("li",[t._v("比如本地运行模式数据块大小为32MB\n"),e("ul",[e("li",[t._v("一个33MB的文件会被保存在两个数据块，但是只会被标记为一个切片，因为"),e("code",{pre:!0},[t._v("33")]),t._v("小于"),e("code",{pre:!0},[t._v("32*1.1")])]),t._v(" "),e("li",[t._v("一个65MB的文件会被保存在三个数据块，但是只会被切割为两个切片，一个32MB，一个33MB")]),t._v(" "),e("li",[t._v("一个70MB的文件会被保存在三个数据块，也会被切割为三个切片，分别是32/32/6MB")])])])])])])])])]),t._v(" "),e("li",[t._v("TextInputFormat（默认）\n"),e("ul",[e("li",[t._v("按行读取")]),t._v(" "),e("li",[t._v("key是该行在整个文件中的字节偏移量，LongWritable")]),t._v(" "),e("li",[t._v("value是这一行的内容，不包括回车换行，Text")])])]),t._v(" "),e("li",[t._v("KeyValueTextInputFormat\n"),e("ul",[e("li",[t._v("按行读取，根据某个分隔符把行的内容处理为k-v对，默认分隔符为制表符\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v('conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, "\\t");')])])])]),t._v(" "),e("li",[t._v("key和value都是Text类型")])])]),t._v(" "),e("li",[t._v("NLineInputFormat\n"),e("ul",[e("li",[t._v("指定每个切片由多少行组成")]),t._v(" "),e("li",[t._v("k-v和TextInputFormat一样")])])]),t._v(" "),e("li",[t._v("CombineTextInputFormat\n"),e("ul",[e("li",[t._v("如果小文件很多，使用FileInputFormat就会导致过多的切片")]),t._v(" "),e("li",[t._v("使用CombineTextInputFormat可以合并小文件形成切片，从而解决此问题")])])]),t._v(" "),e("li",[t._v("自定义InputFormat\n"),e("ul",[e("li",[t._v("继承"),e("code",{pre:!0},[t._v("FileInputFormat")])]),t._v(" "),e("li",[t._v("改写"),e("code",{pre:!0},[t._v("RecordReader")]),t._v("，")]),t._v(" "),e("li",[t._v("可以用来实现小文件的合并，比如有很多小文件，文件名是key，文件内容是value，可以写一个MR程序把他们聚合成新的文件，输出为SequenceFile，以后用SequenceFileInputFormat读取")])])]),t._v(" "),e("li",[t._v("类似地，还有OutputFormat，比如TextOutputFormat。此处略")])])])])]),t._v(" "),e("li",[t._v("MR并行度\n"),e("ul",[e("li",[t._v("决定了任务并发能力")]),t._v(" "),e("li",[t._v("由于启动MapTask也需要消耗时间，所以并不是并行度越高越好")]),t._v(" "),e("li",[t._v("切片数量决定了Map任务并行度。Reduce任务并行度需要我们手动设置（通常等于分区数量）")])])])]),t._v(" "),e("h5",{attrs:{id:"shuffle"}},[t._v("Shuffle")]),t._v(" "),e("p",[t._v("数据处理过程称之为Shuffle。包括分区、排序、合并、溢写、压缩")]),t._v(" "),e("ul",[e("li",[t._v("分区\n"),e("ul",[e("li",[t._v("HashPartitioner（默认分区策略）\n"),e("ul",[e("li",[t._v("根据k的hash值对ReduceTask数量取模得到\n"),e("ul",[e("li",[t._v("多个"),e("code",{pre:!0},[t._v("k->[v]")]),t._v("会保存在同一个分区")])])]),t._v(" "),e("li",[t._v("用户没法控制哪个key存储到哪个分区")])])]),t._v(" "),e("li",[t._v("支持自定义Partitioner\n"),e("ul",[e("li",[t._v("自定义类集成Partitioner即可")]),t._v(" "),e("li",[t._v("k-v对应map输出的k-v")]),t._v(" "),e("li",[t._v("在Job驱动中设置自定义Partitioner: "),e("code",{pre:!0},[t._v("job.setPartitionerClass(XXX.class)")])]),t._v(" "),e("li",[t._v("根据自定义Partitioner的逻辑，设置相应的ReduceTask数量（默认值为1）\n"),e("ul",[e("li",[t._v("如果ReduceTask数量为0，则没有Reduce阶段，输出文件个数和Map个数一致")]),t._v(" "),e("li",[t._v("如果ReduceTask数量为1，它负责处理所有的分区")]),t._v(" "),e("li",[t._v("如果ReduceTask数量大于分区数量，则多出来的ReduceTask不会处理分区，输出空文件")]),t._v(" "),e("li",[t._v("如果ReduceTask数量大于1，小于分区数量，报错")]),t._v(" "),e("li",[t._v("分区号从0开始")])])])])])])]),t._v(" "),e("li",[t._v("排序\n"),e("ul",[e("li",[t._v("map阶段和reduce阶段都有排序\n"),e("ul",[e("li",[t._v("map阶段，环形缓冲区达到80%，落盘之前会排序。快排实现字典序")]),t._v(" "),e("li",[t._v("一个map任务结束后，使用归并排序")]),t._v(" "),e("li",[t._v("reduce拉数据之后，对不同map任务 数据进行归并排序")])])]),t._v(" "),e("li",[t._v("排序需求的分类\n"),e("ul",[e("li",[t._v("部分排序\n"),e("ul",[e("li",[t._v("根据k对数据排序，保证输出的每个文件（分区）有序")])])]),t._v(" "),e("li",[t._v("全排序\n"),e("ul",[e("li",[t._v("把ReduceTask数量设置为1，最后输出的结果只有一个文件。处理大型文件时效率低，无法并行")])])]),t._v(" "),e("li",[t._v("辅助排序(Grouping Comparator分组排序)\n"),e("ul",[e("li",[t._v("在Reduce端对k进行分组，使相同的k进入同一个reduce方法")])])]),t._v(" "),e("li",[t._v("X次排序\n"),e("ul",[e("li",[t._v("在自定义排序过程中，如果compareTo中的判断条件有X个，就是X次排序（希尔排序）")])])])])]),t._v(" "),e("li",[t._v("重写k对象的WritableComparable接口即可实现排序")])])]),t._v(" "),e("li",[t._v("合并\n"),e("ul",[e("li",[t._v("是一种优化手段。原理是局部汇总以减少网络IO")]),t._v(" "),e("li",[t._v("Combiner是Mapper/Reducer之外的组件（插件）")]),t._v(" "),e("li",[t._v("Combiner组件的父类的Reducer")]),t._v(" "),e("li",[t._v("Combiner和Reducer的区别是运行的位置\n"),e("ul",[e("li",[t._v("Reducer接收全局所有Mapper的输出结果后运行")]),t._v(" "),e("li",[t._v("Combiner是在每一个MapTask所在的节点运行")])])])])])]),t._v(" "),e("pre",{staticClass:"language-java"},[e("code",{pre:!0,attrs:{class:"language-java"}},[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("HashPartitioner")]),e("span",{pre:!0,attrs:{class:"token generics"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("K")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("V")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("extends")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Partitioner")]),e("span",{pre:!0,attrs:{class:"token generics"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("K")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("V")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("getPartition")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("K")]),t._v(" key"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("V")]),t._v(" value"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" numReduceTasks"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("key"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("hashCode")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Integer")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MAX_VALUE"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" numReduceTasks"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])]),t._v(" "),e("h4",{attrs:{id:"mr进程"}},[t._v("MR进程")]),t._v(" "),e("p",[t._v("MR运行时有三种进程")]),t._v(" "),e("ul",[e("li",[t._v("MrAppMaster\n"),e("ul",[e("li",[t._v("程序调度、状态协调，是一个job的主控")])])]),t._v(" "),e("li",[t._v("MapTask\n"),e("ul",[e("li",[t._v("整个map阶段的数据处理")])])]),t._v(" "),e("li",[t._v("ReduceTask\n"),e("ul",[e("li",[t._v("整个reduce阶段的数据处理")])])])]),t._v(" "),e("h4",{attrs:{id:"mr数据类型与序列化"}},[t._v("MR数据类型与序列化")]),t._v(" "),e("p",[t._v("序列化：把内存中的对象转换为字节序列，以便持久化存储与通信")]),t._v(" "),e("p",[t._v("Java自身的序列化是一个重量级的序列化框架，一个对象序列化之后会附加很多额外的信息（校验、header、继承体系等等）")]),t._v(" "),e("p",[t._v("Hadoop使用自己的数据类型进行轻量序列化")]),t._v(" "),e("ul",[e("li",[t._v("紧凑高效")]),t._v(" "),e("li",[t._v("读写开销小")]),t._v(" "),e("li",[t._v("支持多语言交互")]),t._v(" "),e("li",[t._v("可扩展，可以随着通信协议的升级而升级")])]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Java")]),t._v(" "),e("th",[t._v("Hadoop Writable")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("boolean")]),t._v(" "),e("td",[t._v("BooleanWritable")])]),t._v(" "),e("tr",[e("td",[t._v("byte")]),t._v(" "),e("td",[t._v("ByteWritable")])]),t._v(" "),e("tr",[e("td",[t._v("int")]),t._v(" "),e("td",[t._v("IntWritable")])]),t._v(" "),e("tr",[e("td",[t._v("float")]),t._v(" "),e("td",[t._v("FloatWritable")])]),t._v(" "),e("tr",[e("td",[t._v("long")]),t._v(" "),e("td",[t._v("LongWritable")])]),t._v(" "),e("tr",[e("td",[t._v("double")]),t._v(" "),e("td",[t._v("DoubleWritable")])]),t._v(" "),e("tr",[e("td",[t._v("map")]),t._v(" "),e("td",[t._v("MapWritable")])]),t._v(" "),e("tr",[e("td",[t._v("array")]),t._v(" "),e("td",[t._v("ArrayWritable")])]),t._v(" "),e("tr",[e("td",[t._v("String")]),t._v(" "),e("td",[t._v("Text")])])])]),t._v(" "),e("p",[t._v("除了上述已有的数据类型，也可以自己开发可序列化对象")]),t._v(" "),e("pre",{staticClass:"language-java"},[e("code",{pre:!0,attrs:{class:"language-java"}},[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Test")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("implements")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Writable")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 成员变量")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("private")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("long")]),t._v(" v1"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("private")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("long")]),t._v(" v2"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("private")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("long")]),t._v(" v3"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 默认构造函数，其他类通过反射创建此类的对象时会使用")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Test")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("super")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 序列化")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Override")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("write")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("DataOutput")]),t._v(" out"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOException")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    out"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("writeLong")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v1"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    out"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("writeLong")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v2"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    out"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("writeLong")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v3"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 反序列化")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Override")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("readFields")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("DataInput")]),t._v(" in"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOException")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 顺序和序列化时的顺序必须相同")]),t._v("\n    v1 "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" in"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("readLong")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    v2 "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" in"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("readLong")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    v3 "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" in"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("readLong")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 如果需要写到文件，需要实现toString方法")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Override")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("toString")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"xxx"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])]),t._v(" "),e("h4",{attrs:{id:"压缩"}},[t._v("压缩")]),t._v(" "),e("h5",{attrs:{id:"压缩概述"}},[t._v("压缩概述")]),t._v(" "),e("ul",[e("li",[t._v("用来减少HDFS的IO，提升网络带宽&磁盘空间利用率。")]),t._v(" "),e("li",[t._v("压缩基本原则：IO密集型适合压缩，计算密集型不适合压缩")])]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("压缩格式")]),t._v(" "),e("th",[t._v("hadoop自带")]),t._v(" "),e("th",[t._v("算法")]),t._v(" "),e("th",[t._v("扩展名")]),t._v(" "),e("th",[t._v("可切分")]),t._v(" "),e("th",[t._v("是否需要修改程序")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("DEFLATE")]),t._v(" "),e("td",[t._v("Y")]),t._v(" "),e("td",[t._v("DEFLATE")]),t._v(" "),e("td",[t._v(".deflate")]),t._v(" "),e("td",[t._v("N")]),t._v(" "),e("td",[t._v("N")])]),t._v(" "),e("tr",[e("td",[t._v("Gzip")]),t._v(" "),e("td",[t._v("Y")]),t._v(" "),e("td",[t._v("DEFLATE")]),t._v(" "),e("td",[t._v(".gz")]),t._v(" "),e("td",[t._v("N")]),t._v(" "),e("td",[t._v("N")])]),t._v(" "),e("tr",[e("td",[t._v("bzip2")]),t._v(" "),e("td",[t._v("Y")]),t._v(" "),e("td",[t._v("bzip2")]),t._v(" "),e("td",[t._v(".bz2")]),t._v(" "),e("td",[t._v("Y")]),t._v(" "),e("td",[t._v("N")])]),t._v(" "),e("tr",[e("td",[t._v("LZO")]),t._v(" "),e("td",[t._v("N")]),t._v(" "),e("td",[t._v("LZO")]),t._v(" "),e("td",[t._v(".lzo")]),t._v(" "),e("td",[t._v("Y")]),t._v(" "),e("td",[t._v("Y")])]),t._v(" "),e("tr",[e("td",[t._v("Snappy")]),t._v(" "),e("td",[t._v("N")]),t._v(" "),e("td",[t._v("Snappy")]),t._v(" "),e("td",[t._v(".snappy")]),t._v(" "),e("td",[t._v("N")]),t._v(" "),e("td",[t._v("N")])])])]),t._v(" "),e("h5",{attrs:{id:"如何选择压缩算法"}},[t._v("如何选择压缩算法")]),t._v(" "),e("ul",[e("li",[t._v("Gzip\n"),e("ul",[e("li",[t._v("优点：压缩率比较高，速度比较快，Hadoop自带，不需要改代码，大多数Linux自带Gzip，方便")]),t._v(" "),e("li",[t._v("缺点：不支持切分")]),t._v(" "),e("li",[t._v("场景：文件压缩后在130MB以内（一个块大小），可以考虑用Gzip。比如一天或者一个小时的日志压缩成一个Gzip文件")])])]),t._v(" "),e("li",[t._v("Bzip2\n"),e("ul",[e("li",[t._v("优点：支持切片，压缩率很高，Hadoop自带")]),t._v(" "),e("li",[t._v("缺点：压缩/解压慢")]),t._v(" "),e("li",[t._v("场景：对速度要求不高，但是对压缩率要求高。或者长期存档")])])]),t._v(" "),e("li",[t._v("Lzo\n"),e("ul",[e("li",[t._v("优点：速度快，支持切分，是Hadoop中最流行的压缩格式。可以在Linux系统安装lzop命令")]),t._v(" "),e("li",[t._v("缺点：压缩率低，Hadoop不自带，需要改代码")]),t._v(" "),e("li",[t._v("场景：很大的文本文件，压缩之后仍然大于200MB时可以考虑。单个文件越大，Lzo优势越明显")])])]),t._v(" "),e("li",[t._v("Snappy\n"),e("ul",[e("li",[t._v("优点：高速")]),t._v(" "),e("li",[t._v("缺点：不支持切分，压缩率低，Hadoop不自带")]),t._v(" "),e("li",[t._v("场景：Map输出文件较大，在Map和Reduce之间，或者级联作业之间")])])])]),t._v(" "),e("h5",{attrs:{id:"何时压缩"}},[t._v("何时压缩")]),t._v(" "),e("ul",[e("li",[t._v("输入数据是压缩格式\n"),e("ul",[e("li",[t._v("Hadoop自动检查文件扩展名，然后进行压缩/解压")])])]),t._v(" "),e("li",[t._v("Map之后，Reduce之前\n"),e("ul",[e("li",[t._v("如果Map输出的数据量大，需要考虑使用压缩。比如LZO/Snappy")])])]),t._v(" "),e("li",[t._v("Reduce输出时压缩\n"),e("ul",[e("li",[t._v("通常用于级联作业")])])])]),t._v(" "),e("h4",{attrs:{id:"join"}},[t._v("Join")]),t._v(" "),e("p",[t._v("类似于SQL里面的join")]),t._v(" "),e("p",[t._v("没有这个功能，但是通过巧妙的业务逻辑自己实现")]),t._v(" "),e("p",[t._v("可以在Reduce阶段进行join，也可以在Map阶段进行join")]),t._v(" "),e("h4",{attrs:{id:"工作流"}},[t._v("工作流")]),t._v(" "),e("h5",{attrs:{id:"宏观工作流"}},[t._v("宏观工作流")]),t._v(" "),e("ol",[e("li",[t._v("客户端在submit之前，根据job的配置，生成任务规划（切片信息）")]),t._v(" "),e("li",[t._v("客户端通过submit提交job，包括切片信息、jar包等配置")]),t._v(" "),e("li",[t._v("YARN调用ResourceManager创建MrAppMaster，根据切片信息创建MapTask")]),t._v(" "),e("li",[t._v("MapTask读取数据并处理map任务")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("context.write")]),t._v("会向一个环形缓冲区写数据（默认大小100MB）\n"),e("ol",[e("li",[t._v("半个环用来写元数据（比如数据的起始位置、长度、分区等），半个环用来写数据")]),t._v(" "),e("li",[t._v("写到80%容量的时候，对每个分区的数据根据k使用快排进行排序（字典序），然后把缓冲区数据落盘然后反向继续写。比如分区1有两个k，分别是1和2，分区2有两个k，分别是3和4，那么每次输出应该类似于"),e("code",{pre:!0},[t._v("p1(1->[v], 2->[v]), p2(3->[v], 4->[v])")])]),t._v(" "),e("li",[t._v("归并排序单个task的所有输出，得到多个数据分区，类似于"),e("code",{pre:!0},[t._v("p1(1->[v], 2->[v]), p2(3->[v], 4->[v])")])])])]),t._v(" "),e("li",[t._v("MrAppMaster等待所有map任务执行完毕后，根据所有map任务的不同分区数量启动ReduceTask")]),t._v(" "),e("li",[t._v("ReduceTask把MapTask的结果下载到本地，使用归并排序合并不同task相同分区的结果，然后交给Reducer处理")])]),t._v(" "),e("p",[t._v("以下两个函数是"),e("code",{pre:!0},[t._v("Mapper")]),t._v("类和"),e("code",{pre:!0},[t._v("Reducer")]),t._v("类的"),e("code",{pre:!0},[t._v("run")]),t._v("方法")]),t._v(" "),e("pre",{staticClass:"language-java"},[e("code",{pre:!0,attrs:{class:"language-java"}},[e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Mapper workflow")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("run")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Context")]),t._v(" context"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOException")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("InterruptedException")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("setup")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("context"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("try")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("context"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("netKeyValue")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("map")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("context"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("getCurrentKey")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" context"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("getCurrentValue")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" context"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("finally")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("cleanup")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("context"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Reducer workflow")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("run")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Context")]),t._v(" context"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOException")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("InterruptedException")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("setup")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("context"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("try")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("context"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("nextKey")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("reduce")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("context"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("getCurrentKey")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" context"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("getValues")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" context"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Iterator")]),e("span",{pre:!0,attrs:{class:"token generics"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("VALUEIN"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" iter "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" context"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("getValues")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("iterator")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("iter "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("instanceof")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("ReduceContext"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ValueIterator")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("ReduceContext"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ValueIterator")]),e("span",{pre:!0,attrs:{class:"token generics"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("VALUEIN"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("iter"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("resetBackupStore")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("finally")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("cleanup")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("context"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])]),t._v(" "),e("h5",{attrs:{id:"详细maptask工作流"}},[t._v("详细MapTask工作流")]),t._v(" "),e("ul",[e("li",[t._v("Read阶段\n"),e("ul",[e("li",[t._v("根据InputFormat读取数据")])])]),t._v(" "),e("li",[t._v("Map阶段\n"),e("ul",[e("li",[t._v("根据Mapper计算数据")])])]),t._v(" "),e("li",[t._v("Collet阶段\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("context.write")]),t._v("写到"),e("code",{pre:!0},[t._v("outputCollector")]),t._v("，也就是环形缓冲区")])])]),t._v(" "),e("li",[t._v("溢写阶段\n"),e("ul",[e("li",[t._v("环形缓冲区需要落盘的时候溢写阶段")])])]),t._v(" "),e("li",[t._v("Combine阶段\n"),e("ul",[e("li",[t._v("溢写完毕，把已经落盘的文件进行合并")])])])]),t._v(" "),e("h5",{attrs:{id:"详细reducetask工作流"}},[t._v("详细ReduceTask工作流")]),t._v(" "),e("ul",[e("li",[t._v("Copy阶段\n"),e("ul",[e("li",[t._v("把MapTask的结果文件拷贝到Reducer节点")])])]),t._v(" "),e("li",[t._v("Merge & Sort阶段\n"),e("ul",[e("li",[t._v("合并MapTask的结果")]),t._v(" "),e("li",[t._v("分组，"),e("code",{pre:!0},[t._v("GroupingComparator")])])])]),t._v(" "),e("li",[t._v("Reduce阶段\n"),e("ul",[e("li",[t._v("根据Reducer实现业务逻辑")])])])]),t._v(" "),e("h4",{attrs:{id:"示例项目"}},[t._v("示例项目")]),t._v(" "),e("p",[t._v("输入文件有若干行，每一行有若干单词，单词之间由空格进行分割，输出以26个字母开头的单词的总字母数")]),t._v(" "),e("p",[t._v("示例输入文件：")]),t._v(" "),e("pre",{staticClass:"language-bash"},[e("code",{pre:!0,attrs:{class:"language-bash"}},[t._v("once upon a "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("time")]),t._v(" there was a bug "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" the browser\n"),e("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("test")]),t._v(" suite that caused the browser to crash when the user clicks on the button to start\n")])]),t._v(" "),e("p",[t._v("示例输出文件：")]),t._v(" "),e("pre",{staticClass:"language-bash"},[e("code",{pre:!0,attrs:{class:"language-bash"}},[t._v("a "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# len("a") + len("a")')]),t._v("\nb "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("23")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# len("bug") + len("browser") + len("browser") + len("button")')]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("..")]),t._v(".\n")])]),t._v(" "),e("pre",{staticClass:"language-java"},[e("code",{pre:!0,attrs:{class:"language-java"}},[e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("/*\nDriver, 用来配置任务\n*/")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("TestDriver")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("static")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("main")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" args"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOException")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 初始化配置 & job")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Configuration")]),t._v(" conf "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Configuration")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Job")]),t._v(" job "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Job")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("getInstance")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("conf"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    \n    "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 基于反射获取Driver的jar包位置")]),t._v("\n    job"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("setJarByClass")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("TestDriver")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    \n    "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 基于反射获取Mapper/Reducer的jar包位置")]),t._v("\n    job"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("setMapperClass")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("TestMapper")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    job"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("setReducerClass")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("TestReducer")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n    "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 设置Mapper/Reducer输出格式")]),t._v("\n    job"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("setMapOutputKeyClass")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    job"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("setMapOutputValueClass")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    job"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("setOutputKeyClass")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    job"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("setOutputValueClass")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n    "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 设置输入输出路径")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FileInputFormat")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("setInputPaths")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("job"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Path")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/path/to/input"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 输入通常是args[0]")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FileOutputFormat")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("setOutputPaths")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("job"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Path")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/path/to/output"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 输出通常是args[1]")]),t._v("\n\n    "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 提交job")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("boolean")]),t._v(" result "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" job"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("waitForCompletion")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 冗余输出")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// System.exit(result ? 0 : 1);")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("/*\nMapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT>\n*/")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("TestMapper")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("extends")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Mapper")]),e("span",{pre:!0,attrs:{class:"token generics"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("LongWritable")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),t._v(" keyOut "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),t._v(" valueOut "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// key是偏移量，value是一行文本")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Override")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("protected")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("map")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("LongWritable")]),t._v(" key"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),t._v(" value"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Context")]),t._v(" context"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOException")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("InterruptedException")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" words "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" value"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("toString")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("split")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('" "')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 根据空格拆分单词")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),t._v(" word "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" words"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      keyOut"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("set")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("word"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 以单词首字母作为输出的key")]),t._v("\n      valueOut"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("set")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("word"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("length")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 以单词长度作为输出的value")]),t._v("\n      context"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("write")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("keyOut"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" valueOut"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// output (首字母, 长度)")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("/*\nReducer<KEYIN, VALUEIN, KEYOUT, VALUEOUT>\n*/")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("TestReducer")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("extends")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Reducer")]),e("span",{pre:!0,attrs:{class:"token generics"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),t._v(" valueOut "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// key是单词首字母，values是相同首字母的单词的长度")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Override")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("protected")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("reduce")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),t._v(" key"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Iterable")]),e("span",{pre:!0,attrs:{class:"token generics"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" values"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Context")]),t._v(" context"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOException")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("InterruptedException")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" sum "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),t._v(" value "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" values"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      sum "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" value"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("get")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    valueOut"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("set")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sum"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    context"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("write")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("key"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" valueOut"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// output (首字母, 总长度)")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])]),t._v(" "),e("h4",{attrs:{id:"管理"}},[t._v("管理")]),t._v(" "),e("ul",[e("li",[t._v("web ui\n"),e("ul",[e("li",[t._v("在8088端口有web ui界面")]),t._v(" "),e("li",[t._v("在19888端口有job history的web ui界面")])])]),t._v(" "),e("li",[t._v("计数器\n"),e("ul",[e("li",[t._v("Hadoop为每个job内置若干计数器\n"),e("ul",[e("li",[t._v("已处理字节数、记录数、输入数据量、输出数据量")])])]),t._v(" "),e("li",[t._v("可以自定义计数器，用来debug\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("context.getCounter(XXX).increment(1);")])]),t._v(" "),e("li",[t._v("还可以分组管理计数器")])])])])])]),t._v(" "),e("h4",{attrs:{id:"etl"}},[t._v("ETL")]),t._v(" "),e("p",[t._v("执行核心MR任务之前通常需要进行数据清洗。通常只需要map不需要reduce")]),t._v(" "),e("h3",{attrs:{id:"yarn"}},[t._v("Yarn")]),t._v(" "),e("h4",{attrs:{id:"组成部分"}},[t._v("组成部分")]),t._v(" "),e("ul",[e("li",[t._v("ResourceManager(RM)\n"),e("ul",[e("li",[t._v("是整个集群的主控")]),t._v(" "),e("li",[t._v("处理客户端请求")]),t._v(" "),e("li",[t._v("监控NM")]),t._v(" "),e("li",[t._v("启动或监控AM")]),t._v(" "),e("li",[t._v("资源的分配和调度")])])]),t._v(" "),e("li",[t._v("NodeManager(NM)\n"),e("ul",[e("li",[t._v("管理单个节点上的资源（相当于一个agent）")]),t._v(" "),e("li",[t._v("汇报状态给RM")]),t._v(" "),e("li",[t._v("处理RM的命令")]),t._v(" "),e("li",[t._v("处理AM的命令")])])]),t._v(" "),e("li",[t._v("ApplicationMaster(AM)\n"),e("ul",[e("li",[t._v("负责数据的切分")]),t._v(" "),e("li",[t._v("为应用程序申请资源")]),t._v(" "),e("li",[t._v("任务的监控和容错")])])]),t._v(" "),e("li",[t._v("Container\n"),e("ul",[e("li",[t._v("是Yarn中资源的抽象")]),t._v(" "),e("li",[t._v("封装了某个节点上的多维度资源\n"),e("ul",[e("li",[t._v("内存")]),t._v(" "),e("li",[t._v("CPU")]),t._v(" "),e("li",[t._v("磁盘")]),t._v(" "),e("li",[t._v("网络")])])])])])]),t._v(" "),e("h4",{attrs:{id:"工作流-1"}},[t._v("工作流")]),t._v(" "),e("ol",[e("li",[t._v("客户端提交任务到ResourceManager。即使用YarnRunner之后执行"),e("code",{pre:!0},[t._v("job.waitForCompletion()")])]),t._v(" "),e("li",[t._v("ResourceManager返回Application资源提交路径（HDFS路径，"),e("code",{pre:!0},[t._v("hdfs://xxx/.staging")]),t._v("）和application_id")]),t._v(" "),e("li",[t._v("客户端把数据提交到HDFS。包括切片信息"),e("code",{pre:!0},[t._v("Job.split")]),t._v("，配置文件"),e("code",{pre:!0},[t._v("Job.xml")]),t._v("和Hadoop程序的jar包"),e("code",{pre:!0},[t._v("xxx.jar")]),t._v("。这些文件都是在客户端"),e("code",{pre:!0},[t._v("job.submit()")]),t._v("之后生成的")]),t._v(" "),e("li",[t._v("资源提交完毕，客户端向ResourceManager请求运行mrAppMaster")]),t._v(" "),e("li",[t._v("ResourceManager会把请求初始化为一个Task，进入一个调度器（详见下文）")]),t._v(" "),e("li",[t._v("空闲节点的NodeManager从ResourceManager领取任务")]),t._v(" "),e("li",[t._v("NodeManager在节点上创建容器，分配CPU和内存，启动MRAppMaster")]),t._v(" "),e("li",[t._v("Container下载资源到本地")]),t._v(" "),e("li",[t._v("Container向ResourceManager申请运行MapTask Container")]),t._v(" "),e("li",[t._v("其他空闲节点的NodeManager从ResourceManager领取MapTask任务，创建容器，分配CPU和内存")]),t._v(" "),e("li",[t._v("MRAppMaster节点发送启动脚本到YarnChild执行MapTask")]),t._v(" "),e("li",[t._v("MapTask执行完毕后，MRAppMaster根据分片数量，向ResourceManager申请运行ReduceTask")]),t._v(" "),e("li",[t._v("其他空闲节点的NodeManager从ResourceManager领取ReduceTask，并从MapTask节点拷贝Map结果数据")]),t._v(" "),e("li",[t._v("整个MR工作结束后，MRAppMaster会通知ResourceManager注销任务")])]),t._v(" "),e("h4",{attrs:{id:"调度器"}},[t._v("调度器")]),t._v(" "),e("ul",[e("li",[t._v("目前调度器有三种：\n"),e("ul",[e("li",[t._v("FIFO队列\n"),e("ul",[e("li",[t._v("一旦有节点空闲，向ResourceManager领取任务，ResourceManager会按照FIFO顺序把第一个Job里面的一个MapTask或ReduceTask分配给节点")])])]),t._v(" "),e("li",[t._v("Capacity Scheduler容量调度器（Hadoop 2.7.2默认）\n"),e("ul",[e("li",[t._v("多个队列。每个队列有一个资源比例。比如3个队列，资源比例是20%/30%/50%")]),t._v(" "),e("li",[t._v("为了防止同一个用户独占所有资源，调度器会对同一个用户提交的作业所占资源量进行限定")]),t._v(" "),e("li",[t._v("计算每个队列中正在执行的Job数量和队列资源的比值。比值最小的队列最闲")]),t._v(" "),e("li",[t._v("结合作业优先级、提交时间、用户资源量限制、内存限制等因素，对队列内的任务进行排序")]),t._v(" "),e("li",[t._v("多个队列并行处理，互不影响。单个队列同时只能处理一个Job")])])]),t._v(" "),e("li",[t._v("Fair Scheduler公平调度器\n"),e("ul",[e("li",[t._v("多个队列，按比例分配资源")]),t._v(" "),e("li",[t._v("单个队列里面的Job可以同时执行")]),t._v(" "),e("li",[t._v("根据优先级给Job分配资源。雨露均沾，不存在没有资源的Job。每个Job需要的资源和实际获得的资源存在缺额")]),t._v(" "),e("li",[t._v("同一个队列中，Job的资源缺额越大，越先获得资源")]),t._v(" "),e("li",[t._v("并发能力极强。如果机器的性能很高，可以使用这种调度器")])])])])])]),t._v(" "),e("h4",{attrs:{id:"推测任务"}},[t._v("推测任务")]),t._v(" "),e("p",[t._v("如果某个任务的执行时间明显高于平均值，一直不结束，如何判断任务是在执行，还是因为硬件/软件原因导致执行失败呢？（停机问题）")]),t._v(" "),e("p",[t._v("可以配置推测任务，即执行一个相同的任务。取先执行完的任务的结果作为结果")]),t._v(" "),e("p",[t._v("前提条件：")]),t._v(" "),e("ul",[e("li",[t._v("每个Task只能有一个备份任务/推测任务")]),t._v(" "),e("li",[t._v("当前Job已完成的Task数量不小于5%")]),t._v(" "),e("li",[t._v("可以在配置文件里面配置参数来启动\n"),e("ul",[e("li",[t._v("在"),e("code",{pre:!0},[t._v("mapred-site.xml")]),t._v("中")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("<name>mapreduce.reduce.speculative</name>")])]),t._v(" "),e("li",[t._v("默认启动")])])])]),t._v(" "),e("p",[t._v("以下情况不要启用推测任务：")]),t._v(" "),e("ul",[e("li",[t._v("非幂等的任务，比如写数据库")]),t._v(" "),e("li",[t._v("任务存在严重负载倾斜，导致某个任务的负载特别高。启动推测任务可能进一步增加原任务的执行时间")])]),t._v(" "),e("pre",{staticClass:"language-python"},[e("code",{pre:!0,attrs:{class:"language-python"}},[e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 计算")]),t._v("\nestimatedRunTime "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("currentTimestamp "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" taskStartTime"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" progress "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 推测执行时长")]),t._v("\nestimatedEndTime "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" estimatedRunTime "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" taskStartTime "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 推测当前任务完成时间")]),t._v("\nbackupEstimateEndTime "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" currentTimestamp "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" averageRunTime "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 备份任务推测完成时间")]),t._v("\n")])]),t._v(" "),e("ul",[e("li",[t._v("MR总是选择"),e("code",{pre:!0},[t._v("estimatedEndTime - backupEstimateEndTime")]),t._v("最大的任务启动备份任务")]),t._v(" "),e("li",[t._v("为了防止大量的任务同时启动备份任务，MR为每个作业设置了同时启动的备份任务数量上限")])]),t._v(" "),e("h3",{attrs:{id:"目录结构"}},[t._v("目录结构")]),t._v(" "),e("ul",[e("li",[e("code",{pre:!0},[t._v("bin")]),t._v("存放二进制文件\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("hadoop")])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs")])])])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("sbin")]),t._v("存放hadoop集群管理脚本\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("hadoop-daemon.sh")])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("slaves.sh")])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("start-*.sh")])])])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("etc")]),t._v("存放配置文件\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("*-env.sh")])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("*.xml")])])])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("include")]),t._v("存放C语言头文件"),e("code",{pre:!0},[t._v(".h")])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("lib")]),t._v("本地库，包括"),e("code",{pre:!0},[t._v(".a")]),t._v("/"),e("code",{pre:!0},[t._v(".so")]),t._v("文件。未来可以用来添加功能")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("libexec")]),t._v("类似于"),e("code",{pre:!0},[t._v("lib")])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("share")]),t._v("存放文档、示例程序")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("data")]),t._v("数据文件夹。格式化NameNode时产生")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("logs")]),t._v("日志文件夹")])]),t._v(" "),e("h2",{attrs:{id:"运行模式"}},[t._v("运行模式")]),t._v(" "),e("ul",[e("li",[t._v("本地模式\n"),e("ul",[e("li",[t._v("学习与测试")])])]),t._v(" "),e("li",[t._v("伪分布式\n"),e("ul",[e("li",[t._v("在单机按照分布式的配置进行搭建")]),t._v(" "),e("li",[t._v("学习与测试")])])]),t._v(" "),e("li",[t._v("完全分布式\n"),e("ul",[e("li",[t._v("生产环境")])])])]),t._v(" "),e("h2",{attrs:{id:"配置"}},[t._v("配置")]),t._v(" "),e("h3",{attrs:{id:"配置文件"}},[t._v("配置文件")]),t._v(" "),e("p",[t._v("配置文件有两类：默认配置文件、自定义配置文件")]),t._v(" "),e("ul",[e("li",[t._v("默认配置文件，在jar包里面，保存了所有配置的默认值\n"),e("ul",[e("li",[t._v("core-default.xml")]),t._v(" "),e("li",[t._v("hdfs-default.xml")]),t._v(" "),e("li",[t._v("yarn-default.xml")]),t._v(" "),e("li",[t._v("mapreduce-default.xml")])])]),t._v(" "),e("li",[t._v("自定义配置文件，用来覆盖默认配置。可以存在于集群上，也可以存在于客户端。客户端的配置优先级高于集群上的配置\n"),e("ul",[e("li",[t._v("core-site.xml\n"),e("ul",[e("li",[t._v("NameNode的主机名称和端口号")]),t._v(" "),e("li",[t._v("数据存储路径")])])]),t._v(" "),e("li",[t._v("hdfs-site.xml\n"),e("ul",[e("li",[t._v("副本数量")])])]),t._v(" "),e("li",[t._v("yarn-site.xml\n"),e("ul",[e("li",[t._v("NodeManager配置")]),t._v(" "),e("li",[t._v("ResourceManager配置")]),t._v(" "),e("li",[t._v("日志和日志聚集")])])]),t._v(" "),e("li",[t._v("mapreduce-site.xml\n"),e("ul",[e("li",[t._v("配置MapReduce在YARN上执行程序")])])])])])]),t._v(" "),e("h3",{attrs:{id:"启动与停止集群"}},[t._v("启动与停止集群")]),t._v(" "),e("ul",[e("li",[t._v("各个组件逐一启动/停止\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("hadoop-daemon.sh start/stop namenode/datanode/secondarynamenode")])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("yarn-daemon.sh start/stop resourcemanager/nodemanager")])])])]),t._v(" "),e("li",[t._v("整个模块一起启动（需要配置节点之间的SSH免密登录）。"),e("strong",[t._v("常用")]),t._v(" "),e("ul",[e("li",[e("code",{pre:!0},[t._v("start-dfs.sh/stop-dfs.sh")])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("start-yarn.sh/stop-yarn.sh")])])])]),t._v(" "),e("li",[t._v("全部启动（官方不建议使用）\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("start-all.sh/stop-all.sh")])])])])]),t._v(" "),e("h3",{attrs:{id:"常见流程"}},[t._v("常见流程")]),t._v(" "),e("ol",{attrs:{start:"0"}},[e("li",[t._v("配置集群时间同步。配置定时任务，周期性使用NTP同步时间")]),t._v(" "),e("li",[t._v("配置"),e("code",{pre:!0},[t._v("hadoop-env.sh")]),t._v("，需要配置"),e("code",{pre:!0},[t._v("JAVA_HOME")]),t._v(" "),e("ol",[e("li",[t._v("可以直接执行"),e("code",{pre:!0},[t._v("echo $JAVA_HOME")]),t._v("获取当前环境变量")]),t._v(" "),e("li",[t._v("修改"),e("code",{pre:!0},[t._v("hadoop-env.sh")]),t._v("文件，配置"),e("code",{pre:!0},[t._v("JAVA_HOME")])])])]),t._v(" "),e("li",[t._v("修改"),e("code",{pre:!0},[t._v("core-site.xml")])])]),t._v(" "),e("pre",{staticClass:"language-xml"},[e("code",{pre:!0,attrs:{class:"language-xml"}},[e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("\x3c!-- 配置HDFS中NameNode的地址 --\x3e")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("fs.defaultFS"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("hdfs://xxxx"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("\x3c!-- 配置Hadoop运行时产生的临时文件的存储目录 --\x3e")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("hadoop.tmp.dir"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("/xxx/xxx"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n")])]),t._v(" "),e("ol",{attrs:{start:"3"}},[e("li",[t._v("修改"),e("code",{pre:!0},[t._v("hdfs-site.xml")]),t._v("（可选）")])]),t._v(" "),e("pre",{staticClass:"language-xml"},[e("code",{pre:!0,attrs:{class:"language-xml"}},[e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("\x3c!-- 指定HDFS副本数量 --\x3e")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("dfs.replication"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("3"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("\x3c!-- Secondary NameNode --\x3e")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("dfs.namenode.secondary.http_address"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("xxx"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n")])]),t._v(" "),e("ol",{attrs:{start:"4"}},[e("li",[t._v("配置"),e("code",{pre:!0},[t._v("yarn-env.sh")]),t._v("中的"),e("code",{pre:!0},[t._v("JAVA_HOME")])]),t._v(" "),e("li",[t._v("修改"),e("code",{pre:!0},[t._v("yarn-site.xml")])])]),t._v(" "),e("pre",{staticClass:"language-xml"},[e("code",{pre:!0,attrs:{class:"language-xml"}},[e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("\x3c!-- 配置reducer获取数据的方式 --\x3e")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("yarn.nodemanager.aux-services"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("mapreduce_shuffle"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("\x3c!-- 配置ResourceManager在哪个主机上 --\x3e")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("yarn.resourcemanager.hostname"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("xxx"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n")])]),t._v(" "),e("ol",{attrs:{start:"6"}},[e("li",[t._v("配置"),e("code",{pre:!0},[t._v("mapred-env.sh")]),t._v("中的"),e("code",{pre:!0},[t._v("JAVA_HOME")])]),t._v(" "),e("li",[t._v("修改"),e("code",{pre:!0},[t._v("mapred-site.xml")])])]),t._v(" "),e("pre",{staticClass:"language-xml"},[e("code",{pre:!0,attrs:{class:"language-xml"}},[e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("\x3c!-- 声明MapReduce运行在YARN上 --\x3e")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("mapreduce.framework.name"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("yarn"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("\x3c!-- 历史服务器地址 --\x3e")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("mapreduce.jobhistory.address"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("xxx.com:10020"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("\x3c!-- 历史服务器web端地址 --\x3e")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("mapreduce.jobhistory.webapp.address"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("xxx.com:19888"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n")])]),t._v(" "),e("ol",{attrs:{start:"8"}},[e("li",[t._v("修改"),e("code",{pre:!0},[t._v("etc/slaves")]),t._v("，添加从节点，每一行都是一个数据节点的域名，注意不要有空格和空行")]),t._v(" "),e("li",[t._v("把上述配置同步到集群的所有节点")]),t._v(" "),e("li",[t._v("检查所有节点的NameNode\n"),e("ol",[e("li",[t._v("执行"),e("code",{pre:!0},[t._v("jps")]),t._v("，确保NameNode/DataNode没有处于启动状态")]),t._v(" "),e("li",[t._v("删除"),e("code",{pre:!0},[t._v("data")]),t._v("目录和"),e("code",{pre:!0},[t._v("logs")]),t._v("目录")])])]),t._v(" "),e("li",[t._v("格式化NameNode: "),e("code",{pre:!0},[t._v("bin/hdfs namenode -format")])]),t._v(" "),e("li",[t._v("会生成一个集群ID")]),t._v(" "),e("li",[t._v("启动HDFS："),e("code",{pre:!0},[t._v("sbin/start-dfs.sh")]),t._v("，包括"),e("code",{pre:!0},[t._v("core-site.xml")]),t._v("/"),e("code",{pre:!0},[t._v("hdfs-site.xml")]),t._v("/"),e("code",{pre:!0},[t._v("slaves")]),t._v("中指定的所有DN/NN/2NN。此脚本相当于在不同节点执行如下脚本\n"),e("ol",[e("li",[t._v("在本机启动NameNode: "),e("code",{pre:!0},[t._v("sbin/hadoop-daemon.sh start namenode")])]),t._v(" "),e("li",[t._v("在本机启动DataNode: "),e("code",{pre:!0},[t._v("sbin/hadoop-daemon.sh start datanode")]),t._v(" "),e("ol",[e("li",[t._v("启动后会生成和NameNode一样的集群ID")]),t._v(" "),e("li",[t._v("此时如果再次格式化NameNode，就会得到新的集群ID，DataNode和NameNode就无法正常通信\n"),e("ol",[e("li",[t._v("解决方法：格式化NameNode之前先删除DataNode里面的数据")])])])])])])]),t._v(" "),e("li",[t._v("启动YARN："),e("code",{pre:!0},[t._v("sbin/start-yarn.sh")]),t._v("。此命令会在"),e("code",{pre:!0},[t._v("yarn-site.xml")]),t._v("指定的节点上执行如下命令\n"),e("ol",[e("li",[t._v("在本机启动ResourceManager: "),e("code",{pre:!0},[t._v("yarn-daemon.sh start resourcemanager")])]),t._v(" "),e("li",[t._v("在本机启动NodeManager: "),e("code",{pre:!0},[t._v("yarn-daemon.sh start nodemanager")]),t._v("。每个节点都会有NodeManager")])])]),t._v(" "),e("li",[t._v("启动jobhistory服务器: "),e("code",{pre:!0},[t._v("mr-jobhistory-daemon.sh start historyserver")])])]),t._v(" "),e("h2",{attrs:{id:"命令"}},[t._v("命令")]),t._v(" "),e("h3",{attrs:{id:"hadoop命令"}},[t._v("hadoop命令")]),t._v(" "),e("ul",[e("li",[e("code",{pre:!0},[t._v("hadoop jar <mr-jar-file> <params>")]),t._v("执行MR程序")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hadoop fs")]),t._v("操作HDFS\n"),e("ul",[e("li",[t._v("相当于"),e("code",{pre:!0},[t._v("hdfs dfs")]),t._v("命令。dfs是fs的实现类，即，"),e("code",{pre:!0},[t._v("hadoop fs")]),t._v("其实是在调用"),e("code",{pre:!0},[t._v("hdfs dfs")])])])])]),t._v(" "),e("h3",{attrs:{id:"hadoop-daemon命令"}},[t._v("hadoop-daemon命令")]),t._v(" "),e("ul",[e("li",[e("code",{pre:!0},[t._v("hadoop-daemon.sh start namenode")]),t._v("启动NameNode")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hadoop-daemon.sh start datanode")]),t._v("启动DataNode")])]),t._v(" "),e("h3",{attrs:{id:"hdfs命令"}},[t._v("hdfs命令")]),t._v(" "),e("ul",[e("li",[e("code",{pre:!0},[t._v("hdfs dfs -help <command>")]),t._v("查看帮助\n"),e("ul",[e("li",[t._v("例："),e("code",{pre:!0},[t._v("hdfs dfs -help ls")])])])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs dfs -<command>")]),t._v("执行类似于linux自带的文件系统命令\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("hdfs dfs -mkdir -p /123/123/123")]),t._v("创建目录")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs dfs -ls <dir>")]),t._v("列出目录")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs dfs -cat /xxx/xxx")]),t._v("输出文件")]),t._v(" "),e("li",[t._v("类似的命令："),e("code",{pre:!0},[t._v("chgrp")]),t._v("/"),e("code",{pre:!0},[t._v("chmod")]),t._v("/"),e("code",{pre:!0},[t._v("chown")]),t._v("/"),e("code",{pre:!0},[t._v("mv")]),t._v("/"),e("code",{pre:!0},[t._v("cp")]),t._v("/"),e("code",{pre:!0},[t._v("tail")]),t._v("/"),e("code",{pre:!0},[t._v("rm")])])])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs dfs -put <src> <dest>")]),t._v("把本地文件上传到HDFS（复制）\n"),e("ul",[e("li",[t._v("同样效果的命令："),e("code",{pre:!0},[t._v("copyFromLocal")])])])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs dfs -get <src> <dst>")]),t._v("把HDFS文件下载到本地（复制）\n"),e("ul",[e("li",[t._v("同样效果的命令："),e("code",{pre:!0},[t._v("copyToLocal")])])])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs dfs -moveFromLocal <src> <dst>")]),t._v("把本地文件上传到HDFS（剪切）")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs dfs -appendToFile <src> <dst>")]),t._v("把本地文件内容追加到HDFS中的文件")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs dfs -getmerge <...src> <dst>")]),t._v("合并下载文件到本地。可以用来合并日志")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs dfs -du <dir>")]),t._v("统计文件夹下每个dirent的大小\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("-h")]),t._v("易读模式输出")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("-s")]),t._v("输入指定文件夹的总大小，而不是每个dirent的大小")])])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs dfs -setrep <num> <file>")]),t._v("设置单个文件的副本数量")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs namenode -format")]),t._v("格式化NameNode，生成"),e("code",{pre:!0},[t._v("data")]),t._v("文件夹")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs oiv -p <processor> -i <input-file> -o <output-file>")]),t._v("离线查看fsimage\n"),e("ul",[e("li",[t._v("processor可以是XML")])])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs oev -p <processor> -i <input-file> -o <output-file>")]),t._v("离线查看edits")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("hdfs dfsadmin -safemode <command>")]),t._v("管理安全模式\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("get")]),t._v("查看安全模式")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("enter")]),t._v("进入安全模式")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("leave")]),t._v("离开安全模式")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("wait")]),t._v("阻塞shell，直到退出安全模式。通常用来写sh脚本")])])])]),t._v(" "),e("h3",{attrs:{id:"java相关命令"}},[t._v("JAVA相关命令")]),t._v(" "),e("ul",[e("li",[e("code",{pre:!0},[t._v("jps")]),t._v("查看JAVA进程。用来查看Hadoop集群是否正常运行")])]),t._v(" "),e("h2",{attrs:{id:"优化"}},[t._v("优化")]),t._v(" "),e("ul",[e("li",[t._v("MapReduce跑得慢\n"),e("ul",[e("li",[t._v("硬件性能瓶颈\n"),e("ul",[e("li",[t._v("CPU/内存/磁盘/网络")])])]),t._v(" "),e("li",[t._v("IO操作优化\n"),e("ul",[e("li",[t._v("数据倾斜")]),t._v(" "),e("li",[t._v("Map/Reduce任务数设置不合理")]),t._v(" "),e("li",[t._v("Map运行时间太长，Reduce长时间等待")]),t._v(" "),e("li",[t._v("小文件太多")]),t._v(" "),e("li",[t._v("大量不可分块的超大文件")]),t._v(" "),e("li",[t._v("溢写次数过多")]),t._v(" "),e("li",[t._v("归并排序次数过多")])])])])]),t._v(" "),e("li",[t._v("优化\n"),e("ul",[e("li",[t._v("数据输入\n"),e("ul",[e("li",[t._v("合并小文件")]),t._v(" "),e("li",[t._v("使用CombineTextInputFormat合并小文件")])])]),t._v(" "),e("li",[t._v("Map\n"),e("ul",[e("li",[t._v("减少溢写次数，从而减少IO\n"),e("ul",[e("li",[t._v("调整"),e("code",{pre:!0},[t._v("io.sort.mb")]),t._v(" & "),e("code",{pre:!0},[t._v("sort.spill.percent")]),t._v("，增大触发溢写的内存上限")])])]),t._v(" "),e("li",[t._v("减少合并的次数\n"),e("ul",[e("li",[t._v("调整"),e("code",{pre:!0},[t._v("io.sort.factor")]),t._v("，添加merge的文件数量，从而减少合并次数")])])]),t._v(" "),e("li",[t._v("如果不影响业务逻辑，可以使用Combiner")])])]),t._v(" "),e("li",[t._v("Reduce\n"),e("ul",[e("li",[t._v("合理设置Map和Reduce数量")]),t._v(" "),e("li",[t._v("设置Map/Reduce共存\n"),e("ul",[e("li",[t._v("调整"),e("code",{pre:!0},[t._v("slowstart.completedmaps")]),t._v("，使map任务运行到一定程度后，reduce就开始执行，减少reduce等待时间")])])]),t._v(" "),e("li",[t._v("不使用Reduce阶段，完全使用Map处理业务逻辑")]),t._v(" "),e("li",[t._v("合理设置Reduce的buffer，减少IO\n"),e("ul",[e("li",[t._v("调整"),e("code",{pre:!0},[t._v("mapred.job.reduce.input.buffer.percent")])])])])])]),t._v(" "),e("li",[t._v("IO\n"),e("ul",[e("li",[t._v("数据压缩，使用Snappy & LZO")]),t._v(" "),e("li",[t._v("使用SequenceFile二进制文件作为中间文件")])])]),t._v(" "),e("li",[t._v("数据倾斜\n"),e("ul",[e("li",[t._v("分类\n"),e("ul",[e("li",[t._v("频率倾斜：比如中国的外卖次数比梵蒂冈的多，且长期存在")]),t._v(" "),e("li",[t._v("大小倾斜：比如只有双十一那几天存在异常记录")])])]),t._v(" "),e("li",[t._v("解决方案\n"),e("ul",[e("li",[t._v("抽样和范围分区\n"),e("ul",[e("li",[t._v("根据抽样的数据得到结果而不是所有数据")]),t._v(" "),e("li",[t._v("对数据进行手动分区后再处理")])])]),t._v(" "),e("li",[t._v("自定义分区\n"),e("ul",[e("li",[t._v("把特殊的数据用特殊的方法处理。比如大的分区手动打散")])])]),t._v(" "),e("li",[t._v("使用Combine，在map阶段处理")]),t._v(" "),e("li",[t._v("使用map join而不是reduce join")])])])])])])])]),t._v(" "),e("p",[t._v("常用调优参数：详见视频p191")])])}]};t.exports={attributes:{title:"Big Data(Part 2)",description:"Hadoop, HDFS, MapReduce, YARN",tags:["Hadoop"]},vue:{render:e.render,staticRenderFns:e.staticRenderFns,component:{data:function(){return{templateRender:null}},render:function(t){return this.templateRender?this.templateRender():t("div","Rendering")},created:function(){this.templateRender=e.render,this.$options.staticRenderFns=e.staticRenderFns}}}}}}]);
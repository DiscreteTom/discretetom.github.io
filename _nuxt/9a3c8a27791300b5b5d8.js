(window.webpackJsonp=window.webpackJsonp||[]).push([[178],{1280:function(t,n){const e={render:function(){this.$createElement;return this._self._c,this._m(0)},staticRenderFns:[function(){var t=this,n=t.$createElement,e=t._self._c||n;return e("div",{staticClass:"frontmatter-markdown"},[e("h2",{attrs:{id:"摘要"}},[t._v("摘要")]),t._v(" "),e("p",[t._v("云计算经过了十余年的发展，现今通常以提供基础设施的方式赋能各个行业。作为云计算的重要组成部分，云端数据存储服务相较于本地数据存储服务有诸多优点，越来越多的企业选择将数据托管在公有云平台。为了保障公有云平台上数据的完整性，诸多云环境下远程数据完整性审计方案被提出，以保障数据拥有者的权益；如果云存储中保存的数据被检测出有误，则数据拥有者会得到云存储提供商的赔偿。但是数据持有者也可能是恶意的，如果数据持有者一开始上传到云端的数据就是错误的，而云存储提供商不加验证地保存了这些数据，数据持有者就可以诬陷云存储提供商从而获得赔偿。为了解决这个问题，本文提出了远程数据完整性审计方案的一种新的功能特性：双向验证，并对其系统模型、安全模型等属性进行了分析。本文基于同态合并的思想提出了两个检测方法，分别是高次随机合并(HTRM)方法和动态高维防御(DHDD)方法，使用这两个方法可以对已有的远程数据完整性审计方案进行改进使其支持双向验证。同时，使用基于同态合并的双向验证解决方案会为系统引入新的攻击方式：补偿攻击。本文分析了高次随机合并方法和动态高维防御方法对补偿攻击的防御能力和其计算量，并指出了这两个方法未来的改进方向。")]),t._v(" "),e("h2",{attrs:{id:"关键词"}},[t._v("关键词")]),t._v(" "),e("ul",[e("li",[t._v("云计算")]),t._v(" "),e("li",[t._v("云存储")]),t._v(" "),e("li",[t._v("数据完整性验证")]),t._v(" "),e("li",[t._v("双向验证")])]),t._v(" "),e("h2",{attrs:{id:"abstract"}},[t._v("Abstract")]),t._v(" "),e("p",[t._v("After evolving more than 10 years, nowadays cloud computing can enable many industries by providing IT infrastructures. As an important part of cloud computing, cloud storage services have many advantages compared to local data storage, more and more enterprises choose to store their data on public cloud platforms. To protect data integrity on the public cloud, many remote data integrity audit schemes have been proposed to protect the rights of data owners. If the data on the cloud is proved to be wrong, the data owner will get compensation from the cloud storage provider. But data owners may also be malicious, if they upload wrong data to the cloud, and the cloud storage provider stores the data without inspection, data owners can frame the cloud provider to gain compensation. To solve this problem, in this paper we introduce a new feature of remote data auditing schemes: bidirectional verification, and analyze the system model and security model of this feature.\nMoreover, based on homomorphic merging, we come up with two methods: High-Times Random Merging (HTRM) and Dynamic High-Dimensional Defense (DHDD), which can be used to improve existing data auditing schemes such that they support bidirectional verification.\nMeanwhile, using homomorphic merging will bring a new type of attack to the system: Complementation Attack. At the end of this paper, we analyze the defense and computational complexity of HTRM and DHDD, and point out the improvement direction about the two methods in the future.")]),t._v(" "),e("h2",{attrs:{id:"key-words"}},[t._v("Key Words")]),t._v(" "),e("ul",[e("li",[t._v("cloud computing")]),t._v(" "),e("li",[t._v("cloud storage")]),t._v(" "),e("li",[t._v("data integrity verification")]),t._v(" "),e("li",[t._v("bidirectional verification")])]),t._v(" "),e("h2",{attrs:{id:"引言"}},[t._v("引言")]),t._v(" "),e("p",[t._v("云计算的概念在2006年8月的搜索引擎会议上被首次提出[1]，并迅速成为了学术界、工业界关注的焦点[2]。经过了十余年的飞速发展，现今云计算通常以提供基础设施的方式赋能各个行业；用户可以按需获取几乎任意规模的算力、存储、网络带宽等IT资源，并“像水电一样”按实际使用量付费[3]。")]),t._v(" "),e("p",[t._v("作为云计算的重要组成部分，云端数据存储服务通常会为使用者提供近乎无限的存储容量，相对低廉的存储成本与极高的数据持久性保障。由于企业自行搭建私有云平台具有较高的资金与技术门槛，越来越多的企业选择将数据托管在公有云平台[4,5]。")]),t._v(" "),e("p",[t._v("（公有）云服务提供商(Cloud Service Provider, CSP)通常被视为不完全可信的。云服务提供商可能为了压缩存储成本而主动删除部分不常被访问的数据，或故意隐瞒数据丢失事故[4]。因此，数据拥有者需要云环境下的数据完整性审计方案来验证云端数据的完整性。已有的远程数据完整性审计方案主要可分为两大类：数据持有性证明(Provable Data Possession, PDP)和可恢复数据证明(Proof of Retrievability, POR)[4]。不同的远程数据完整性审计方案也会拥有不同的功能特性，比如支持动态验证的PDP[6]与POR[7]、支持公开验证的PDP[8]与POR[9]，更多的特性如批量审计、隐私保护等[4,5]。")]),t._v(" "),e("p",[t._v("然而，除了云服务提供商，数据持有者也可能是不完全可信的[10,11]。恶意数据持有者可能将错误的数据上传至云端。当数据持有者向云服务提供商发起挑战时，数据完整性验证必然无法通过。数据持有者可以以此为由诬陷云服务提供商，从而获得数据丢失的赔偿。所以，在设计远程数据完整性审计方案时，云服务提供商需要有能力检测数据持有者一开始上传的数据是否为正确的数据。")]),t._v(" "),e("p",[t._v("本文将定义远程数据完整性审计方案的一种新的功能特性：双向验证，并分析具有双向验证性质的远程数据完整性审计方案的系统模型与安全模型。利用同态合并的思想对已有的云环境远程数据完整性审计方案进行改进，在允许数据持有者验证云端数据是否完整与正确的同时，使得云服务提供商也可以验证数据持有者上传的数据是否真实有效。从而在保证云端数据完整性的前提下，使云服务提供商能够防御恶意数据持有者发起的诬陷攻击，维护良好的云计算生态。")]),t._v(" "),e("h2",{attrs:{id:"预备知识"}},[t._v("预备知识")]),t._v(" "),e("h3",{attrs:{id:"双向验证的定义与相关研究"}},[t._v("双向验证的定义与相关研究")]),t._v(" "),e("p",[t._v("定义具有双向验证(Bidirectional Verification)性质的远程数据完整性审计方案：如果一个远程数据完整性审计方案能够使云服务提供商在接收数据时验证数据拥有者上传的数据是否真实有效，则称该方案为具有双向验证性质的远程数据完整性审计方案。")]),t._v(" "),e("p",[t._v("参考文献[10,11]中提到在远程数据完整性审计方案可能存在恶意的数据持有者，但是这两篇文献中给出的远程数据完整性审计方案并不是为了解决双向验证这一问题而设计的。本文将针对双向验证这一问题提出对已有远程数据完整性审计方案的改造思路。所以，后文中并不会与参考文献[10,11]中的远程数据完整性审计方案进行性能对比。")]),t._v(" "),e("h3",{attrs:{id:"系统模型"}},[t._v("系统模型")]),t._v(" "),e("p",[t._v("本系统由三个实体组成：数据持有者（下称“C端”），公有云存储提供商（下称“S端”），可信的第三方审计机构（下称“A端”）。\nC端需要将本地数据上传至S端，且C端需要将数据完整性审计任务委托给A端。 S端负责对C端上传的数据进行持久化保存。S端需要在接收数据时验证C端上传的数据是否正确。如果S端经过验证发现C端上传的数据不正确，则拒绝保存C端此次上传的所有数据或要求C端重新上传错误部分的数据。A端在收到C端的委托后，负责向S端发起数据完整性挑战。")]),t._v(" "),e("p",[t._v("本系统使用同态验证技术进行远程数据完整性审计。C端在上传数据到S端之前会首先将原数据拆分为若干个数据块，然后计算每个数据块对应的同态标签，之后把所有数据块及其对应的标签上传至S端。\nC端委托A端发起远程数据完整性挑战时，A端会向S端发送挑战信息，S端根据挑战信息和本地存储的数据块与标签向A端返回挑战结果，A端验证挑战结果是否正确。如果挑战结果正确，则说明C端的数据在S端被正确保存；如果挑战结果错误，则说明C端的数据在S端已被损坏。\nS端接受C端上传的数据块与标签时需要根据C端公布的算法参数验证C端上传的数据块与标签是否匹配。如果存在不匹配的情况，则说明C端可能恶意上传了错误的数据块与标签；如果不存在不匹配的情况，则说明C端上传的数据块和标签是可被接收的。")]),t._v(" "),e("p",[t._v("本系统包含以下5个多项式时间内算法：")]),t._v(" "),e("ol",[e("li",[t._v("$KeyGen(k) \\rightarrow (pk, sk)$能够根据传入的随机参数$k$生成私有参数集$sk$和公开参数集$pk$。")]),t._v(" "),e("li",[t._v("$TagBlock(pk, sk, m) \\rightarrow T$能够根据参数集$sk$和$pk$对数据块$m$进行非对称加密，得到此数据块对应的标签$T$。")]),t._v(" "),e("li",[t._v("$CheckTag(pk, m, T) \\rightarrow {true, false}$能够根据公开参数集$pk$判断传入的数据块$m$和标签$T$是否匹配。")]),t._v(" "),e("li",[t._v("$GenProof(pk, {m}, {T}, chal) \\rightarrow v$能够基于公开参数集$pk$、所有数据块${m}$、所有标签${T}$和挑战参数集$chal$生成证明$v$。")]),t._v(" "),e("li",[t._v("$CheckProof(pk, chal, v) \\rightarrow {true, false}$能够基于公开参数集$pk$验证挑战参数集$chal$对应的证明$v$是否正确。")])]),t._v(" "),e("p",[t._v("本系统包含三个阶段：准备阶段(Setup)，验证阶段(Validate)和挑战阶段(Challenge)。准备阶段如下：")]),t._v(" "),e("ol",[e("li",[t._v("C端将原数据$M$拆分为$n$个数据块：$m_1, m_2, \\cdots, m_n$。")]),t._v(" "),e("li",[t._v("C端生成随机数$k$，并将其作为参数调用算法$KeyGen$得到私有参数集$sk$和公开参数集$pk$，然后把公开参数集$pk$公开。")]),t._v(" "),e("li",[t._v("C端使用私有参数集$sk$和公开参数集$pk$分别为每个数据块调用$TagBlock$计算其对应的标签：$T_1, T_2, \\cdots, T_n$。")]),t._v(" "),e("li",[t._v("C端将所有数据块与对应的标签发送到S端。")])]),t._v(" "),e("p",[t._v("验证阶段如下：")]),t._v(" "),e("ol",[e("li",[t._v("S端根据公开参数集$pk$调用函数$CheckTag$来验证C端上传的所有数据块与标签是否匹配。如果存在不匹配的数据块和标签，S端向C端发出重传这些不匹配的数据块和标签的申请，直到S端所有数据块和标签都是匹配的。")]),t._v(" "),e("li",[t._v("S端保存C端上传的所有数据块和标签并通知C端可以删除本地数据块和标签。")])]),t._v(" "),e("p",[t._v("挑战阶段如下：")]),t._v(" "),e("ol",[e("li",[t._v("C端委托A端向S端发起远程数据完整性挑战。")]),t._v(" "),e("li",[t._v("A端构建挑战参数集$chal$并发送给S端。")]),t._v(" "),e("li",[t._v("S端根据挑战参数集$chal$、公开参数集$pk$和本地保存的数据块与标签，调用函数$GenProof$生成证明$v$并发送给A端。")]),t._v(" "),e("li",[t._v("A端根据挑战参数集$chal$和公开参数集$pk$，调用函数$CheckProof$验证S端返回的证明$v$是否正确，并将验证结果通知到C端。")])]),t._v(" "),e("p",[t._v("由于已有的远程数据完整性审计方案通常已经具备函数$KeyGen, TagBlock, GenProof, CheckProof$和准备阶段、挑战阶段的功能，所以本系统的重点为设计验证阶段中使用的函数$CheckTag$。称支持双向验证的远程数据完整性审计方案中使用的已有远程数据完整性审计方案为“宿主方案(host)”。")]),t._v(" "),e("h3",{attrs:{id:"安全模型"}},[t._v("安全模型")]),t._v(" "),e("p",[t._v("此系统相较于宿主方案所使用的系统，存在一种新的攻击方式：失配攻击。")]),t._v(" "),e("p",[t._v("定义失配攻击(Mismatch Attack)：在本系统中，如果C端在准备阶段向S端上传的数据块标签对中存在不匹配的数据块标签对，则称C端对系统发起了失配攻击。如果S端在验证阶段没有检测出所有的失配数据块标签对，则称C端对系统发起的失配攻击是成功的；如果S端在验证阶段检测出了所有的失配数据块标签对，则称C端对系统发起的失配攻击是失败的。")]),t._v(" "),e("p",[t._v("本系统中的实体：")]),t._v(" "),e("ul",[e("li",[t._v("S端是不完全可信的，可能会在挑战阶段对系统发起替换攻击、伪造攻击、重放攻击[4,5]等形式的攻击。宿主方案通常已经解决了这些问题。")]),t._v(" "),e("li",[t._v("A端是可信的，不会和C端勾结在挑战阶段对系统发起共谋攻击。同时A端是好奇的[5]，可能试图通过S端返回的挑战结果推测C端在S端保存的数据。宿主方案通常已经解决了这些问题。")]),t._v(" "),e("li",[t._v("C端是不完全可信的，可能会在准备阶段对系统发起失配攻击。如果S端不加验证就接收了这些数据块和标签，则C端会发起特定的挑战使得S端返回的挑战结果不正确，从而获得赔偿。")])]),t._v(" "),e("p",[t._v("因为如果C端在准备阶段上传了不匹配的数据块和标签且S端没有能够在验证阶段成功检测出这些不匹配的数据块和标签，C端就可以针对这些不匹配的数据块和标签发起远程数据完整性审计挑战，导致S端每次返回的挑战结果必然不正确，所以S端不允许C端选择远程数据完整性挑战中要检查的数据块标签对的下标，而是由A端产生这些数据块标签对的下标。换言之，系统中使用的宿主方案必须支持公开审计，且S端不接受由C端发起的挑战，仅接受由可信的A端发起的挑战。")]),t._v(" "),e("p",[t._v("如果C端上传的失配数据块标签对数量过多，虽然挑战阶段A端更易触发S端证明失败，但是在验证阶段，过多的失配数据块标签对更易被S端检测到；如果C端上传的失配数据块标签对数量过少，虽然在验证阶段不易被S端检测到，但是在挑战阶段A端不易触发S端证明失败。\n根据香农箴言，“敌人了解系统(the enemy knows the system)”[12]，本系统中认为C端知道S端所使用的双向验证方案的任何细节，C端可以根据S端的检测手段把失配的数据块标签对数量和值设置为理论最优，此处的理论最优意为，对于S端所使用的双向验证方案，C端上传的数据块标签对集合能够使此双向验证方案的检测成功率达到最低。所以在评估一个双向验证方案时，需要将其最低检测成功率作为评估指标。")]),t._v(" "),e("h3",{attrs:{id:"逐个验证方案"}},[t._v("逐个验证方案")]),t._v(" "),e("p",[t._v("由于宿主方案支持公开审计，所以在此提出一个基础方案：逐个验证方案。此方案将作为后文中提及的其他方案的性能参照。")]),t._v(" "),e("p",[t._v("在逐个验证方案的验证阶段中，S端将使用C端公布的公开参数集$pk$调用函数$CheckTag$逐个验证C端上传的每一对数据块和标签是否匹配。在此方案中，S端必然能够检测出C端上传的任何失配数据块标签对，防御成功率为$100%$，但是S端需要进行$n$次验证。所以对逐个验证方案的改进目标为如何在保证S端检测失配数据块标签对成功率可接受的前提下减少S端验证所需计算量。")]),t._v(" "),e("h3",{attrs:{id:"禁用随机抽样"}},[t._v("禁用随机抽样")]),t._v(" "),e("p",[t._v("很多宿主方案会使用随机抽样技术减少挑战阶段的计算量。但是在本系统的验证阶段，S端在验证C端上传的数据块和标签是否匹配时不能像很多宿主方案一样使用随机抽样技术仅检测部分数据块和标签是否匹配。因为在挑战阶段，A端可以向S端发起无数次挑战，而在验证阶段中，S端仅对数据块标签对进行了一次验证。")]),t._v(" "),e("p",[t._v("根据安全模型中的香农箴言，C端知道S端使用了随机抽样技术尝试减少计算量。假设C端仅设置了一对失配的数据块和标签，数据块标签对总量为$n$，S端随机抽样检测$n’$个数据块标签对，则S端检测出失配数据块和标签的概率为：")]),t._v(" "),e("p",[t._v("$$\nP_1 = \\frac{C_1^1 \\cdot C_{n-1}^{n’-1}}{C_n^{n’}} = \\frac{n’}{n}\n$$")]),t._v(" "),e("p",[t._v("即S端在抽样检测中几乎需要检测所有数据块标签对才能以较高的概率发现失配的那一对数据块标签对。")]),t._v(" "),e("p",[t._v("反观挑战阶段，假设A端每次随机抽取$n’’$个数据块标签对，每次发现错误数据块标签对的概率为：")]),t._v(" "),e("p",[t._v("$$\nP_2 = \\frac{n’’}{n}\n$$")]),t._v(" "),e("p",[t._v("在$k$次挑战中发现错误数据块标签对的挑战次数的数学期望为：")]),t._v(" "),e("p",[t._v("$$\nE = \\frac{k \\cdot n’’}{n}\n$$")]),t._v(" "),e("p",[t._v("即使取$n’’ = n / 100$，即A端每次挑战仅验证$1%$的数据块标签对，那么只需要100次验证，A端发现错误数据块标签对的挑战次数数学期望就超过了$1$。")]),t._v(" "),e("p",[t._v("由此可以看出，虽然S端在验证阶段通过一次验证发现失配数据块标签对的概率公式和A端在挑战阶段每次挑战发现失配数据块标签对的概率公式相同，但是因为A端可以多次发起挑战从而把计算量在时间维度上均摊，而S端在验证阶段使用多次随机抽样验证得到的成功检测概率不如提升单次验证所检查的数据块标签对数量而得到的成功检测概率，所以使用抽样技术无法在保证检测成功率可接受的前提下实现有效减少计算量的效果。因此，任何双向验证方案在验证阶段都应该对所有数据块标签对进行至少一次验证。")]),t._v(" "),e("h2",{attrs:{id:"基于同态合并的双向验证解决方案"}},[t._v("基于同态合并的双向验证解决方案")]),t._v(" "),e("h3",{attrs:{id:"基本思想"}},[t._v("基本思想")]),t._v(" "),e("p",[t._v("根据上文系统模型中的描述，宿主方案使用同态验证技术进行远程数据完整性审计。得益于数据块与对应标签的同态性，合并部分数据块与合并这些数据块对应的标签，得到的结果也是匹配的，且验证次数从多次减少为了一次。那么S端可以基于此性质，通过合并部分数据块标签对来减少验证次数。所以在基于同态合并的双向验证解决方案中，S端通常会设置若干个缓冲区，用来缓存合并数据块和标签产生的临时结果。")]),t._v(" "),e("p",[t._v("不过宿主方案使用同态验证技术的主要目的是把多个数据块标签对合并以减少挑战过程中数据传输的花销和验证的次数，而验证次数的减少并不能保证计算量的减少。如果每次验证所需的时间复杂度较高，且使用同态技术合并数据块和标签之后导致验证函数的参数膨胀，则使用同态验证技术并不能减少计算量。")]),t._v(" "),e("p",[t._v("定义基于同态合并的双向验证解决方案的计算量基本需求：宿主方案中每次验证所需的时间复杂度较低。")]),t._v(" "),e("p",[t._v("如果宿主方案满足上述需求，则合并数据块标签对后导致验证参数的膨胀不会导致验证所需的时间明显增加，整个验证过程所需计算量与验证次数成正比。所以后文将使用验证次数表示计算量。验证次数较少即意味着方案的计算量较少。")]),t._v(" "),e("h3",{attrs:{id:"补偿攻击"}},[t._v("补偿攻击")]),t._v(" "),e("p",[t._v("使用同态合并的方式减少计算量会为系统引入一种新的失配攻击方式：补偿攻击。")]),t._v(" "),e("p",[t._v("假设存在$k$对不匹配的数据块和标签，使用同态技术合并这些数据块和标签之后得到的结果可能是匹配的。对此，我们先介绍另一个新的概念：数据块标签对的逻辑值。")]),t._v(" "),e("p",[t._v("定义数据块标签对的逻辑值(Logic Value)：如果一对数据块和标签是匹配的，则此数据块标签对的逻辑值为$0$。如果一对数据块和标签不匹配，则它们的逻辑值非$0$。如果使用同态技术合并多个数据块和标签得到的结果是匹配的，则这些数据块标签对的逻辑值的之和为$0$。")]),t._v(" "),e("p",[t._v("定义广义补偿攻击(Generalized Complementation Attack)：如果C端上传的所有数据块标签对中，存在逻辑值非$0$的数据块标签对，且所有逻辑值非$0$的数据块标签对的逻辑值之和为$0$，则称C端对系统发起了广义补偿攻击。")]),t._v(" "),e("p",[t._v("定义$k$阶补偿攻击(k-order Complementation Attack)：如果C端对系统发起了广义补偿攻击，且对于C端上传的数据块标签对集合，能够对系统发起广义补偿攻击的数据块标签对子集的基数最小为$k$，则称C端对系统发起了$k$阶补偿攻击。")]),t._v(" "),e("p",[t._v("定义精确$k$阶补偿攻击(Exact k-order Complementation Attack)：如果C端对系统发起的$k$阶补偿攻击中C端上传的所有逻辑值非$0$的数据块标签对数量恰好为$k$，则称此$k$阶补偿攻击为精确的$k$阶补偿攻击。")]),t._v(" "),e("p",[t._v("显然精确$k$阶补偿攻击属于$k$阶补偿攻击的特殊情况；$k$阶补偿攻击属于广义补偿攻击的特殊情况。为了方便理解，表3-1为补偿攻击举例与分析。")]),t._v(" "),e("p",[t._v("表3-1：补偿攻击举例与分析。")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("C端上传的数据块标签对的逻辑值集合")]),t._v(" "),e("th",[t._v("补偿攻击类型")]),t._v(" "),e("th",[t._v("分析")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("${0,0,0,0,0}$")]),t._v(" "),e("td",[t._v("非补偿攻击")]),t._v(" "),e("td",[t._v("没有逻辑值非$0$的数据块标签对")])]),t._v(" "),e("tr",[e("td",[t._v("${0,+1,-1,+1,0}$")]),t._v(" "),e("td",[t._v("非补偿攻击")]),t._v(" "),e("td",[t._v("所有逻辑值非$0$的数据块标签对的逻辑值之和不为$0$")])]),t._v(" "),e("tr",[e("td",[t._v("${+2,+1,-1,-1,-1}$")]),t._v(" "),e("td",[t._v("非精确$2$阶补偿攻击")]),t._v(" "),e("td",[t._v("能够发起广义补偿攻击，且能够对系统发起广义补偿攻击的数据块标签对子集的基数最小为$2$，比如集合${+1,-1}$")])]),t._v(" "),e("tr",[e("td",[t._v("${+1,-1,0,0,0}$")]),t._v(" "),e("td",[t._v("精确$2$阶补偿攻击")]),t._v(" "),e("td",[t._v("能够发起$2$阶补偿攻击，且所有逻辑值非$0$的数据块标签对数量恰好为$2$")])]),t._v(" "),e("tr",[e("td",[t._v("${+2,-1,-1,0,0}$")]),t._v(" "),e("td",[t._v("精确$3$阶补偿攻击")]),t._v(" "),e("td",[t._v("能够发起$3$阶补偿攻击，且所有逻辑值非$0$的数据块标签对数量恰好为$3$")])])])]),t._v(" "),e("p",[t._v("根据上文的结论我们禁用了随机抽样技术在验证阶段的使用，所以在验证阶段所有数据块标签对都会被同态合并后验证至少一次。如果C端在准备阶段上传的数据块标签对中只有一个数据块标签对的逻辑值非$0$，则C端发起的失配攻击一定会在验证阶段被检测S端检测到。所以在C端发起的补偿攻击中，C端至少上传$2$个逻辑值非$0$的数据块标签对，且在验证阶段S端进行同态合并验证时这些逻辑值非$0$的数据块标签对恰好位于同一个缓冲区中，可以互相补偿时，S端才无法在验证阶段检测出这些逻辑值非$0$的数据块标签对，C端的失配攻击才会成功。")]),t._v(" "),e("h3",{attrs:{id:"全局合并方案"}},[t._v("全局合并方案")]),t._v(" "),e("p",[t._v("作为基于同态合并的双向验证解决方案中的基础方案，S端可以在验证阶段直接将C端在准备阶段上传的所有数据块标签对进行合并，并对最终的合并结果进行验证。所以此方案中S端在验证阶段只需要一次验证，计算量最小。")]),t._v(" "),e("p",[t._v("但是根据安全模型中提到的香农箴言，C端知道S端使用了全局合并方案，所以只要C端发起任意广义补偿攻击，S端就无法进行防御。此方案没有实用性。")]),t._v(" "),e("h3",{attrs:{id:"随机合并方案"}},[t._v("随机合并方案")]),t._v(" "),e("h4",{attrs:{id:"简单随机合并方案"}},[t._v("简单随机合并方案")]),t._v(" "),e("p",[t._v("作为对全局合并方案的改进，S端可以设置$a$个缓冲区，下标为$1, 2, \\cdots, a$。每个缓冲区都由一个数据域和一个标签域组成，且每个缓冲区的数据域初始值和此缓冲区的标签域初始值相匹配。在验证阶段，对于C端在准备阶段上传的每一对数据块标签对，S端都会产生一个值域为$[1, a]$的随机数作为缓冲区的下标，然后把当前数据块标签对中的数据与对应下标缓冲区的数据域合并，把当前数据块标签对中的标签与对应下标缓冲区的标签域合并，最后验证$a$个缓冲区的数据域与标签域是否匹配。")]),t._v(" "),e("p",[t._v("根据安全模型，我们同样需要假设C端知道S端使用了随机合并方案，且C端会选择失配攻击成功率最高的补偿攻击方案。")]),t._v(" "),e("p",[t._v("假设C端在准备阶段发起了精确$k$阶补偿攻击，那么只有这$k$个数据块标签对在验证阶段被随机分配到同一个缓冲区从而可以相互补偿时，此次攻击才能够避开S端的检测。所以这种情况下C端的失配攻击成功率为：")]),t._v(" "),e("p",[t._v("$$\nP = \\frac{1}{a^{k-1}}\n$$")]),t._v(" "),e("p",[t._v("如果C端在准备阶段发起了非精确的$k$阶补偿攻击，即在以上精确$k$阶补偿攻击的基础上添加了逻辑值非$0$的数据块标签对，为了使补偿攻击成功，除了满足上述精确$k$阶补偿攻击的所有条件，新增的逻辑值非$0$的数据块标签对需要满足额外的位置关系才能够避开S端在验证阶段的检测，这将导致失配攻击成功率的下降。所以在随机合并方案中，精确$k$阶补偿攻击的攻击成功率高于非精确$k$阶补偿攻击的攻击成功率。")]),t._v(" "),e("p",[t._v("对于精确$k$阶补偿攻击，当$k$取最小值$2$时其在随机合并方案中的失配攻击成功率最高，为$1/a$。所以C端会对随机合并方案使用精确$2$阶补偿攻击来实现失配攻击成功率的最大化。")]),t._v(" "),e("p",[t._v("结论：在随机合并方案中，补偿攻击成功率最高的方案为精确$2$阶补偿攻击，补偿攻击成功率为$1/a$，S端在验证阶段的计算量为$a$次验证。")]),t._v(" "),e("h4",{attrs:{id:"高次随机合并方案htrm"}},[t._v("高次随机合并方案(HTRM)")]),t._v(" "),e("p",[t._v("如果S端希望在使用相同方案的前提下降低C端发起的补偿攻击的成功率，有两种方式：")]),t._v(" "),e("ol",[e("li",[t._v("S端增加缓冲区的数量$a$。如令新的缓冲区数量为$2a$，此时C端发起的精确$2$阶补偿攻击成功率为$1/2a$，S端计算量为$2a$次验证。")]),t._v(" "),e("li",[t._v("S端保持缓冲区数量$a$不变，但是在验证阶段多次调用随机合并方案。如S端调用$2$次缓冲区数量为$a$的随机合并方案，C端发起的精确$2$阶补偿攻击成功率为$1/a^2$，S端计算量为$2a$。")])]),t._v(" "),e("p",[t._v("显然在计算量相同的前提下，方式2拥有更强的防御能力。我们可以利用方式2的特性来改造原本的随机合并方案。")]),t._v(" "),e("p",[t._v("定义$a$阶$b$次随机合并方案：如果S端设置的缓冲区数量为$a$，且在验证阶段调用了$b$次随机合并方案，则称S端使用了$a$阶$b$次随机合并方案。")]),t._v(" "),e("p",[t._v("实际使用中S端确定$a$和$b$的步骤：")]),t._v(" "),e("ol",[e("li",[t._v("S端确定期望的C端补偿攻击成功率最大值$p$，如$0.01%$。")]),t._v(" "),e("li",[t._v("由于$1/a^b = p$，所以$a$和$b$满足约束关系$a = (\\frac1p)^{\\frac1b}$，S端在验证阶段的计算量为$b \\cdot a = b(\\frac1p)^{\\frac1b}$，计算可得$b = ln(1/p), a=e$时计算量最小。由于$a$和$b$都为整数，所以令$a=3, b = \\lceil log_3(1/p) \\rceil$时S端在验证阶段的计算量最小。")])]),t._v(" "),e("p",[t._v("可以看出无论S端期望的C端补偿攻击成功率为多少，无论C端在准备阶段上传了多少数据块标签对，S端都应该准备$3$个缓冲区，而重复调用随机合并方案的次数$b$由S端期望的C端补偿攻击成功率决定。")]),t._v(" "),e("p",[t._v("除此之外，为了得到相同的C端补偿攻击成功率，重复调用低次随机合并方案得到的S端在验证阶段的计算量和调用一次高次随机合并方案得到的S端在验证阶段的计算量是相同的。比如，为了使C端发起的补偿攻击成功率为$1/3^{2b}$，S端可以在验证阶段调用两次$3$阶$b$次随机合并方案，或者调用一次$3$阶$2b$次随机合并方案，这两种方式的计算量是相同的，因为它们都相当于调用了六次$3$阶$1$次随机合并方案。")]),t._v(" "),e("p",[t._v("定义自我最优性(Self Optimality)：对于一个基于同态合并的双向验证解决方案，如果为了得到相同的C端补偿攻击成功率，重复调用此方案得到的S端在验证阶段所需的计算量和修改此方案的参数得到的S端在验证阶段所需的计算量相同，则称此方案具有自我最优性。")]),t._v(" "),e("p",[t._v("显然，对于任意阶数$a$，$a$阶$b$次随机合并方案都是具备自我最优性的，即高次随机合并方案具备自我最优性。对$a$阶$b$次随机合并方案的$c$次重复调用得到的S端在验证阶段所需计算量与C端补偿攻击成功率最大值和调用一次$a$阶$c \\cdot b$次随机合并方案得到的S端在验证阶段所需计算量与C端补偿攻击成功率最大值是相同的。")]),t._v(" "),e("p",[t._v("结论：")]),t._v(" "),e("ol",[e("li",[t._v("高次随机合并方案具备自我最优性。")]),t._v(" "),e("li",[t._v("在C端补偿攻击成功率最大值相同的前提下，$3$阶$b$次随机合并方案中S端在验证阶段的计算量是所有$a$阶$b$次随机合并方案中S端在验证阶段的计算量的最小值。")]),t._v(" "),e("li",[t._v("对于$3$阶$b$次随机合并方案，S端在验证阶段的计算量为$3b$次验证，C端发起的精确$2$阶补偿攻击成功率为$1/3^b$，。")])]),t._v(" "),e("h3",{attrs:{id:"动态高维防御方案dhdd"}},[t._v("动态高维防御方案(DHDD)")]),t._v(" "),e("h4",{attrs:{id:"基本原理"}},[t._v("基本原理")]),t._v(" "),e("p",[t._v("假设C端在准备阶段上传了$n$个数据块标签对。在验证阶段，S端为C端上传的每一个数据块标签对随机分配一个$x$维逻辑坐标，这些逻辑坐标在维度$x_i$（$1 \\le i \\le x$）上的取值有$n_i$种可能，分别是$0, 1, 2, \\cdots, n_i - 1$，且所有数据块标签对的逻辑坐标都是唯一的。进行合并验证时，对于每个维度$x_i$（$1 \\le i \\le x$）上的每个可能的取值$j$（$0 \\le j \\le n_i - 1$），合并所有在维度$x_i$上取值为$j$的数据块标签对并进行验证，一共需要$\\sum_{i=1}^x n_i$次验证。")]),t._v(" "),e("p",[t._v("在上述$x$维空间中，可能的逻辑坐标取值一共有$\\prod_{1}^x n_i$个，令$N = \\prod_{1}^x n_i$。因为每个数据块标签对的逻辑坐标是唯一的，所以根据鸽巢原理$N \\ge n$。为了使验证阶段S端的验证次数最少，根据经典不等式，当$n_1 = n_2 = \\cdots = n_x = N^{\\frac1x}$时，$\\sum_{i=1}^x n_i$值达到最小，即验证次数最少，为$x \\cdot N^{\\frac1x}$次。计算可得当$x = ln(N)$时验证次数最少，此时要求每个维度有$e$种取值。因为每个维度的取值数量需要为整数，计算可得当每个维度有$3$种取值、维度数量为$log_3(N)$时，验证次数最少，为$3 \\cdot log_3(N)$次验证。由于维度数量$log_3(N)$也需要为整数，所以$N$必须是$3$的整数次方。由于$N \\ge n$，所以在$n$确定时，$N$的取值也是确定的，为不小于$n$的最小的$3$的整数次方。")]),t._v(" "),e("p",[t._v("由于每个数据块标签对都被分配了一个$x$维坐标，所以每个数据块标签对都被合并了$x$次，这使得所有逻辑值非$0$的数据块标签对之间需要满足复杂的$x$维补偿关系才能避过S端在验证阶段的检测。所以此处高维的思想保证了C端无法获得较高的失配攻击成功率。具体的失配攻击成功率将在后文中计算。而上述$N$的取值推理过程保证了在数据块标签对数量$n$确定时此方案计算量的最小化。所以此方案理论上是一个兼顾计算量和防御能力的优秀方案。")]),t._v(" "),e("p",[t._v("结论：此方案具有较强的补偿攻击防御能力和较小的验证开销。为了使此方案在验证阶段的计算量达到最小，令$N$为不小于$n$的最小的$3$的整数次方，则高维空间的维度数量$x$应该满足$x = log_3(N)$；所有数据块标签对的逻辑坐标在任意一个维度上的分量只有$3$种取值；S端在验证阶段的计算量为$3 \\cdot log_3(N)$次验证。")]),t._v(" "),e("p",[t._v("因为使用此方案防御补偿攻击所需要的维度数量$x$随数据块标签对数量$n$的增加而阶段性增加，所以称此方案为动态高维防御(Dynamic High-Dimensional Defense, DHDD)方案。")]),t._v(" "),e("h4",{attrs:{id:"方案流程"}},[t._v("方案流程")]),t._v(" "),e("h5",{attrs:{id:"准备阶段"}},[t._v("准备阶段")]),t._v(" "),e("ol",[e("li",[t._v("C端首先通知S端即将上传的数据块标签对数量$n$，S端计算不小于$n$的最小的$3$的整数次方$N$，并令维度数量$x = log_3(N)$。符号$\\Z_N$表示集合${0,1,2,\\cdots,N-1}$。符号$G_x$表示$x$维空间中所有维度的坐标取值都在$\\Z_3 = {0, 1, 2}$中的点的坐标集合。")]),t._v(" "),e("li",[t._v("S端选择一个伪随机排列$\\pi: \\Z_N \\rightarrow G_x$，输入数据块标签对的原始下标$i$时输出唯一的伪随机逻辑位置$p_i$。$p_i$是一个$x$维坐标，其在维度$j$上的分量表示为$p_{i,j}$。S端还需要设置此伪随机排列的反函数$\\pi^{-1}$，输入逻辑位置$p_i$时输出数据块标签对的原始下标$i$。")]),t._v(" "),e("li",[t._v("S端设置$3x$个缓冲区，每个缓冲区由数据域和标签域构成。令这$3x$个缓冲区编号分别为$(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2),\\cdots, (x-1, 0), (x-1, 1), (x-1, 2)$。编号为$(i, j)$的缓冲区负责聚合在维度$i$上值为$j$的数据块标签对，其中$0 \\le i \\le x - 1$且$0 \\le j \\le 2$。")]),t._v(" "),e("li",[t._v("S端收到C端上传的数据块标签对之后，使用函数$\\pi$根据数据块标签对的原始下标$i$得到逻辑位置$p_i$。对于此逻辑位置在维度$j$上的分量$k = p_{i,j}$，缓冲区$(j, k)$的数据域需要聚合此数据块标签对中的数据$m_i$，缓冲区的标签域需要聚合此数据块标签对中的标签$T_i$。因为逻辑位置$p_i$有$x$个分量，所以一共有$x$个缓冲区需要聚合此数据块。所有数据块上传完毕时，一共发生了$nx$次聚合操作。此步骤也可以在C端上传完毕所有数据块标签对之后放在验证阶段执行。")])]),t._v(" "),e("h5",{attrs:{id:"验证阶段"}},[t._v("验证阶段")]),t._v(" "),e("p",[t._v("C端将所有数据块标签对上传完毕后，S端计算每个缓冲区中数据域和标签域是否匹配即可。\n如果所有缓冲区的数据域和标签域都是匹配的，则说明C端有很大的概率没有发动失配攻击，具体概率会在下文“安全性”一节讲到。\n如果存在缓冲区的数据域与标签域不匹配，那么根据失配缓冲区的下标$(i, j)$可以得到结论：逻辑坐标在维度$i$上的值为$j$的最多$N/3$个数据块标签对中存在不匹配的数据块标签对。\n所以S端可以根据每个结果不匹配的缓冲区下标反推出逻辑值可能非$0$的最多$N/3$个数据块标签对的逻辑位置，然后调用$\\pi$的反函数$\\pi^{-1}$，根据逻辑位置反推数据块标签对原始下标，从而向C端申请重传数据块标签对，然后仅验证重传的数据块标签对的逻辑值是否为$0$。当然对重传的数据块标签对的验证也可以使用DHDD方法。所有数据块标签对都验证完毕后，S端通知C端可以删除本地数据块和标签。")]),t._v(" "),e("p",[t._v("此处可以看出，虽然DHDD兼顾计算量和防御能力，但是在检测出C端发起失配攻击之后无法高效定位错误的数据块标签对的原始下标。")]),t._v(" "),e("h4",{attrs:{id:"方案算法"}},[t._v("方案算法")]),t._v(" "),e("p",[t._v("根据上一节的结论，DHDD在执行的过程中需要以下几个算法（后文中的算法均使用python 3作为编程语言）。")]),t._v(" "),e("p",[t._v("算法1：生成随机排列的算法。此算法接受一个参数"),e("code",{pre:!0},[t._v("x")]),t._v("作为维度数量，返回两个字典，分别表示映射$\\pi$和映射$\\pi^{-1}$。在这两个字典中，使用数字$0$到$3^x - 1$作为数据块标签对原始下标的取值范围，使用长度为$x$的、由"),e("code",{pre:!0},[t._v("0")]),t._v("、"),e("code",{pre:!0},[t._v("1")]),t._v("、"),e("code",{pre:!0},[t._v("2")]),t._v("组成的字符串表示$x$维空间中的逻辑坐标，比如"),e("code",{pre:!0},[t._v('"01021"')]),t._v("表示一个$5$维空间的坐标，且此坐标在每个维度的分量分别为$0,1,0,2,1$。")]),t._v(" "),e("pre",{staticClass:"language-py"},[e("code",{pre:!0,attrs:{class:"language-py"}},[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" itertools\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" random "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" shuffle\n\n\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("random_permutation_generator")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n  Input the number of dimensions `x`.\n\n  Return 2 dictionaries,\n  the first is the random permutation map `pi`,\n  the second is the reverse of `pi`.\n\n  `pi` will accept a number as the index of a data block,\n  then return a string constructed by `012` to represent the logic position of the data block.\n\n  E.g.: if `pi[5] = '01021'`, then `pi_reverse['01021'] = 5`.\n  '''")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# generate logic positions as a string list")]),t._v("\n  logic_p "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" v "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" itertools"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("product"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'012'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" repeat"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("x"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# random shuffle logic positions")]),t._v("\n  shuffle"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("logic_p"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# construct map")]),t._v("\n  pi "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("zip")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("logic_p"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" logic_p"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  pi_reverse "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("zip")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("logic_p"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("logic_p"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# return map")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" pi"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pi_reverse\n")])]),t._v(" "),e("p",[t._v("算法2：S端缓冲区初始化算法。此处假设每个缓冲区的数据域初始值为$0$，标签域初始值为$0$。此算法接受一个参数"),e("code",{pre:!0},[t._v("x")]),t._v("作为维度数量，返回一个二维数组"),e("code",{pre:!0},[t._v("buffers")]),t._v("，此二维数组的每个元素都是一个缓冲区，"),e("code",{pre:!0},[t._v("buffers[i][j]")]),t._v("表示负责合并$x$维空间中在维度$i$上的值为$j$的所有数据块标签对的缓冲区。")]),t._v(" "),e("pre",{staticClass:"language-py"},[e("code",{pre:!0,attrs:{class:"language-py"}},[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("init_buffers")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n  Input the number of dimensions `x`.\n\n  Return `buffers`.\n  You can use `buffers[i][j]` to access the buffer with value `j` at dimension `i`.\n  '''")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'tag'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])]),t._v(" "),e("p",[t._v("算法3：S端合并C端上传数据块的算法。此算法接受的参数包括缓冲区阵列"),e("code",{pre:!0},[t._v("buffers")]),t._v("、所有数据块标签对"),e("code",{pre:!0},[t._v("data_tags")]),t._v("、数据块标签对原始下标到$x$维空间逻辑位置的映射"),e("code",{pre:!0},[t._v("pi")]),t._v("、由宿主方案定义的数据域合并函数"),e("code",{pre:!0},[t._v("data_merger")]),t._v("和同样由宿主方案定义的标签域合并函数"),e("code",{pre:!0},[t._v("tag_merger")]),t._v("。返回合并所有数据块标签对之后的缓冲区阵列"),e("code",{pre:!0},[t._v("buffers")]),t._v("。")]),t._v(" "),e("pre",{staticClass:"language-py"},[e("code",{pre:!0,attrs:{class:"language-py"}},[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("merge")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("buffers"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data_tags"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pi"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data_merger"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          tag_merger"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n  Merge data and tags in `data_tags` to `buffers` according to the position map `pi`.\n\n  `data_merger` and `tag_merger` are defined by the host PDP or POR algorithm.\n\n  Return `buffers` after merging.\n  '''")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# for each data-tag pair")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_tags"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    data_tag "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_tags"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    logic_p "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pi"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# get logic position")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# for each dimension of the logic position")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" j "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("logic_p"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      k "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("logic_p"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("j"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# the partial position at dimension `j`")]),t._v("\n      "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# merge data field and tag field")]),t._v("\n      buffers"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("j"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("k"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_merger"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("buffers"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("j"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("k"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                          data_tag"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      buffers"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("j"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("k"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'tag'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tag_merger"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("buffers"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("j"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("k"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'tag'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data_tag"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'tag'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" buffers\n")])]),t._v(" "),e("p",[t._v("算法4：S端检测C端错误数据块的算法。此算法接受的参数包括缓冲区阵列"),e("code",{pre:!0},[t._v("buffers")]),t._v("和由宿主方案定义的数据域标签域匹配检测函数"),e("code",{pre:!0},[t._v("match")]),t._v("。返回值为一个失配缓冲区的编号列表"),e("code",{pre:!0},[t._v("mismatch_buffers")]),t._v("。")]),t._v(" "),e("pre",{staticClass:"language-py"},[e("code",{pre:!0,attrs:{class:"language-py"}},[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("check_buffers")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("buffers"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" match"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n  `match` is the match function defined by the host PDP or POR algorithm.\n\n  Return the index of mismatch buffers as a list of `(dimension, value)`.\n  '''")]),t._v("\n  results "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# for each dimension")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("buffers"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# for each value in this dimension")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" j "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("buffers"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# check buffer")]),t._v("\n      "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" match"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("buffers"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("j"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" buffers"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("j"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'tag'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        results"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("i"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" j"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" results\n")])]),t._v(" "),e("h4",{attrs:{id:"dhdd中的补偿攻击"}},[t._v("DHDD中的补偿攻击")]),t._v(" "),e("h5",{attrs:{id:"最优补偿攻击设计思路"}},[t._v("最优补偿攻击设计思路")]),t._v(" "),e("p",[t._v("在前文所描述的全局合并方案和简单随机合并方案中，每个数据块标签对都只被合并了一次。而类似于高次随机合并方案，在DHDD中，假设维度为$x$，则每个数据块标签对会被合并$x$次。如果C端在准备阶段上传的数据块标签对中存在逻辑值非$0$的数据块标签对，则对于每个逻辑值非$0$的数据块标签对，有$x$个缓冲区会受到逻辑值污染，这$x$个缓冲区都需要其他逻辑值非$0$的数据块标签对来补偿逻辑值才能够在验证阶段避过S端的检测。")]),t._v(" "),e("p",[t._v("令合并了逻辑值非$0$数据块标签对的缓冲区为受攻击的缓冲区；令受攻击的缓冲区中数据域和标签域仍然匹配的缓冲区为被成功攻击的缓冲区；令受攻击的缓冲区中数据域和标签域不匹配的缓冲区为未被成功攻击的缓冲区；令没有合并逻辑值非$0$数据块标签对的缓冲区为未受攻击的缓冲区。显然如果S端在验证阶段没有检测出任何失配缓冲区、即C端的补偿攻击成功时，S端的所有缓冲区要么是被成功攻击的缓冲区，要么是未受攻击的缓冲区。")]),t._v(" "),e("p",[t._v("根据安全模型，C端知道S端使用了DHDD来防御补偿攻击，且C端会使用攻击成功率最高的补偿攻击方案。")]),t._v(" "),e("p",[t._v("假设C端成功对系统发起了$k$阶补偿攻击，那么S端所有被成功攻击的缓冲区中，逻辑值非$0$的数据块标签对数量必然不小于$k$，因为如果存在一个被成功攻击的缓冲区中逻辑值非$0$的数据块标签对数量小于$k$，就说明对于C端上传的数据块标签对集合，能够对系统发起广义补偿攻击的数据块标签对子集的基数最小值小于$k$而不等于$k$，违背了$k$阶补偿攻击的定义。")]),t._v(" "),e("p",[t._v("由于每个被成功攻击的缓冲区中逻辑值非$0$的数据块标签对数量都不小于$k$，且S端随机排列所有数据块标签对之后恰好互相补偿的$k$个逻辑值非$0$的数据块标签对在同一个缓冲区的概率远大于多于$k$个逻辑值非$0$数据块标签对在同一个缓冲区中的概率，所以为了尽可能提升攻击成功率，最优补偿攻击必然会保证：在攻击成功的前提下，在逻辑值非$0$数据块标签对尽可能分散在不同缓冲区这样的最坏情况中，有尽可能多的被攻击的缓冲区中恰好有$k$个逻辑值非$0$的数据块标签对。")]),t._v(" "),e("p",[t._v("其次，对于不同阶的补偿攻击，由于低阶补偿攻击需要更少的逻辑值非$0$的数据块标签对在同一个缓冲区中，所以在被成功攻击的缓冲区数量相同的情况下，低阶补偿攻击的攻击成功率会远高于高阶补偿攻击的成功率。")]),t._v(" "),e("p",[t._v("最后，因为补偿攻击成功时每个被攻击的缓冲区都必须是被成功攻击的缓冲区，所以为了提升攻击成功率，一个优秀的补偿攻击在攻击成功时会造成尽可能少的被攻击的缓冲区。")]),t._v(" "),e("p",[t._v("综上所述，一个优秀的补偿攻击方案应该具备的条件为：")]),t._v(" "),e("ol",[e("li",[t._v("阶数$k$尽可能低。")]),t._v(" "),e("li",[t._v("攻击成功时被攻击缓冲区数量最大值尽可能小。")]),t._v(" "),e("li",[t._v("最坏情况下，所有被攻击缓冲区中，逻辑值非$0$的数据块标签对数量恰好为$k$的缓冲区占比尽可能大。")])]),t._v(" "),e("p",[t._v("如果存在一个补偿攻击方案，能够证明：")]),t._v(" "),e("ol",[e("li",[t._v("此方案的阶数$k$不大于其他所有补偿攻击方案的阶数。")]),t._v(" "),e("li",[t._v("此方案攻击成功时，造成的被攻击缓冲区数量最大值，是所有补偿攻击方案攻击成功时造成的被攻击缓冲区数量最大值的最小值。")]),t._v(" "),e("li",[t._v("最坏情况下，此方案攻击成功时，所有被攻击缓冲区中都仅有$k$个逻辑值非$0$的数据块标签对。")])]),t._v(" "),e("p",[t._v("那么此补偿攻击方案就是最优补偿攻击方案。")]),t._v(" "),e("h5",{attrs:{id:"最优补偿攻击"}},[t._v("最优补偿攻击")]),t._v(" "),e("p",[t._v("下面使用贪心算法构建最优补偿攻击。")]),t._v(" "),e("p",[t._v("假设C端上传的数据块标签对中已经存在一个逻辑值为$+1$的数据块标签对A位于$(a_1,a_2,\\cdots,a_x)$。对于任意维度$x_i$（$1 \\le i \\le x$），当合并$x_i = a_i$的所有数据块标签对时，会产生$x$个被攻击缓冲区。为了补偿这些缓冲区，使这些被攻击缓冲区成为被成功攻击的缓冲区，且产生的新的被攻击缓冲区数量最少（因为我们希望C端使用此方案发起补偿攻击后S端在验证阶段没有检测出失配缓冲区时被成功攻击的缓冲区数量的最大值最小），引入一个逻辑值为$-1$的数据块标签对B$(b_1, b_2, \\cdots, b_x)$，且B会试图在尽可能多的维度$i$上满足$a_i = b_i$，使A产生的被攻击缓冲区被补偿为被成功攻击的缓冲区。由于所有数据块标签对的逻辑坐标都不相同，所以数据块标签对B和数据块标签对A至少有一个维度的逻辑坐标分量不同。假设这个维度为$x_i$，则：")]),t._v(" "),e("p",[t._v("$$\na_1 = b_1\\\\\\\na_2 = b_2\\\\\\\n\\vdots\\\\\\\na_{i-1} = b_{i-1}\\\\\\\na_i \\ne b_i\\\\\\\na_{i+1} = b_{i+1}\\\\\\\n\\vdots\\\\\\\na_{x-1} = b_{x-1}\\\\\\\na_x = b_x\n$$")]),t._v(" "),e("p",[t._v("在这种情况下，有$x-1$个被攻击缓冲区为被成功攻击的缓冲区；有$2$个被攻击缓冲区为未被成功攻击的缓冲区，这两个未被成功攻击的缓冲区分别是合并逻辑坐标在维度$x_i$上的分量为$a_i$的所有数据块标签对的缓冲区，和合并逻辑坐标在维度$x_i$上的分量为$b_i$的所有数据块标签对的缓冲区。")]),t._v(" "),e("p",[t._v("为了补偿上述两个未被成功攻击的缓冲区，引入一个逻辑值为$-1$的数据块标签对C$(c_1, c_2, \\cdots, c_x)$和一个逻辑值为$+1$的数据块标签对D$(d_1, d_2, \\cdots, d_c)$。数据块标签对C负责补偿合并逻辑坐标在维度$x_i$上的分量为$a_i$的所有数据块标签对的缓冲区，即$c_i = a_i$。数据块标签对D负责补偿合并逻辑坐标在维度$x_i$上的分量为$b_i$的所有数据块标签对的缓冲区，即$d_i = b_i$。为了不产生新的未被成功攻击的缓冲区，数据块标签对C和数据块标签对D的逻辑坐标在维度$x_i$以外的分量均相同，即：")]),t._v(" "),e("p",[t._v("$$\nc_1 = d_1\\\\\\\nc_2 = d_2\\\\\\\n\\vdots\\\\\\\nc_{i-1} = d_{i-1}\\\\\\\na_i = c_i \\ne d_i = b_i\\\\\\\nc_{i+1} = d_{i+1}\\\\\\\n\\vdots\\\\\\\nc_{x-1} = d_{x-1}\\\\\\\nc_x = d_x\n$$")]),t._v(" "),e("p",[t._v("由于要分析C端发起补偿攻击后S端在验证阶段没有检测出失配缓冲区时被成功攻击的缓冲区数量的最大值的最小值，所以需要计算此补偿攻击方案在验证阶段没有检测出失配缓冲区时被成功攻击的缓冲区数量的最大值。此时需要假设数据块标签对C和数据块标签对D的逻辑坐标在维度$x_i$以外的分量和数据块标签对A与数据块标签对B的逻辑坐标在相同维度的分量不同。即，最坏情况下，此补偿攻击方案上传的逻辑值非$0$的数据块标签对A、B、C、D在被S端随机排列后，$x$维坐标需要满足如下关系：")]),t._v(" "),e("p",[t._v("$$\na_1 = b_1 \\ne c_1 = d_1\\\\\\\na_2 = b_2 \\ne c_2 = d_2\\\\\\\n\\vdots\\\\\\\na_{i-1} = b_{i-1} \\ne c_{i-1} = d_{i-1}\\\\\\\na_i = c_i \\ne b_i = d_i\\\\\\\na_{i+1} = b_{i+1} \\ne c_{i+1} = d_{i+1}\\\\\\\n\\vdots\\\\\\\na_{x-1} = b_{x-1} \\ne c_{x-1} = d_{x-1}\\\\\\\na_x = b_x \\ne c_x = d_x\\\\\\\n$$")]),t._v(" "),e("p",[t._v("所以此补偿攻击方案在如上最坏情况下，被成功攻击的缓冲区数量达到最大值，为$2x$。")]),t._v(" "),e("p",[t._v("由于在设计数据块标签对B的逻辑坐标时使用了贪心的思想，所以如果数据块标签对B的逻辑坐标和数据块标签对A的逻辑坐标在超过了$1$个维度上的分量不同，就会导致只引入数据块标签对C和数据块标签对D是不足以补偿数据块A和数据块标签对B产生的未被成功攻击的缓冲区，所以需要引入更多的逻辑值非$0$的数据块标签对。引入更多逻辑值非$0$的数据块标签对会使它们的$x$维位置关系要求变得复杂，导致补偿攻击方案在验证阶段没有检测出失配缓冲区时被成功攻击的缓冲区数量最大值大于$2x$。")]),t._v(" "),e("p",[t._v("综上所述，如果C端上传的数据块标签对中仅包含$4$个逻辑值非$0$的数据块标签对，且它们的逻辑值分别为$+1,-1,-1,+1$，就可以满足最优补偿攻击的三个条件：")]),t._v(" "),e("ol",[e("li",[t._v("此方案是阶数是$2$，为补偿攻击最低阶数，不大于其他所有补偿攻击方案的阶数。")]),t._v(" "),e("li",[t._v("得益于上述贪心的过程，此方案攻击成功时，造成的被攻击缓冲区数量最大值，是所有补偿攻击方案攻击成功时造成的被攻击缓冲区数量最大值的最小值。")]),t._v(" "),e("li",[t._v("最坏情况下，此方案攻击成功时，所有的$2x$个被攻击的缓冲区都仅有$2$个逻辑值非$0$的数据块标签对。")])]),t._v(" "),e("p",[t._v("所以此补偿攻击方案对于DHDD为最优补偿攻击方案，其攻击成功率最高。C端会使用此方案对系统进行攻击。")]),t._v(" "),e("h4",{attrs:{id:"安全性"}},[t._v("安全性")]),t._v(" "),e("p",[t._v("此处仅讨论系统针对上述最优补偿攻击的防御能力。对重放攻击、伪造攻击、删除攻击等攻击形式的防御由宿主方案提供。")]),t._v(" "),e("p",[t._v("假设C端在准备阶段上传了$n$个数据块标签对，且发动了最优补偿攻击，即这$n$个数据块标签对中包含$4$个逻辑值非$0$的数据块标签对，且它们的逻辑值分别为$+1,-1,-1,+1$。令$N$是不小于$n$的最小的$3$的整数次方，令$x = log_3(N)$为维度数量。除了C端上传的$n$个数据块标签对会对应$n$个$x$维空间坐标，剩余的$N - n$个$x$维坐标由任意逻辑值为$0$的数据块标签对一一对应。将这$N$个$x$维空间坐标对应的所有数据块标签对称为逻辑数据块标签对。在所有逻辑数据块标签对中的逻辑值非$0$的逻辑数据块标签对数量和C端在准备阶段上传的逻辑值非$0$的数据块标签对数量相同，都是$4$个。C端在$n$个数据块标签对中发起最优补偿攻击和在$N$个逻辑数据块标签对中发起最优补偿攻击，攻击成功率是相同的。令这$N$个逻辑数据块标签对分别为$q_1, q_2, \\cdots, q_N$。因为S端对这$N$个数据块标签对进行了随机排列，所以一共有$N!$种排列方式。")]),t._v(" "),e("p",[t._v("下面计算C端使用最优补偿攻击且攻击成功时这$N$个逻辑数据块标签对的排列方式数量。")]),t._v(" "),e("p",[t._v("假设$N$个逻辑数据块标签对中的$q_1, q_2, q_3, q_4$逻辑值分别为$+1, -1, -1, +1$，从而试图发起最优补偿攻击。令符号$q_{i, j}$表示逻辑数据块标签对$q_i$的逻辑坐标在维度$j$上的分量，其中$1 \\le i \\le N$，$1 \\le j \\le x$。")]),t._v(" "),e("p",[t._v("假如我们首先确定$q_1$的逻辑位置，它有$N$种可能的逻辑位置取值。")]),t._v(" "),e("p",[t._v("$q_4$的逻辑位置至少在两个维度上的分量和$q_1$的逻辑位置在相同维度上的分量不同。\n证明：首先因为所有逻辑数据块标签对的逻辑位置是唯一的，所以$q_1$和$q_4$的逻辑位置至少在一个维度上的分量不同。如果$q_1$和$q_4$的逻辑位置仅在一个维度上的分量不同，假设此维度是$x_1$，即$q_{1,1} \\ne q_{4,1}$，在其他维度$i$上$q_{1,i}=q_{4,i}$，其中$2 \\le i \\le x$，那么合并所有在维度$x_i$上取值为$q_{1,i}$的逻辑数据块标签对的$x-1$个缓冲区的逻辑值为$+2$。为了补偿这$x-1$个缓冲区，$q_2$和$q_3$的逻辑坐标在维度$x_i$上的分量也必须为$q_{1,i}$，即$q_{1,i}=q_{2,i}=q_{3,i}=q_{4,i}$。由于所有逻辑数据块标签对的逻辑坐标唯一，所以$q_1,q_2,q_3,q_4$的逻辑坐标仅在维度$x_1$上的分量不同，即$q_{1,1} \\ne q_{2,1} \\ne q_{3,1} \\ne q_{4,1}$。那么当合并在维度$x_1$上值分别为$q_{1,1},q_{2,1},q_{3,1},q_{4,1}$的所有逻辑数据块标签对时对应缓冲区的逻辑值分别为$+1,-1,-1,+1$而不可能为$0$，所以如果$q_4$的逻辑位置和$q_1$的逻辑位置只有一个维度的分量不同，S端必然能够在验证阶段检测出C端发起的最优补偿攻击。\n而如果$q_4$的逻辑位置和$q_1$的逻辑位置有两个维度的分量不同，假设这两个维度是$x_1$和$x_2$，那么根据上文的结论$q_1, q_2, q_3, q_4$的逻辑坐标仅在维度$x_1$和$x_2$上的分量不同。比如$q_1, q_2, q_3, q_4$在维度$x_1, x_2$上的分量分别为$(0, 0),(0, 1),(1, 0),(1, 1)$就可以保证所有缓冲区的逻辑值为$0$，C端攻击成功。如果$q_1$和$q_4$的逻辑位置有两个以上维度的分量不同，易证C端也可以攻击成功。所以$q_4$的逻辑位置至少在两个维度上的分量和$q_1$的逻辑位置在相同维度上的分量不同。")]),t._v(" "),e("p",[t._v("如果$q_4$的逻辑位置有$y$个维度和$q_1$不同，在其余$x-y$个维度和$q_1$相同，因为每个维度只有三种取值，所以$q_4$的逻辑位置有$C_x^y \\cdot (3-1)^y = C_x^y \\cdot 2^y$种，其中$2 \\le y \\le x$。令这$y$个维度分别为$i_1, i_2, \\cdots, i_y$，则：")]),t._v(" "),e("p",[t._v("$$\nq_{1, i_1} \\ne q_{4, i_1} \\\\\\\\\\\nq_{1, i_2} \\ne q_{4, i_2} \\\\\\\\\\\n\\vdots \\\\\\\\\\\nq_{1, i_y} \\ne q_{4, i_y} \\\\\\\\\\\n$$")]),t._v(" "),e("p",[t._v("根据最优补偿攻击中的描述，$q_2$的逻辑坐标在每个维度的分量要么和$q_1$相同，要么和$q_4$相同。由于$q_1$和$q_4$的逻辑坐标仅在$i_1, i_2, \\cdots, i_y$这$y$个维度的分量不同，所以在这$y$个维度以外的$x-y$个维度上$q_2$的逻辑坐标分量是确定的。而在$i_1, i_2, \\cdots, i_y$这$y$个维度上，每个维度$q_2$的逻辑坐标分量都有两种取值，分别是对应维度$q_1$的逻辑坐标分量和对应维度$q_4$的逻辑坐标分量，所以$q_2$的逻辑位置有$2^y - 2$种情况，减去的$2$表示了$q_2$的逻辑位置不能和$q_1$与$q_4$的逻辑位置重合。")]),t._v(" "),e("p",[t._v("假设$q_2$在$i_1, i_2, \\cdots, i_y$这$y$个维度中有$z$个维度的逻辑坐标分量和$q_1$的对应维度逻辑坐标分量相同，在$y-z$个维度上的逻辑坐标分量和$q_4$的对应维度逻辑坐标分量相同，则只要$q_3$在这$y-z$个维度上的逻辑坐标分量和$q_1$在对应维度的逻辑坐标分量相同，在剩余的$z$个维度上的逻辑坐标分量和$q_4$在对应维度的逻辑坐标分量相同，就可以实现S端所有缓冲区的数据域和标签域都匹配、C端攻击成功。所以当$q_1, q_2, q_4$的位置确定时$q_3$的位置也被确定，$q_3$的逻辑位置恰好为$q_2$逻辑位置关于$q_1$和$q_4$逻辑位置中心点的镜像。")]),t._v(" "),e("p",[t._v("其他$N - 4$个逻辑值为$0$的逻辑数据块标签对有$(N-4)!$种逻辑位置取值，所以DHDD的最优补偿攻击成功概率：")]),t._v(" "),e("p",[t._v("$$\n\\begin{align}\nP & = \\sum_{y=2}^{x} \\frac{N \\cdot C_x^y \\cdot 2^y \\cdot (2^y - 2) (N-4)!}{N!} \\\\\\\n& = \\frac{\\sum_{y=2}^{x} (C_x^y \\cdot 2^y \\cdot (2^y - 2))}{(N-1)(N-2)(N-3)} \\\\\\\n\\end{align}\n$$")]),t._v(" "),e("p",[t._v("为了方便计算，在$N = 3^x$较大时我们可以简化上述计算：")]),t._v(" "),e("p",[t._v("$$\n\\begin{align}\nP & = \\frac{\\sum_{y=2}^{x} (C_x^y \\cdot 2^y \\cdot (2^y - 2))}{(N-1)(N-2)(N-3)} \\\\\\\n& \\approx \\frac{\\sum_{y=0}^{x} (C_x^y \\cdot 2^y \\cdot (2^y))}{(N-1)(N-2)(N-3)} \\\\\\\n& = \\frac{\\sum_{y=0}^{x} (C_x^y \\cdot 2^y \\cdot 2^y)}{(N-1)(N-2)(N-3)} \\\\\\\n& = \\frac{\\sum_{y=0}^{x} (C_x^y \\cdot 4^y)}{(N-1)(N-2)(N-3)} \\\\\\\n& = \\frac{(1+4)^x}{(N-1)(N-2)(N-3)} \\\\\\\n& = \\frac{5^x}{(N-1)(N-2)(N-3)} \\\\\\\n& \\approx \\frac{5^x}{(3^x)^3} \\\\\\\n& = \\frac{5^x}{27^x} \\\\\\\n& = (\\frac{5}{27})^x \\\\\\\n\\end{align}\n$$")]),t._v(" "),e("p",[t._v("算法5为使用公式得到的具体攻击成功率计算算法，此算法接受参数"),e("code",{pre:!0},[t._v("x")]),t._v("作为维度数量，在控制台以分数的形式输出经过约分化简后的DHDD最优补偿攻击成功率。")]),t._v(" "),e("p",[t._v("算法5：DHDD的最优补偿攻击成功概率。")]),t._v(" "),e("pre",{staticClass:"language-python"},[e("code",{pre:!0,attrs:{class:"language-python"}},[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" math\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" fractions "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Fraction\n\n\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("theoretical_probability")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n  `x` should be larger than `1`.\n  Print result as the format: `successful_attack/total`.\n  '''")]),t._v("\n  n "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v("x\n  numerator "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" y "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# `math.comb` is only available when python >= 3.8")]),t._v("\n    numerator "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" math"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("comb"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v("y "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v("y "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  denominator "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  result "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Fraction"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("numerator"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" denominator"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'%s/%s\\n'")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("result"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("numerator"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" result"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("denominator"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),e("p",[t._v("算法6使用穷举法验证DHDD的最优补偿攻击成功率。因为维度为$x$时逻辑数据块标签对有$N = 3^x$个，所以会有$N!$种排列方式，在$x=3$时就需要$10^{28}$次验证，所以需要减少验证所需计算量。\n下面的算法仅验证$4$个逻辑值非$0$的数据块标签对的逻辑位置，在维度为$x$时这$4$个错误的数据块标签对的位置有$C_{N}^4 \\cdot 4! = N(N - 1)(N - 2)(N - 3)$种，故需要约$N^4$次计算。\n为了加速得到结果，使用多进程计算进行验证。算法中的"),e("code",{pre:!0},[t._v("match")]),t._v("、"),e("code",{pre:!0},[t._v("data_merger")]),t._v("和"),e("code",{pre:!0},[t._v("tag_merger")]),t._v("为宿主方案提供的判断数据块标签对是否匹配的函数、数据域合并函数和标签域合并函数；函数"),e("code",{pre:!0},[t._v("mapper")]),t._v("为工作进程函数，每次调用负责处理$C_N^4$种情况中的一种。函数"),e("code",{pre:!0},[t._v("mapper_initializer")]),t._v("负责初始化"),e("code",{pre:!0},[t._v("mapper")]),t._v("函数中的必要变量。函数"),e("code",{pre:!0},[t._v("multi_process_exhaustive_test")]),t._v("为入口函数，负责多进程的调度，在控制台以分数的形式输出经过约分化简后的DHDD最优补偿攻击成功率。")]),t._v(" "),e("p",[t._v("算法6：通过多进程穷举计算DHDD最优补偿攻击成功率。")]),t._v(" "),e("pre",{staticClass:"language-python"},[e("code",{pre:!0,attrs:{class:"language-python"}},[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" multiprocessing"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" itertools\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" fractions "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Fraction\n\n\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("match")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tag"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("bool")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n  This function is defined by the host PDP or POR algorithm.\n  '''")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" data "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" tag\n\n\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("data_merger")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("original"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n  This function is defined by the host PDP or POR algorithm.\n  '''")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" original "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" data\n\n\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("tag_merger")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("original"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tag"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n  This function is defined by the host PDP or POR algorithm.\n  '''")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" original "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" tag\n\n\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("mapper")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("position_set"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n  Process a position set and return successful attack count and total count.\n\n  `position_set`: 4 logic positions.\n  '''")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# init result")]),t._v("\n  total "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n  successful_atk "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# permutate these 4 logic positions")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" position_permutation "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" itertools"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("permutations"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("position_set"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                                     "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("position_set"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# construct logic position map")]),t._v("\n    pi "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("zip")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("position_permutation"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                  position_permutation"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# pi[index] = str(logic_position)")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# init buffers")]),t._v("\n    buffers "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" init_buffers"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mapper"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("x"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# merge")]),t._v("\n    buffers "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" merge"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("buffers"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mapper"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataset"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pi"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data_merger"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tag_merger"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# check whether mismatch buffer exists")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("check_buffers"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("buffers"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" match"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# successful attack")]),t._v("\n      successful_atk "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# increase counter")]),t._v("\n    total "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" successful_atk"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" total\n\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ref: https://stackoverflow.com/a/3843313/12407789")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("mapper_initializer")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dataset"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n  Because we use `imap_unordered` to pass iterable values to `mapper`, so `mapper` can only has one parameter.\n  Consider the function `mapper` as a callerable *Object*.\n  Assign attribute to it to realize passing arguments not via parameter.\n  '''")]),t._v("\n  mapper"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataset "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dataset\n  mapper"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("x "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x\n\n\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("multi_process_exhaustive_test")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n  `x` should be larger than `1`.\n  Print result as the format: `successful_attack/total`.\n  '''")]),t._v("\n\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# init dataset of size 4")]),t._v("\n  dataset "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'tag'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n  dataset"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dataset"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# two data is 1")]),t._v("\n  dataset"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dataset"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# two data is -1")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# init process pool")]),t._v("\n  proc_pool "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" multiprocessing"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Pool"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("initializer"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("mapper_initializer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                   initargs"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("dataset"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# init all logic position")]),t._v("\n  logic_p "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" v "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" itertools"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("product"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'012'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" repeat"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("x"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# select 4 logic positions from all logic positions and test")]),t._v("\n  results "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" proc_pool"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("imap_unordered"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mapper"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                     itertools"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("combinations"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("logic_p"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                     chunksize"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  successful_atk "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n  total "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" result "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" results"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    successful_atk "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" result"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    total "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" result"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# output result, simplify the fraction")]),t._v("\n  result "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Fraction"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("successful_atk"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" total"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'%s/%s\\n'")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("result"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("numerator"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" result"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("denominator"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),e("p",[t._v("经过计算，$10$维以下的DHDD最优补偿攻击成功率见表3-2。表格中没有"),e("code",{pre:!0},[t._v("*")]),t._v("的攻击成功率表示理论值和穷举验证结果相符，带有"),e("code",{pre:!0},[t._v("*")]),t._v("的攻击成功率表示因为穷举时情况太多、所需时间过长而没有经过穷举验证的理论值，这也侧面说明了DHDD防御能力的强大。")]),t._v(" "),e("p",[t._v("表3-2：$10$维以下DHDD最优补偿攻击成功率（"),e("code",{pre:!0},[t._v("*")]),t._v("表示未经穷举验证的理论值）。")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("维度x")]),t._v(" "),e("th",[t._v("最优补偿攻击成功率P")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("2")]),t._v(" "),e("td",[t._v("1/42")])]),t._v(" "),e("tr",[e("td",[t._v("3")]),t._v(" "),e("td",[t._v("3/650")])]),t._v(" "),e("tr",[e("td",[t._v("4")]),t._v(" "),e("td",[t._v("29/30810")])]),t._v(" "),e("tr",[e("td",[t._v("5")]),t._v(" "),e("td",[t._v("1/5302")])]),t._v(" "),e("tr",[e("td",[t._v("6")]),t._v(" "),e("td",[t._v("23/623766*")])]),t._v(" "),e("tr",[e("td",[t._v("7")]),t._v(" "),e("td",[t._v("439/62093330*")])]),t._v(" "),e("tr",[e("td",[t._v("8")]),t._v(" "),e("td",[t._v("11797/8817854010*")])]),t._v(" "),e("tr",[e("td",[t._v("9")]),t._v(" "),e("td",[t._v("3987/15881819122*")])]),t._v(" "),e("tr",[e("td",[t._v("10")]),t._v(" "),e("td",[t._v("109631/2339434227702*")])])])]),t._v(" "),e("p",[t._v("结论：在DHDD方案中，如果维度$x=10$，则S端在验证阶段只需30次验证，即可验证$3^{10}$（约60000）个数据块，且C端发起的最优补偿攻击成功的概率约等于中国国内中奖概率最低的大乐透中奖（约2142万分之一）。")]),t._v(" "),e("p",[t._v("在实际使用中，S端可以设置一个阈值。如$3^{10}$。当C端在准备阶段上传的数据块标签对数量小于$3^{10}$时始终使用$10$维验证，数据块标签对数量高于$3^{10}$时维度数量动态增加。")]),t._v(" "),e("h4",{attrs:{id:"自我最优性"}},[t._v("自我最优性")]),t._v(" "),e("p",[t._v("DHDD方案不像随机合并方案一样可以通过重复调用来提升性能，因为在上文DHDD的基本原理中我们已经使用了和高次随机合并方案类似的思想，所以DHDD具有自我最优性。")]),t._v(" "),e("p",[t._v("这意味着，当C端在准备阶段上传的数据块标签对数量变大时，提升DHDD方案的维度和重复调用低维方案性能相同。例如处理$3^x$个数据块标签对需要$x$个维度，验证$3x$次，C端最优补偿攻击成功率约为$(5/27)^x$；如果S端在验证阶段调用两次$x$维验证，则C端发起最优补偿攻击成功率约为$(5/27)^{2x}$，S端验证次数为$2 \\cdot 3x = 6x$次。为了得到同样的成功率，S端也可以在验证阶段使用一次$2x$维验证，C端发起最优补偿攻击成功率同样约为$(5/27)^{2x}$，且S端验证次数同样为$3 \\cdot 2x = 6x$次。")]),t._v(" "),e("h4",{attrs:{id:"对比高次随机合并方案"}},[t._v("对比高次随机合并方案")]),t._v(" "),e("p",[t._v("不难看出，DHDD和高次随机合并方案使用了相似的设计思想，所以它们拥有相似的S端计算量计算公式。$x$维DHDD方案就像是$x$次随机合并方案，二者的S端计算量是相同的。只不过$x$次随机合并方案中，每一次随机合并和其他随机合并之间都是没有关联的，而$x$维DHDD方案中，每一个维度和其他维度都是有关联的，这就导致DHDD方案中C端在准备阶段上传的数据块标签对需要满足更复杂的逻辑值与逻辑位置关系才能够避开S端在验证阶段的检测，所以在S端验证次数相同的前提下，$x$维DHDD方案对于最优补偿攻击的防御能力会高于$x$次随机合并方案。")]),t._v(" "),e("p",[t._v("但是由于$x$维DHDD方案的复杂逻辑位置限制也决定了它只能处理$3^x$个数据块标签对，如果C端上传的数据块标签对数量更多，就需要提升维度，虽然这可以导致攻击成功率的进一步降低，但是也会导致S端在验证阶段计算量的提升，而在$x$维DHDD攻击成功率已经很低的情况下进一步降低攻击成功率可能意义不是很大，此时数据块标签对逻辑位置的复杂约束导致的数据块标签对数量限制变成了DHDD方案的性能瓶颈。因为高次随机合并方案没有数据块数量限制，因此可以将DHDD方案与高次随机合并方案配合使用，当C端在准备阶段上传的数据块标签对数量低于某个阈值时，使用相同S端计算量下防御能力更强的DHDD方案；当C端在准备阶段上传的数据块标签对数量高于此阈值时，追求更高的防御能力可能意义不大，此时可以使用计算量更小、防御能力可以接受的高次随机合并方案。")]),t._v(" "),e("h3",{attrs:{id:"应用场景"}},[t._v("应用场景")]),t._v(" "),e("h4",{attrs:{id:"宿主方案需求"}},[t._v("宿主方案需求")]),t._v(" "),e("p",[t._v("综上所述，如果一个已经存在的远程数据完整性验证方案希望作为基于同态合并的双向验证解决方案的宿主方案，与高次随机合并方案或DHDD结合使用，那么此远程数据完整性验证方案需要满足如下条件：")]),t._v(" "),e("ol",[e("li",[t._v("使用同态验证技术进行远程数据完整性审计以满足本系统的系统模型。")]),t._v(" "),e("li",[t._v("支持公开审计以满足本系统的安全模型。")]),t._v(" "),e("li",[t._v("验证数据域与标签域是否匹配的算法拥有较低的时间复杂度以满足基于同态合并的双向验证解决方案的计算量基本需求。")])]),t._v(" "),e("p",[t._v("只要一个已经存在的远程数据完整性验证方案满足上述所有条件即可作为基于同态合并的双向验证解决方案的宿主方案。比如基于RSA的S-PDP[8]和基于BLS的POR[9]就都可以作为基于同态合并的双向验证解决方案的宿主方案。下文将以基于RSA的S-PDP为例，描述DHDD方案在实际使用中的情况。")]),t._v(" "),e("h4",{attrs:{id:"示例dhdd结合基于rsa的s-pdp"}},[t._v("示例：DHDD结合基于RSA的S-PDP")]),t._v(" "),e("h5",{attrs:{id:"算法流程"}},[t._v("算法流程")]),t._v(" "),e("p",[t._v("首先我们来回顾一下基于RSA的S-PDP。")]),t._v(" "),e("p",[t._v("经过简化，基于RSA的S-PDP在准备阶段的流程如下：")]),t._v(" "),e("ol",[e("li",[t._v("C端调用函数$KeyGen$得到RSA非对称密钥对$sk,pk$等算法参数。")]),t._v(" "),e("li",[t._v("C端调用函数$TagBlock$，使用RSA加密算法为每个数据块$m_i$计算对应的标签$T_{i,m_i}$。")]),t._v(" "),e("li",[t._v("C把数据块标签对上传到S端，并公布验证数据块标签对所需参数。")]),t._v(" "),e("li",[t._v("C端删除本地数据块标签对。")])]),t._v(" "),e("p",[t._v("DHDD不会对算法的挑战阶段造成影响，只会在C端删除本地数据块标签对之前在S端被调用以进行验证。所以结合了DHDD的基于RSA的S-PDP算法在准备阶段的流程如下：")]),t._v(" "),e("ol",[e("li",[t._v("C端调用函数$KeyGen$得到RSA非对称密钥对$sk,pk$等算法参数。")]),t._v(" "),e("li",[t._v("C端调用函数$TagBlock$，使用RSA加密算法为每个数据块$m_i$计算对应的标签$T_{i,m_i}$。")]),t._v(" "),e("li",[t._v("C把数据块标签对上传到S端，并公布验证数据块标签对所需参数。")]),t._v(" "),e("li",[t._v("S端使用DHDD验证C端上传的数据块标签对是否正确。")]),t._v(" "),e("li",[t._v("C端删除本地数据块标签对。")])]),t._v(" "),e("h5",{attrs:{id:"s端计算量评估"}},[t._v("S端计算量评估")]),t._v(" "),e("p",[t._v("为了使基于RSA的S-PDP能够与DHDD结合，基于RSA的S-PDP需要满足上文提到的“基于同态合并的双向验证解决方案的计算量基本需求”，即每次验证所需的时间复杂度较低。")]),t._v(" "),e("p",[t._v("基于RSA的S-PDP使用指数函数进行数据块标签对的验证。由于计算指数函数可以使用快速幂算法，其时间复杂度为$O(log(n))$，所以随着参数$n$的膨胀，计算指数函数所需的时间并没有大幅度增加，因此可以使用指数函数的调用次数代表计算量。")]),t._v(" "),e("p",[t._v("除此之外，虽然双向验证需要基于RSA的S-PDP的公开审计这一特性作为支撑，但是由于双向验证不使用抽样技术且只需要被调用一次，所以对数据块标签对的验证不必像原本基于RSA的S-PDP方案中的挑战阶段一样设置伪随机数函数和伪随机排列函数来保证算法的安全性。因此在计算单个数据块标签对是否匹配时，只需要使用RSA参数对标签进行解密即可，即验证下式是否成立：")]),t._v(" "),e("p",[t._v("$$\nT^e_{i,m_i} \\stackrel{?}{\\equiv} g^{m_i}h(W_i) (mod\\ N)\n$$")]),t._v(" "),e("p",[t._v("在合并多个数据块标签对并进行验证的时候，假设合并所有$n$个数据块标签对，需要执行的验证方程为：")]),t._v(" "),e("p",[t._v("$$\n(\\prod_{i=1}^n T_{i,m_i})^e \\stackrel{?}{\\equiv} g^{\\sum_{i=1}^n m_i} \\prod_{i=1}^n h(W_i) (mod\\ N)\n$$")]),t._v(" "),e("p",[t._v("所以，如果逐个验证数据块标签对是否匹配，需要$2n$次指数运算。而结合DHDD后，仅需要$6 \\cdot \\lceil log_3(n) \\rceil$次指数运算，因此使用DHDD可以很大程度上减少S端的计算压力。")]),t._v(" "),e("h2",{attrs:{id:"结束语"}},[t._v("结束语")]),t._v(" "),e("p",[t._v("双向验证是目前远程数据完整性审计方案研究中所缺少的一个重要功能特性。为了维护一个良好的云计算商业环境，我们在保障数据持有者的权益、为数据持有者提供云端数据完整性检测能力的同时，也需要保障云服务提供商的权益、为云服务提供商赋予检测恶意数据持有者的能力。")]),t._v(" "),e("p",[t._v("本文定义了双向验证及其系统模型、安全模型等相关性质，并提出了两种基于同态合并的双向验证方案，即高次随机合并方案和动态高维防御方案，二者的计算量和防御能力都较为理想，希望能为日后的研究提供研究思路与参考。")]),t._v(" "),e("p",[t._v("同时，虽然高次随机合并方案和动态高维防御方案都有很不错的检错能力，但是它们都不具备高效地定位错误的能力。如何定位错误的数据块标签对将会成为后续工作的一个方向。")]),t._v(" "),e("h2",{attrs:{id:"refs"}},[t._v("Refs")]),t._v(" "),e("ul",[e("li",[t._v("[1] 罗晓慧.浅谈云计算的发展[J].电子世界.2019(08):104.")]),t._v(" "),e("li",[t._v("[2] 谭霜,贾焰,韩伟红.云存储中的数据完整性证明研究及进展[J].计算机学报,2015,38(01):164-177.")]),t._v(" "),e("li",[t._v("[3] 本刊编辑部.云计算是什么[J].黑龙江档案,2019(05):111.")]),t._v(" "),e("li",[t._v("[4] 袁颖,朱洪亮,陈玉玲等.基于数据持有性证明的完整性验证技术综述[J].计算机工程与应用,2019,55(18):1-7+52.")]),t._v(" "),e("li",[t._v("[5] 秦志光,吴世坤,熊虎.云存储服务中数据完整性审计方案综述[J].信息网络安全,2014(07):1-6.")]),t._v(" "),e("li",[t._v("[6] Erway C C, Küpçü A, Papamanthou C, et al. Dynamic provable data possession[J]. ACM Transactions on Information and System Security (TISSEC), 2015, 17(4): 1-29.")]),t._v(" "),e("li",[t._v("[7] Cash D, Küpçü A, Wichs D. Dynamic proofs of retrievability via oblivious RAM[J]. Journal of Cryptology, 2017, 30(1): 22-57.")]),t._v(" "),e("li",[t._v("[8] Ateniese G, Burns R, Curtmola R, et al. Provable data possession at untrusted stores[C].Proceedings of the 14th ACM conference on Computer and communications security. 2007: 598-609.")]),t._v(" "),e("li",[t._v("[9] Shacham H, Waters B. Compact proofs of retrievability[C]. International Conference on the Theory and Application of Cryptology and Information Security. Springer, Berlin, Heidelberg, 2008: 90-107.")]),t._v(" "),e("li",[t._v("[10] Armknecht F, Bohli J M, Karame G O, et al. Outsourced proofs of retrievability[C]. Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security. 2014: 831-843.")]),t._v(" "),e("li",[t._v("[11] Armknecht F, Barman L, Bohli J M, et al. Mirror: Enabling proofs of data replication and retrievability in the cloud[C]. 25th USENIX Security Symposium (USENIX Security 16). 2016: 1051-1068.")]),t._v(" "),e("li",[t._v("[12] Shannon C E. Communication theory of secrecy systems[J]. Bell system technical journal, 1949, 28(4): 656-715.")])])])}]};t.exports={attributes:{title:"云环境下双向验证的远程数据完整性审计研究",description:"Research on Remote Data Integrity Auditing with Bidirectional Verification in Cloud"},vue:{render:e.render,staticRenderFns:e.staticRenderFns,component:{data:function(){return{templateRender:null}},render:function(t){return this.templateRender?this.templateRender():t("div","Rendering")},created:function(){this.templateRender=e.render,this.$options.staticRenderFns=e.staticRenderFns}}}}}}]);
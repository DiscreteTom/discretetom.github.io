(window.webpackJsonp=window.webpackJsonp||[]).push([[126],{1160:function(t,n){const e={render:function(){var t=this;t.$createElement;return t._self._c,t._m(0)},staticRenderFns:[function(){var t=this,n=t.$createElement,e=t._self._c||n;return e("div",{staticClass:"frontmatter-markdown"},[e("h2",{attrs:{id:"前言"}},[t._v("前言")]),t._v(" "),e("p",[t._v("本文是观看"),e("a",{attrs:{href:"https://www.bilibili.com/video/BV1a4411B7V9"}},[t._v("此视频")]),t._v("时的笔记")]),t._v(" "),e("h2",{attrs:{id:"kafka"}},[t._v("Kafka")]),t._v(" "),e("h3",{attrs:{id:"概述"}},[t._v("概述")]),t._v(" "),e("ul",[e("li",[t._v("分布式集群")]),t._v(" "),e("li",[t._v("基于pub/sub模式的消息队列(MQ)")]),t._v(" "),e("li",[t._v("Scala语言开发")]),t._v(" "),e("li",[t._v("需要ZooKeeper作为集中配置\n"),e("ul",[e("li",[t._v("保存集群的配置，比如每个topic的副本数，topic的leader在哪个broker等")]),t._v(" "),e("li",[t._v("Kafka版本低于0.9的时候，ZK还要保存消费者的进度(offset)，以便故障恢复\n"),e("ul",[e("li",[t._v("0.9之后进度保存在Kafka集群中（作为一个拥有50个分片的topic），避免消费者维护和ZK的连接，并高频请求ZK")])])])])]),t._v(" "),e("li",[t._v("消息队列的两种模式\n"),e("ul",[e("li",[t._v("点对点模式（一对一）\n"),e("ul",[e("li",[t._v("消息保存在Queue中，消费者取出消息，消费，然后删除Queue中的消息")]),t._v(" "),e("li",[t._v("一个消息只能被消费一次")])])]),t._v(" "),e("li",[t._v("发布/订阅模式(pub/sub)\n"),e("ul",[e("li",[t._v("生产者把消息发送到topic中，所有消费者都可以订阅消息")]),t._v(" "),e("li",[t._v("消费者可以使用轮询的模式拉取(pull)消息，或者直接接收topic的推送(push)")])])]),t._v(" "),e("li",[t._v("Kafka是拉取型pub/sub MQ")])])]),t._v(" "),e("li",[t._v("消息默认保存在磁盘，默认保存7天")]),t._v(" "),e("li",[t._v("监控：可以使用开源的Eagle")])]),t._v(" "),e("h3",{attrs:{id:"架构"}},[t._v("架构")]),t._v(" "),e("ul",[e("li",[t._v("一个Kafka集群包含多个Broker")]),t._v(" "),e("li",[t._v("一个Kafka集群可以设置多个Topic，横跨多个Broker。每个Broker上面都有Topic的不同分区\n"),e("ul",[e("li",[t._v("每个分区都至少被保存一份(leader)，可以在其他Broker上面保存副本(follower)")]),t._v(" "),e("li",[t._v("生产者和消费者都会访问leader。follower只是做高可用，不做负载均衡")])])]),t._v(" "),e("li",[t._v("多个消费者构成消费者组\n"),e("ul",[e("li",[t._v("一个消费者组负责消费某个topic的所有分区")]),t._v(" "),e("li",[t._v("一个分区只能被同一个消费者组里面的一个消费者消费，以确保每个消息仅被一个消费者组消费一次")]),t._v(" "),e("li",[t._v("无法实现全局有序（跨broker），但是可以实现broker内的数据有序")])])])]),t._v(" "),e("h3",{attrs:{id:"关键配置"}},[t._v("关键配置")]),t._v(" "),e("pre",{staticClass:"language-bash"},[e("code",{pre:!0,attrs:{class:"language-bash"}},[e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# file: server.properties")]),t._v("\nbroker.id"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 每个broker唯一的ID，整数")]),t._v("\ndelete.topic.enable"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("true "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 允许删除topic")]),t._v("\nlog.dirs"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("/tmp/kafka-logs "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Kafka暂存数据的目录")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 不是日志目录，包含日志")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 日志文件是server.log")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 也包含各个consumer的offset（版本>0.9）")]),t._v("\nlog.retention.hours"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("168")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 数据保存时间，不是日志")]),t._v("\nzookeeper.connect"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("localhost:2181 "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ZK地址")]),t._v("\n")])]),t._v(" "),e("ul",[e("li",[t._v("Kafka不被视为每个节点都保存数据（因为是分布式存储），所以把节点上的数据视为append only logs，所以数据目录也叫log")]),t._v(" "),e("li",[t._v("默认情况下日志和数据都保存在"),e("code",{pre:!0},[t._v("log.dirs")]),t._v("目录下。也可以配置日志和数据分离，需要修改配置、删除数据并关闭、重新启动集群。具体操作可以自行搜索")])]),t._v(" "),e("h3",{attrs:{id:"原理"}},[t._v("原理")]),t._v(" "),e("h4",{attrs:{id:"文件存储"}},[t._v("文件存储")]),t._v(" "),e("ul",[e("li",[t._v("在数据目录下（某个topic的某个分区下），包含如下文件\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("xxx.log")]),t._v(" "),e("ul",[e("li",[t._v("保存具体数据。只存数据不存边界")]),t._v(" "),e("li",[t._v("append only logs")]),t._v(" "),e("li",[t._v("会根据配置文件里面的配置"),e("code",{pre:!0},[t._v("log.segment.bytes")]),t._v("对数据进行分块(segment)\n"),e("ul",[e("li",[t._v("第一个文件是"),e("code",{pre:!0},[t._v("00000000000000000000.log")])]),t._v(" "),e("li",[t._v("假设第一个文件里面有6条消息(0-5)，第二个文件就是"),e("code",{pre:!0},[t._v("00000000000000000006.log")])])])])])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("xxx.index")]),t._v(" "),e("ul",[e("li",[t._v("保存索引，用来定位"),e("code",{pre:!0},[t._v("xxx.log")]),t._v("里面的数据")]),t._v(" "),e("li",[t._v("也就是"),e("code",{pre:!0},[t._v("第几个消息 => offset")])]),t._v(" "),e("li",[t._v("每一个索引的大小都是固定的，这样就可以直接用索引的偏移量快速查索引")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("000.index")]),t._v("对应"),e("code",{pre:!0},[t._v("000.log")]),t._v("，"),e("code",{pre:!0},[t._v("006.index")]),t._v("对应"),e("code",{pre:!0},[t._v("006.log")])]),t._v(" "),e("li",[t._v("假设需要查找第X个事件，则首先使用二分查找，定位到index文件，然后在index文件中直接根据索引偏移量，得到log文件中的偏移量")])])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("xxx.timeindex")])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("xxx.snapshot")])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("leader-epoch-checkpoint")])])])])]),t._v(" "),e("h4",{attrs:{id:"生产者"}},[t._v("生产者")]),t._v(" "),e("ul",[e("li",[t._v("通过SDK写数据的时候，有三种分区策略\n"),e("ul",[e("li",[t._v("直接使用分区ID（一个整数）指定分区")]),t._v(" "),e("li",[t._v("使用分区键，根据策略（哈希）分区")]),t._v(" "),e("li",[t._v("不指定分区，轮询分区写数据。第一个数据随机分区")])])]),t._v(" "),e("li",[t._v("副本同步策略，通常有两种策略\n"),e("ul",[e("li",[t._v("半数副本完成同步就视为写入成功，返回ACK\n"),e("ul",[e("li",[t._v("低延迟")]),t._v(" "),e("li",[t._v("选举新的Leader的时候，容忍n台节点的故障，需要2n+1个副本（需要有n+1个副本存活，才能保障数据是正确的）")])])]),t._v(" "),e("li",[t._v("全部副本完成同步才视为写入成功，返回ACK\n"),e("ul",[e("li",[t._v("延迟高")]),t._v(" "),e("li",[t._v("选举新的leader的时候，容忍n台节点的故障，需要n+1个副本（只要存在一个副本存活，数据就是正确的）")]),t._v(" "),e("li",[e("strong",[t._v("Kafka使用了此方案")]),t._v("，因为方案1的数据冗余太多，而网络延迟并不是Kafka的主要瓶颈\n"),e("ul",[e("li",[t._v("如果一个节点故障/延迟高，Kafka就永远不会返回ACK了？Kafka使用了ISR进行优化")])])])])])])]),t._v(" "),e("li",[t._v("生产者ACKS配置\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("0")]),t._v("：生产者不等待broker的ACK，实现最低延迟。broker接收到数据后，还没写入磁盘就返回OK。broker故障时丢失数据")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("1")]),t._v("：生产者仅等待leader的ACK。如果在第一个follower同步完毕之前leader出现故障，则丢失数据")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("all/-1")]),t._v("：生产者等待leader和follower全部写入完毕后的ACK。如果follower同步完毕后，leader发送ACK之前，leader故障，则数据可能重复")])])])]),t._v(" "),e("h4",{attrs:{id:"消费者"}},[t._v("消费者")]),t._v(" "),e("ul",[e("li",[t._v("consumer使用pull的方式从broker拉数据\n"),e("ul",[e("li",[t._v("push模式很难适应不同消费速度的消费者，而pull模式可以根据消费者自己的消费速度拉取数据")]),t._v(" "),e("li",[t._v("如果kafka里面没有数据，消费者可能会一直收到空数据\n"),e("ul",[e("li",[t._v("kafka的优化方式是，使用长请求，即消费者的消费请求中会带有一个timeout参数，如果当前集群没有数据，消费者会等待一段时间然后再返回空数据。避免一直收到空数据")])])])])]),t._v(" "),e("li",[t._v("分区分配策略（哪个分区由消费者组中的哪个消费者消费）\n"),e("ul",[e("li",[t._v("RoundRobin\n"),e("ul",[e("li",[t._v("假设有两个topic分别为t1, t2，它们都有3个分区，即t1-1, t1-2, t1-3和t2-1, t2-2, t2-3")]),t._v(" "),e("li",[t._v("由两个消费者A和B消费（在一个消费者组中）")]),t._v(" "),e("li",[t._v("这两个topic会被视为一个整体，即"),e("code",{pre:!0},[t._v("(t1-1, t1-2, t1-3, t2-1, t2-2, t2-3)")]),t._v("，然后分配给A和B（分配之前可能会先shuffle一下打乱分区数据）")]),t._v(" "),e("li",[t._v("A得到"),e("code",{pre:!0},[t._v("(t1-1, t1-3, t2-2)")]),t._v("，B得到"),e("code",{pre:!0},[t._v("(t1-2, t2-1, t2-3)")])]),t._v(" "),e("li",[t._v("所有消费者需要消费的分区数量差值最大为1")])])]),t._v(" "),e("li",[t._v("Range（默认）\n"),e("ul",[e("li",[t._v("对于每个topic，不打乱分区，把topic里面的分区尽可能平均地分发给消费者")]),t._v(" "),e("li",[t._v("假设有两个topic分别为t1, t2，它们都有3个分区，即t1-1, t1-2, t1-3和t2-1, t2-2, t2-3")]),t._v(" "),e("li",[t._v("由两个消费者A和B消费（在一个消费者组中）")]),t._v(" "),e("li",[t._v("t1会把t1-1 & t1-2交给A消费，把t1-3交给B消费")]),t._v(" "),e("li",[t._v("t2会把t2-1 & t2-2交给A消费，把t2-3交给B消费")]),t._v(" "),e("li",[t._v("可能会导致消费者负载不均衡。比如上述例子，A消费4个分区，B消费2个分区")])])]),t._v(" "),e("li",[t._v("消费者数量变更（比如消费者挂了）的时候，会重新分配分区")])])]),t._v(" "),e("li",[t._v("维护offset\n"),e("ul",[e("li",[t._v("以"),e("code",{pre:!0},[t._v("(消费者组, 主题, 分区)")]),t._v("为主键，维护offset\n"),e("ul",[e("li",[t._v("这样即使消费者组被重新分区，也不会重复读数据")])])]),t._v(" "),e("li",[t._v("版本大于0.9时，在Kafka集群中维护offset\n"),e("ul",[e("li",[t._v("保存在"),e("code",{pre:!0},[t._v("__consumer_offsets")]),t._v("主题中")]),t._v(" "),e("li",[t._v("可以修改消费者配置文件"),e("code",{pre:!0},[t._v("consumer.properties")]),t._v("里面的"),e("code",{pre:!0},[t._v("exclude.internal.topics=false")]),t._v("实现读取内置主题")]),t._v(" "),e("li",[t._v("0.11版本之前，需要"),e("code",{pre:!0},[t._v('--formatter "kafka.coordinator.GroupMetadataManager\\$OffsetsMessageFormatter"')]),t._v("作为"),e("code",{pre:!0},[t._v("kafka-console-consumer.sh")]),t._v("的参数，格式化数据")]),t._v(" "),e("li",[t._v("0.11版本之后（含0.11），需要"),e("code",{pre:!0},[t._v('--formatter "kafka.coordinator.group.GroupMetadataManager\\$OffsetsMessageFormatter')]),t._v("作为"),e("code",{pre:!0},[t._v("kafka-console-consumer.sh")]),t._v("的参数，格式化数据")]),t._v(" "),e("li",[t._v("命令："),e("code",{pre:!0},[t._v("kafka-console-consumer.sh --topic __consumer_offsets --zookeeper ip:port --formatter xxx --consumer.config config/consumer.properties --from-beginning")]),t._v(" "),e("ul",[e("li",[t._v("为什么要连接ZK而不是Kafka保存offset？如果连接Kafka直接读取offset，则消费者会在读取offset的时候同时写入offset导致数据循环。所以在查询Kafka里面的offset的时候要让消费者把自己的offset保存在ZK")])])])])]),t._v(" "),e("li",[t._v("版本小于0.9时，offset保存在ZK\n"),e("ul",[e("li",[t._v("ZK里面除了"),e("code",{pre:!0},[t._v("/zookeeper")]),t._v("之外的其他节点都是Kafka创建的，包括"),e("code",{pre:!0},[t._v("cluster")]),t._v(", "),e("code",{pre:!0},[t._v("controller")]),t._v(", "),e("code",{pre:!0},[t._v("brokers")]),t._v(", "),e("code",{pre:!0},[t._v("admin")]),t._v(", "),e("code",{pre:!0},[t._v("consumers")]),t._v("等\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("/controller")]),t._v("里面可以看到Kafka集群的controller信息。通常情况是第一个启动的broker。controller负责向ZK写数据，并和其他broker同步数据")]),t._v(" "),e("li",[t._v("可以在ZK的"),e("code",{pre:!0},[t._v("/brokers")]),t._v("中看到broker信息")]),t._v(" "),e("li",[t._v("在"),e("code",{pre:!0},[t._v("/consumers")]),t._v("中保存消费者组的信息，里面包括offset")])])])])]),t._v(" "),e("li",[t._v("默认情况下，每秒提交一次offset。ZK里面的数据延迟可能更高。如果没有流数据，消费者可能会使用长轮询，可能5秒提交一次offset")])])])]),t._v(" "),e("h4",{attrs:{id:"isrin-sync-replica-set同步副本集"}},[t._v("ISR(In-Sync Replica set)同步副本集")]),t._v(" "),e("ul",[e("li",[t._v("如果一个follower长时间没有向leader同步数据，则把follower踢出ISR。")]),t._v(" "),e("li",[t._v("这个时间长度由"),e("code",{pre:!0},[t._v("replica.lag.time.max.ms")]),t._v("参数设定。默认10秒")]),t._v(" "),e("li",[t._v("leader故障后，会从ISR中重新选举Leader")]),t._v(" "),e("li",[t._v("ISR和leader信息都保存在ZK")]),t._v(" "),e("li",[t._v("老版本(低于0.9)时，还会根据事件落后的条数来判断是否把follower提出ISR。后来发现每次写入的事件比较多，可能会导致follower频繁出入ISR，给ZK造成压力，所以废弃")])]),t._v(" "),e("h4",{attrs:{id:"数据一致性问题"}},[t._v("数据一致性问题")]),t._v(" "),e("ul",[e("li",[t._v("如果leader挂了，不同的follower保存的事件也不一致，如何解决")]),t._v(" "),e("li",[t._v("Log文件中有两个指针\n"),e("ul",[e("li",[t._v("LEO(Log End Offset)\n"),e("ul",[e("li",[t._v("当前副本保存的最后一个事件的offset")])])]),t._v(" "),e("li",[t._v("HW(High Watermark)\n"),e("ul",[e("li",[t._v("所有ISR中的副本的最小LEO")])])])])]),t._v(" "),e("li",[t._v("只有HW之前的数据对consumer可见。所以consumer只能消费到所有副本中都有的数据（实现读取一致性）")]),t._v(" "),e("li",[t._v("新的Leader被选举出来之后，所有follower需要删除自己log中HW之后的数据（超前数据），然后从Leader同步数据（实现存储一致性）")]),t._v(" "),e("li",[t._v("此机制不会解决丢数据/重复数据的问题")])]),t._v(" "),e("h4",{attrs:{id:"exactly-once"}},[t._v("Exactly Once")]),t._v(" "),e("ul",[e("li",[t._v("如果把服务器ACK设置为1，则保障不会丢数据，但是可能出现数据重复，即at least once")]),t._v(" "),e("li",[t._v("如果把服务器的ACK设置为0，则保证生产者的每条消息都只会发送一次，但是不保证数据不丢，即at most once")]),t._v(" "),e("li",[t._v("0.11版本之前，Kafka做不到exactly once。只能由消费者做全局去重，性能影响很大")]),t._v(" "),e("li",[t._v("0.11版本的Kafka引入了幂等性，用来实现exactly once\n"),e("ul",[e("li",[t._v("at least once + 幂等性 = exactly once")]),t._v(" "),e("li",[t._v("需要配置"),e("code",{pre:!0},[t._v("enable.idompotence")]),t._v("为"),e("code",{pre:!0},[t._v("true")])]),t._v(" "),e("li",[t._v("kafka会自己做去重，而不是由消费者去重")]),t._v(" "),e("li",[t._v("producer在初始化的时候会获得一个PID(producer id)，发往同一个partition的消息会附带sequence number")]),t._v(" "),e("li",[t._v("broker会把"),e("code",{pre:!0},[t._v("(PID, Partition, SeqNumber)")]),t._v("作为主键进行缓存，实现去重")]),t._v(" "),e("li",[t._v("重启producer的时候PID会变化，不同的partition也会导致主键不同，所以幂等性无法实现跨分区、跨会话的exactly once")])])])]),t._v(" "),e("h4",{attrs:{id:"为什么kafka读写高效"}},[t._v("为什么Kafka读写高效")]),t._v(" "),e("p",[t._v("首先Kafka是分布式，所以可扩展。")]),t._v(" "),e("p",[t._v("其次，Kafka即使是单机，也会比其他的消息队列的速度更快。原因如下")]),t._v(" "),e("ul",[e("li",[t._v("顺序读写\n"),e("ul",[e("li",[t._v("Kafka写数据到log文件是顺序读写，只需要append到文件末尾")]),t._v(" "),e("li",[t._v("官方数据，同样的磁盘，顺序读写速度可以达到600M/s，而随机读写只有100K/s")]),t._v(" "),e("li",[t._v("因为省去了大量的磁头寻址时间")])])]),t._v(" "),e("li",[t._v("零拷贝技术\n"),e("ul",[e("li",[t._v("一般的应用程序，实现网络传输文件，流程如下\n"),e("ul",[e("li",[t._v("用户空间应用程序调用内核空间文件接口，拿到数据（副本）\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("File -> Page Cache -> Application Cache")])])])]),t._v(" "),e("li",[t._v("用户空间应用程序调用内核空间网络接口，发送数据（副本）\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("Application Cache -> Socket Cache -> NIC")])])])])])]),t._v(" "),e("li",[t._v("而Kafka使用的零拷贝技术可以实现：\n"),e("ul",[e("li",[t._v("在内核空间，把文件接口拿到的数据直接发给内核空间的网络接口，不走用户空间\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("File -> Page Cache -> NIC")])])])])])])])])]),t._v(" "),e("h4",{attrs:{id:"zk的作用"}},[t._v("ZK的作用")]),t._v(" "),e("ul",[e("li",[t._v("Kafka集群中会有一个broker被选举为controller，负责管理集群broker的上下线、所有topic的分区副本的分配，以及leader的选举\n"),e("ul",[e("li",[t._v("在ZK可以查到当前controller的broker id")])])]),t._v(" "),e("li",[t._v("controller的管理工作都是依赖ZK的")]),t._v(" "),e("li",[t._v("Partition的Leader选举过程如下\n"),e("ul",[e("li",[t._v("controller监听ZK里面的"),e("code",{pre:!0},[t._v("/brokers/ids")])]),t._v(" "),e("li",[t._v("一旦出现变化，则根据ISR，协调选举出新的leader")]),t._v(" "),e("li",[t._v("controller更新ISR和leader信息")])])])]),t._v(" "),e("h4",{attrs:{id:"事务"}},[t._v("事务")]),t._v(" "),e("ul",[e("li",[t._v("0.11版本引入事务")]),t._v(" "),e("li",[t._v("可以保证在Exactly Once的基础上，生产/消费可以跨分区/会话，且具有原子性（要么全部成功，要么全部失败）\n"),e("ul",[e("li",[t._v("比如生产者可能希望向3个分区分别写入一个数据，且要么全部写入成功，要么全部写入失败")])])]),t._v(" "),e("li",[t._v("生产者事务\n"),e("ul",[e("li",[t._v("为了实现跨分区跨会话的事务，需要引入一个全局唯一的Transaction ID，并把生产者的PID（producer ID）和Transaction ID绑定\n"),e("ul",[e("li",[t._v("生产者重启时根据Transaction ID获取PID")])])]),t._v(" "),e("li",[t._v("Kafka引入了新的组件Transaction Coordinator用来管理事务\n"),e("ul",[e("li",[t._v("生产者通过和Transaction Coordinator交互获取Transaction ID对应的任务状态")]),t._v(" "),e("li",[t._v("Transaction Coordinator负责把事务写入Kafka内部的Topic，实现故障恢复")])])])])]),t._v(" "),e("li",[t._v("消费者事务\n"),e("ul",[e("li",[t._v("消费者事务相对较弱")]),t._v(" "),e("li",[t._v("无法保证commit信息被精确消费")]),t._v(" "),e("li",[t._v("因为消费者可以根据offset访问任意信息")])])])]),t._v(" "),e("h3",{attrs:{id:"命令"}},[t._v("命令")]),t._v(" "),e("ul",[e("li",[e("code",{pre:!0},[t._v("bin/kafka-server-start.sh -daemon config/server.properties")]),t._v(" & "),e("code",{pre:!0},[t._v("bin/kafka-server-stop.sh config/server.properties")]),t._v(" "),e("ul",[e("li",[t._v("后台启动服务端/关闭服务端")]),t._v(" "),e("li",[t._v("可以用"),e("code",{pre:!0},[t._v("jps")]),t._v("查看kafka进程")])])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("bin/kafka-console-consumer.sh")]),t._v(" & "),e("code",{pre:!0},[t._v("bin/kafka-console-producer.sh")]),t._v(" "),e("ul",[e("li",[t._v("控制台消费者/生产者")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("--topic xxx --broker-list ip:port")]),t._v("创建生产者\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("broker-list")]),t._v("可以写任意一个broker，因为broker之间会复制数据。但是推荐写上所有broker，避免某个节点故障")])])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("--topic xxx --zookeeper ip:port")]),t._v("创建基于ZK的消费者（老版本）\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("--from-beginning")]),t._v("从头读取")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("zookeeper")]),t._v("可以写任意一个节点的地址，但是推荐写上所有ZK节点，避免单点故障")])])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("--topic xxx --bootstrap-server ip:port")]),t._v("创建基于Kafka的消费者（新版本）")])])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("bin/kafka-consumer-perf-test.sh")]),t._v(" & "),e("code",{pre:!0},[t._v("bin/kafka-producer-perf-test.sh")]),t._v(" "),e("ul",[e("li",[t._v("压力测试工具")])])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("bin/kafka-topics.sh")]),t._v(" "),e("ul",[e("li",[t._v("Topic相关的命令")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("--zookeeper xxx")]),t._v("指定ZK。下面的命令都需要指定ZK地址")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("--list")]),t._v("查看所有topic")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("--create --replication-factor 3 --partitions 1 --topic topic-name")]),t._v("创建topic\n"),e("ul",[e("li",[t._v("副本数不能超过Broker数量")]),t._v(" "),e("li",[t._v("副本数包含了leader")])])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("--delete --topic xxx")]),t._v("删除topic")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("--describe --topic xxx")]),t._v("描述topic，比如分区数量，副本数量")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("--alter --topic xxx --partitions 6")]),t._v("修改分区数量")])])])]),t._v(" "),e("h3",{attrs:{id:"api"}},[t._v("API")]),t._v(" "),e("h4",{attrs:{id:"生产者api"}},[t._v("生产者API")]),t._v(" "),e("ul",[e("li",[t._v("消息发送流程\n"),e("ul",[e("li",[t._v("异步发送，批量写入")]),t._v(" "),e("li",[t._v("两个线程\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("main")]),t._v("和"),e("code",{pre:!0},[t._v("sender")])]),t._v(" "),e("li",[t._v("两个线程共享一个变量："),e("code",{pre:!0},[t._v("RecordAccumulator")])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("main")]),t._v("不断把消息发给"),e("code",{pre:!0},[t._v("RecordAccumulator")])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("sender")]),t._v("不断从"),e("code",{pre:!0},[t._v("RecordAccumulator")]),t._v("拉取数据发送到broker")])])]),t._v(" "),e("li",[t._v("流程\n"),e("ul",[e("li",[t._v("生产者调用send方法")]),t._v(" "),e("li",[t._v("数据交给拦截器(Interceptors)进行数据增强或过滤\n"),e("ul",[e("li",[t._v("拦截器可以自定义")])])]),t._v(" "),e("li",[t._v("数据交给序列化器(Serializer)进行序列化，变为字节数组\n"),e("ul",[e("li",[t._v("序列化器可以自定义")])])]),t._v(" "),e("li",[t._v("数据交给分区器(Partitioner)进行分区\n"),e("ul",[e("li",[t._v("分区器可以自定义")])])]),t._v(" "),e("li",[t._v("数据被交给RecordAccumulator（中的多个分区）")]),t._v(" "),e("li",[t._v("被sender线程拉取，并发送给broker")])])]),t._v(" "),e("li",[t._v("配置\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("batch.size")]),t._v("数据积累到此值之后，sender发送数据")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("linger.ms")]),t._v("数据超过此时间还未发送时，sender发送数据")])])])])]),t._v(" "),e("li",[t._v("API\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("send")]),t._v(" "),e("ul",[e("li",[t._v("支持传入回调函数，就可以得到事件的offset等信息，或者进行奕常处理")]),t._v(" "),e("li",[t._v("支持根据参数使用不同分区方式。详见上文")]),t._v(" "),e("li",[t._v("返回一个"),e("code",{pre:!0},[t._v("Future")]),t._v("。如果选择使用"),e("code",{pre:!0},[t._v("Future.get()")]),t._v("则可以实现同步发送而非异步\n"),e("ul",[e("li",[t._v("同步发送可以实现确保消息有序。到那时通常可能会用其他消息队列实现这个功能")])])])])])])])]),t._v(" "),e("pre",{staticClass:"language-java"},[e("code",{pre:!0,attrs:{class:"language-java"}},[e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Properties")]),t._v(" props "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Properties")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\nprops"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("put")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bootstrap.servers"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"xxx"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// use literal string as key")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v('// props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "xxx"); // use enum const as key')]),t._v("\nprops"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("put")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"acks"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"all"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\nprops"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("put")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"retries"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\nprops"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("put")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"batch.size"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("16384")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 16KB")]),t._v("\nprops"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("put")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"linger.ms"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\nprops"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("put")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"buffer.memory"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("33554432")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// RecordAccumulator buffer size, 32MB")]),t._v("\nprops"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("put")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"key.serializer"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"org.apache.kafka.common.serialization.StringSerializer"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\nprops"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("put")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"value.serializer"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"org.apache.kafka.common.serialization.StringSerializer"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("KafkaProducer")]),e("span",{pre:!0,attrs:{class:"token generics"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" producer "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("KafkaProducer")]),e("span",{pre:!0,attrs:{class:"token generics"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("properties"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\nproducer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("send")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("ProducerRecord")]),e("span",{pre:!0,attrs:{class:"token generics"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"topic-name"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"xxx"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\nproducer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("close")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// flush current buffer to sender")]),t._v("\n")])]),t._v(" "),e("blockquote",[e("p",[t._v("如果仅调用了send而没有close，可能会丢数据，因为数据在buffer中，没有被close给清理掉")])]),t._v(" "),e("h4",{attrs:{id:"消费者api"}},[t._v("消费者API")]),t._v(" "),e("ul",[e("li",[t._v("流程\n"),e("ul",[e("li",[t._v("批量读出")]),t._v(" "),e("li",[t._v("仅保证分区内数据有序。不保证分区之间数据有序")])])]),t._v(" "),e("li",[t._v("提交offset\n"),e("ul",[e("li",[t._v("支持自动提交，即刷新集群里面保存的消费者的offset")]),t._v(" "),e("li",[t._v("也可以手动提交，避免消费者故障导致的数据读取丢失。处理完数据之后再手动提交\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("commitSync")]),t._v(" "),e("ul",[e("li",[t._v("阻塞，自动重试")])])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("commitAsync")]),t._v(" "),e("ul",[e("li",[t._v("非阻塞，不重试，可能失败。支持回调函数")])])])])]),t._v(" "),e("li",[t._v("手动提交仍然无法解决数据重复的问题\n"),e("ul",[e("li",[t._v("因为存在分布式事务。比如读取kafka里面的数据并保存在MySQL，如果MySQL已经更新而offset没有更新，此时进程挂掉，就会数据重复")]),t._v(" "),e("li",[t._v("自定义存储offset，可能可以解决上述问题\n"),e("ul",[e("li",[t._v("比如把offset保存在MySQL。更新MySQL和更新offset就可以在同一个MySQL事务中执行。MySQL的更新和offset的更新具有原子性，可以避免数据重复")]),t._v(" "),e("li",[t._v("但是需要自行解决消费者rebalance的问题（即消费者数量变化时，如何更新所有消费者的offset）\n"),e("ul",[e("li",[t._v("可以使用"),e("code",{pre:!0},[t._v("ConsumerRebalanceListener")]),t._v("监听消费者变化")]),t._v(" "),e("li",[t._v("主要是实现两个方法\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("getOffset")]),t._v("根据分区获取offset")]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("commitOffset")]),t._v("提交当前消费者对应的所有分区的offset")])])])])])])])])])])])]),t._v(" "),e("pre",{staticClass:"language-java"},[e("code",{pre:!0,attrs:{class:"language-java"}},[e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Properties")]),t._v(" props "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Properties")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\nprops"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("put")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("ConsumerConfig")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("BOOTSTRAP_SERVERS_CONFIG"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"xxx"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\nprops"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("put")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("ConsumerConfig")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ENABLE_AUTO_COMMIT_CONFIG"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 开启自动提交（刷新offset）")]),t._v("\nprops"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("put")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("ConsumerConfig")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("AUTO_COMMIT_INTERVAL_MS_CONFIG"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 自动提交延迟")]),t._v("\nprops"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("put")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("ConsumerConfig")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("KEY_DESERIALIZER_CLASS_CONFIG"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"xxx"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 反序列化类")]),t._v("\nprops"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("put")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("ConsumerConfig")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("VALUE_DESERIALIZER_CLASS_CONFIG"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"xxx"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 反序列化类")]),t._v("\nprops"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("put")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("ConsumerConfig")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("GROUP_ID_CONFIG"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"xxx"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 消费者组")]),t._v("\nprops"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("put")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("ConsumerConfig")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("AUTO_OFFSET_RESET_CONFIG"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"earliest"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 是否重置offset")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v('// "earliest" 或 "latest"，默认latest')]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("KafkaConsumer")]),e("span",{pre:!0,attrs:{class:"token generics"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" consumer "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("KafkaConsumer")]),e("span",{pre:!0,attrs:{class:"token generics"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("props"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\nconsumer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("subscribe")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Arrays")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("asList")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"topic-name"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"topic-name2"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 可以订阅多个主题")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v('// 或者用 Collections.singletonList("topic-name") 如果只有一个主题')]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("ConsumerRecords")]),e("span",{pre:!0,attrs:{class:"token generics"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" records "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" consumer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("poll")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 接收数据。如果没有数据，长轮询100毫秒")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\nconsumer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("close")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])]),t._v(" "),e("h4",{attrs:{id:"自定义拦截器"}},[t._v("自定义拦截器")]),t._v(" "),e("ul",[e("li",[t._v("通常用来增强数据（比如添加时间戳）/过滤数据/统计数据")]),t._v(" "),e("li",[t._v("继承"),e("code",{pre:!0},[t._v("ProducerInterceptor")]),t._v(" "),e("ul",[e("li",[t._v("主要实现以下方法\n"),e("ul",[e("li",[e("code",{pre:!0},[t._v("configure(configs)")]),t._v(" "),e("ul",[e("li",[t._v("获取配置信息和初始化数据")])])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("onSend(ProducerRecord)")]),t._v(" "),e("ul",[e("li",[t._v("在消息被序列化、分区之前调用此方法")]),t._v(" "),e("li",[t._v("最好不要修改消息所属的topic和分区，否则会影响目标分区的计算")])])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("onAcknowledgement(RecordMetadata, Exception)")]),t._v(" "),e("ul",[e("li",[t._v("不要执行很重的逻辑，会影响生产者效率")])])]),t._v(" "),e("li",[e("code",{pre:!0},[t._v("close")]),t._v(" "),e("ul",[e("li",[t._v("清理资源")])])])])])])]),t._v(" "),e("li",[t._v("拦截器可能被运行在多个线程。需要自行确保线程安全")]),t._v(" "),e("li",[t._v("如果指定了多个拦截器，则会按顺序被调用")])]),t._v(" "),e("h4",{attrs:{id:"自定义分区器"}},[t._v("自定义分区器")]),t._v(" "),e("ul",[e("li",[t._v("序列化之后才进行分区，所以key是字节数组")]),t._v(" "),e("li",[t._v("可以根据key分区，也可以根据value")]),t._v(" "),e("li",[t._v("在producer config里面指定分区器："),e("code",{pre:!0},[t._v('props.put("partitioner.class", "xxx")')])])]),t._v(" "),e("pre",{staticClass:"language-java"},[e("code",{pre:!0,attrs:{class:"language-java"}},[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("MyPartitioner")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("implements")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Partitioner")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Override")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("partition")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),t._v(" topic"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Object")]),t._v(" key"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("byte")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" keyBytes"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Object")]),t._v(" value"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("byte")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" valueBytes"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Cluster")]),t._v(" cluster"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 以下是常用的函数")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("List")]),e("span",{pre:!0,attrs:{class:"token generics"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("PartitionInfo")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" partitions "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cluster"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("partitionsForTopic")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("topic"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 获取topic的分区信息。主要用来获取分区数量")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("List")]),e("span",{pre:!0,attrs:{class:"token generics"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("PartitionInfo")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" availablePartitions "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cluster"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("availablePartitionsForTopic")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("topic"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 获取可用的分区信息")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Integer")]),t._v(" count "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cluster"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("partitionCountForTopic")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("topic"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 获取分区数量")]),t._v("\n\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 返回分区ID")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n  "),e("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Override")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("close")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n  "),e("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Override")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("configure")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Map")]),e("span",{pre:!0,attrs:{class:"token generics"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("?")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" configs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])])}]};t.exports={attributes:{title:"Big Data(Part 7)",description:"Kafka"},vue:{render:e.render,staticRenderFns:e.staticRenderFns,component:{data:function(){return{templateRender:null}},render:function(t){return this.templateRender?this.templateRender():t("div","Rendering")},created:function(){this.templateRender=e.render,this.$options.staticRenderFns=e.staticRenderFns}}}}}}]);
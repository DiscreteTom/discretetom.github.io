(window.webpackJsonp=window.webpackJsonp||[]).push([[17,575,576,577,578,579,580,581,583,584,585,586,587,588,589,590,591,592,593],{1115:function(v,_,t){const n={render:function(){this.$createElement;return this._self._c,this._m(0)},staticRenderFns:[function(){var v=this,_=v.$createElement,n=v._self._c||_;return n("div",{staticClass:"frontmatter-markdown"},[n("h2",[v._v("概率语言模型")]),v._v(" "),n("h3",[v._v("基础")]),v._v(" "),n("p",[v._v("如何评价词串"),n("code",{pre:!0},[v._v("W=w1, w2, w3, ..., wn")]),v._v("的质量（即合法性）")]),v._v(" "),n("p",[n("strong",[v._v("计算W出现的概率"),n("code",{pre:!0},[v._v("p(W)")])]),v._v("，即"),n("code",{pre:!0},[v._v("p(w1, w2, ..., wn)")]),v._v("。根据概率链式法则：")]),v._v(" "),n("p",[n("code",{pre:!0},[v._v("p(W) = p(w1)p(w2|w1)...p(wn|w1, w2, ..., wn-1)")])]),v._v(" "),n("h3",[v._v("极大似然估计")]),v._v(" "),n("p",[v._v("Maximum Likelihood Estimation, MLE")]),v._v(" "),n("p",[n("img",{attrs:{src:t(764),alt:"2-1"}})]),v._v(" "),n("p",[v._v("如评估句子I love you，需要计算"),n("code",{pre:!0},[v._v("p(you|I,love)=C(I love you)/C(I love)")]),v._v('，即"I love you"的出现次数除以"I love"的出现次数')]),v._v(" "),n("h3",[v._v("缺陷与解决方案")]),v._v(" "),n("p",[v._v("当前词出现的概率依赖于前面的词。如果串中词量很大")]),v._v(" "),n("ul",[n("li",[v._v("可能导致某个子串出现次数为0，导致概率无法计算")]),v._v(" "),n("li",[v._v("随着串的增加，概率越来越小")])]),v._v(" "),n("p",[v._v("一个可能的解决方案：当前词的出现概率 "),n("strong",[v._v("仅依赖较短的历史词")])]),v._v(" "),n("h2",[v._v("n-gram语言模型")]),v._v(" "),n("h3",[v._v("基础")]),v._v(" "),n("p",[v._v("马尔科夫假设：位于某个特定状态的概率取决于（约等于）前n-1个状态。应用于语言模型："),n("strong",[v._v("假设每个词的出现只与前面的n-1个词有关，即n-gram模型（n元语法/n元文法）")])]),v._v(" "),n("ul",[n("li",[v._v("1-gram语言模型(unigram) - 0阶马尔科夫链\n"),n("ul",[n("li",[v._v("与前面的0个词有关")]),v._v(" "),n("li",[v._v("p(W)=p(w1)p(w2)…p(wn)")])])]),v._v(" "),n("li",[v._v("2-gram语言模型(bigrams) - 1阶马尔科夫链")]),v._v(" "),n("li",[v._v("3-gram语言模型(trigrams) - 2阶马尔科夫链")])]),v._v(" "),n("h3",[v._v("n-gram模型的参数n")]),v._v(" "),n("p",[v._v("关于n-gram中n的取值。显然n越大生成的句子越好，但是n过大会导致模型不可行，原因是"),n("strong",[v._v("计算量太大")]),v._v("以及"),n("strong",[v._v("概率为0的项太多")])]),v._v(" "),n("p",[n("strong",[v._v("实际应用常取n=3")])]),v._v(" "),n("p",[n("strong",[v._v("参数的估计")]),v._v("：给定一个训练数据集X（和n），从中计算参数")]),v._v(" "),n("h2",[v._v("数据稀疏及平滑")]),v._v(" "),n("p",[v._v("有些训练数据集X中的一些词高频出现，一些词低频出现，造成估计结果不可靠。也可能有词不出现导致概率不可计算")]),v._v(" "),n("p",[v._v("增加训练语料？增加的语料中高频词仍然占绝大部分")]),v._v(" "),n("p",[v._v("主要解决方案：")]),v._v(" "),n("ul",[n("li",[n("strong",[v._v("平滑Smoothing")]),v._v("：重新估计零概率及低值概率，给它们非零值。即给没有见过的事件分配未来发生的可能性")]),v._v(" "),n("li",[n("strong",[v._v("回退Back-off")]),v._v("：高阶n-grams概率难以计算时使用低阶n-grams来统计")])]),v._v(" "),n("h3",[v._v("Zipf定律（略")]),v._v(" "),n("p",[v._v("Zipf’s law：在自然语言的语料库里，一个单词的出现频率与它在频率表里的排名成反比。假设排名表中单词w1, w2, w4的出现频率为最多、第二多、第四多，那么")]),v._v(" "),n("ul",[n("li",[v._v("w1的频率约为w2的2倍")]),v._v(" "),n("li",[v._v("w2的频率约为w4的2倍")])]),v._v(" "),n("h3",[v._v("Laplace Smoothing(Add-one)")]),v._v(" "),n("p",[v._v("以 "),n("strong",[v._v("unigram")]),v._v(" 为例，令"),n("code",{pre:!0},[v._v("ci")]),v._v("为词"),n("code",{pre:!0},[v._v("wi")]),v._v("出现的次数，"),n("code",{pre:!0},[v._v("N")]),v._v("为训练数据集中的单词总量，"),n("code",{pre:!0},[v._v("|V|")]),v._v("为词表大小（即训练数据集中不同的单词的数量）。原始的p(wi)=ci/N。")]),v._v(" "),n("p",[v._v("对词表中的每一个单词的出现次数进行+1处理后，训练数据集中的单词总量修正为"),n("code",{pre:!0},[v._v("N + |V|")]),v._v("，则")]),v._v(" "),n("p",[n("img",{attrs:{src:t(765),alt:"2-2"}})]),v._v(" "),n("p",[v._v("或者，换一种思路去理解，p的分母仍然是N，分子则是ci的修正值：")]),v._v(" "),n("p",[n("img",{attrs:{src:t(766),alt:"2-3"}})]),v._v(" "),n("p",[v._v("其中ci的修正值（也称为 "),n("strong",[v._v("加1折扣计数discount")]),v._v("）为")]),v._v(" "),n("p",[n("img",{attrs:{src:t(767),alt:"2-4"}})]),v._v(" "),n("p",[v._v("同理，在 "),n("strong",[v._v("bigram")]),v._v(" 里面，一个原始的MLE值:")]),v._v(" "),n("p",[n("img",{attrs:{src:t(768),alt:"2-5"}})]),v._v(" "),n("p",[v._v("针对所有的两个单词组成的单词对c(wi, wj)，其出现频率+1，得到的修正概率：")]),v._v(" "),n("p",[n("img",{attrs:{src:t(769),alt:"2-6"}})]),v._v(" "),n("p",[v._v("其中V显然应为单词对的数量。也可以像unigram一样改变理解方式：")]),v._v(" "),n("p",[n("img",{attrs:{src:t(770),alt:"2-7"}})]),v._v(" "),n("p",[v._v("其中c的修正值：")]),v._v(" "),n("p",[n("img",{attrs:{src:t(771),alt:"2-8"}})]),v._v(" "),n("h3",[v._v("Lidstone smoothing(Add-Delta)")]),v._v(" "),n("p",[v._v("是add-one平滑的一种扩展，把add-one中的1改为delta或δ，取值范围"),n("code",{pre:!0},[v._v("0 <= delta <= 1")])]),v._v(" "),n("p",[v._v("显然delta=1时即为Laplace平滑")]),v._v(" "),n("p",[v._v("如何选取合适的delta?常用 "),n("strong",[v._v("held-out estimation(保守估计)")])]),v._v(" "),n("ol",[n("li",[v._v("从训练数据D中分离出一部分数据H(held-out data/validation data)")]),v._v(" "),n("li",[v._v("采用数据H训练具有不同delta值的语言模型")]),v._v(" "),n("li",[v._v("分别测试这些模型在H上的表现")]),v._v(" "),n("li",[v._v("选取最优模型的delta作为最优delta")])]),v._v(" "),n("h3",[v._v("Good-Turing Smoothing")]),v._v(" "),n("p",[n("strong",[v._v("以unigram为例")])]),v._v(" "),n("p",[v._v("根据仅出现一次的unigram的个数来确定未出现的unigram的概率。")]),v._v(" "),n("p",[v._v("现规定仅出现一次的unigram的个数N1:")]),v._v(" "),n("p",[n("img",{attrs:{src:t(772),alt:"2-9"}})]),v._v(" "),n("p",[v._v("出现c次的unigram的个数Nc:")]),v._v(" "),n("p",[n("img",{attrs:{src:t(773),alt:"2-10"}})]),v._v(" "),n("p",[v._v("规定出现c次的词出现次数的修正值（即折扣后的值）为")]),v._v(" "),n("p",[n("img",{attrs:{src:t(774),alt:"2-11"}})]),v._v(" "),n("p",[v._v("那么其出现概率应为修正值除N，即")]),v._v(" "),n("p",[n("img",{attrs:{src:t(775),alt:"2-12"}})]),v._v(" "),n("p",[v._v("出现0次的词的折扣后出现次数为")]),v._v(" "),n("p",[n("img",{attrs:{src:t(776),alt:"2-13"}})]),v._v(" "),n("p",[v._v("则出现0次的词的出现概率为")]),v._v(" "),n("p",[n("img",{attrs:{src:t(777),alt:"2-14"}})]),v._v(" "),n("p",[n("strong",[v._v("对于n-gram")])]),v._v(" "),n("p",[v._v("类似地，令nr表示出现r次的n-grams的个数。则出现r次的单词的出现次数修正为")]),v._v(" "),n("p",[n("img",{attrs:{src:t(778),alt:"2-15"}})]),v._v(" "),n("p",[v._v("Good-Turing的归一化特性证明：")]),v._v(" "),n("p",[n("img",{attrs:{src:t(779),alt:"2-17"}})]),v._v(" "),n("h3",[v._v("回退法Backoff")]),v._v(" "),n("p",[v._v("基本思路：长度为n的串出现次数为0时，使用"),n("strong",[v._v("长度为n-1的以同样单词结尾的串的修正后的出现次数")])]),v._v(" "),n("p",[n("img",{attrs:{src:t(780),alt:"2-18"}})]),v._v(" "),n("p",[v._v("其中α1和α2就是用于修正的因子")]),v._v(" "),n("h3",[v._v("插值法Interpolation（略")]),v._v(" "),n("p",[v._v("基本思路：把"),n("strong",[v._v("同样单词结尾的长度不大于原串长n的串")]),v._v("都纳入考虑范围，加权得到修正值")]),v._v(" "),n("p",[n("img",{attrs:{src:t(781),alt:"2-19"}})]),v._v(" "),n("h3",[v._v("KN平滑（略")]),v._v(" "),n("p",[v._v("思想：出现次数多的词更可能出现在新的上下文中")]),v._v(" "),n("p",[v._v("以词w出现在新的上下文中的概率来取代w的出现概率")])])}]};v.exports={attributes:{layout:"collection",title:"N-gram语言模型",collection:"NaturalLanguageProcessing"},vue:{render:n.render,staticRenderFns:n.staticRenderFns,component:{data:function(){return{templateRender:null}},render:function(v){return this.templateRender?this.templateRender():v("div","Rendering")},created:function(){this.templateRender=n.render,this.$options.staticRenderFns=n.staticRenderFns}}}}},764:function(v,_,t){v.exports=t.p+"img/2876048.png"},765:function(v,_,t){v.exports=t.p+"img/980c800.png"},766:function(v,_,t){v.exports=t.p+"img/333f152.png"},767:function(v,_,t){v.exports=t.p+"img/8d2607a.png"},768:function(v,_,t){v.exports=t.p+"img/2925720.png"},769:function(v,_,t){v.exports=t.p+"img/fa19ab6.png"},770:function(v,_,t){v.exports=t.p+"img/1bc6623.png"},771:function(v,_,t){v.exports=t.p+"img/af2514b.png"},772:function(v,_,t){v.exports=t.p+"img/88515b6.png"},773:function(v,_,t){v.exports=t.p+"img/4ca4a42.png"},774:function(v,_,t){v.exports=t.p+"img/24b1db6.png"},775:function(v,_,t){v.exports=t.p+"img/09af0b6.png"},776:function(v,_,t){v.exports=t.p+"img/8979b4a.png"},777:function(v,_,t){v.exports=t.p+"img/9a12094.png"},778:function(v,_,t){v.exports=t.p+"img/f9fc67e.png"},779:function(v,_,t){v.exports=t.p+"img/7744a54.png"},780:function(v,_,t){v.exports=t.p+"img/80b5b5b.png"},781:function(v,_,t){v.exports=t.p+"img/191ca02.png"}}]);
---
title: Big Data(Part 2)
description: Hadoop, HDFS, MapReduce, YARN
tags:
  - Hadoop
---

## 前言

本文是观看[此视频](https://www.bilibili.com/video/BV1cW411r7c5)时的笔记

## Hadoop概述

Hadoop是由Apache基金会开发的分布式系统基础架构，主要解决海量数据的存储与计算问题

由于很多大数据框架都基于Hadoop，所以【Hadoop】这个名词有时候也代指Hadoop生态圈

基于Google在大数据方面的三篇论文，Hadoop有如下三个核心组件

- HDFS
  - 参考Google GFS的论文
- MR
  - 参考Google Map-Reduce的论文
- HBase
  - 参考Google BigTable的论文

三大发行版本：

- Apache
  - 原生版本，适合入门学习
- CDH
  - Cloudera的Hadoop发行版
  - 商业支持
  - 兼容性、安全性、稳定性更强
  - 在大型互联网企业中用的较多
- Hortonworks发行版
  - Hortonworks的主打产品是HDP(Hortonworks Data Platform)
  - 文档较好

Hadoop的优势：

- 高可靠
  - 底层维护多个数据副本（默认3个）
- 高扩展
  - 任务分配给数以千计的节点，轻松扩展
- 高效
  - 基于MapReduce并行工作
- 高容错
  - 自动把失败的任务重新分配

## 核心组件

### 版本概述

- 1.x
  - Common
    - 辅助工具集
  - HDFS
    - 数据存储
  - MapReduce
    - 计算+资源调度
- 2.x（解耦了计算和资源调度）
  - Common
    - 辅助工具集
  - HDFS
    - 数据存储
  - MapReduce
    - 计算
  - Yarn
    - 资源调度

### HDFS

> 类似于MFS那样的分布式存储/对象存储

> Hadoop Distributed File System

#### HDFS组件

- NameNode(nn)，就是HDFS集群的master node
  - 存储文件的元数据
    - 文件名
    - 目录结构
    - 生成时间
    - 权限
    - 副本数
    - 文件数据块索引
  - 配置副本策略
  - 处理客户端读写请求（类似于MFS，仅处理文件元数据的请求）
  - 数据保存在内存，并持久化到磁盘(FsImage)
    - 所有更新操作都先以追加的形式写入到Edits文件（相当于日志），而不是修改FsImage，从而保证效率（【追加】操作效率较高）
    - 为了避免Edits文件过大，会定期合并Edits和FsImage。如果有2NN，则由2NN执行此操作以减轻NN压力
  - **NN不持久化保存数据块的位置**，而是DN进行维护，并周期性向NN汇报，NN把这些信息保存在内存
  - 支持【多目录】，把文件保存在多个本地目录中，每个目录的内容**相同**。增加了可靠性
- DataNode(dn)
  - 存储文件的数据和部分元数据
    - 数据长度
    - 校验和（CRC校验）
    - 时间戳
  - DN启动后向NN注册，告诉NN自己有哪些文件
  - 启动后，默认每个小时向NN汇报所有信息
  - 心跳
    - 每隔3秒，NN向DN发送请求进行健康检查
    - 如果超过10分钟+30秒，NN没有收到DN的心跳，则判定节点不可用
    - 超时时长计算公式：`2 * dfs.namenode.heartbeat.recheck-interval + 10 * dfs.heartbeat.interval`
      - `dfs.namenode.heartbeat.recheck-interval`默认为5分钟
      - `dfs.heartbeat.interval`默认为3秒
  - 支持【多目录】，每个目录存储的数据**不同**，各个目录的数据加起来是节点的所有数据。可以用来把不同数据放在不同磁盘
- Secondary NameNode(2nn)
  - 监控HDFS状态的辅助程序，周期性获取HDFS元数据快照
  - 不是热备，NN挂掉的时候，2NN并不能代替NN提供服务
  - 分担NN的工作量，比如定期合并FsImage和Edits，然后推送给NN
  - 紧急情况下可以用2NN恢复NN的数据，但是会丢一部分数据
- Client，客户端，是java包
  - 文件上传的时候会把文件切块然后上传，默认每块128MB
  - 和NameNode交互元数据，和DataNode交互数据
  - 提供一些管理HDFS的命令，比如初始化NameNode，也可以增删改查

#### 其他细节

- HDFS是一个文件系统
- 通过目录树来定位文件
- 分布式
- 适合一次写入，多次读出，不支持修改
- 常用来做离线数据分析，无法做实时、低延迟数据分析
- 优点
  - 多副本（默认3），高容错，自动恢复副本数量
    - 如果副本数量为3，则第一个副本在本地机架的某一个节点，第二个副本和第一个副本位于同一个机架的不同节点，第三个副本位于不同机架的节点上，从而同时保证冗余和速度
  - PB级数据存储，百万级文件数量
- 缺点
  - 不适合实时数据访问
  - 无法高效存储大量小文件
    - 因为小文件会浪费NameNode的内存
    - 小文件的寻址时间比读取时间还长
  - 不支持文件随机访问/修改，仅支持末尾追加
  - 不支持并发写入，一个文件只能有一个写进程
- HDFS在同一个物理机上仅保存一个备份，即使replication数量大于物理机数量
- HDFS NameNode默认有一个WEB UI，可以用来进行数据管理
  - HTTP端口：50070
  - HTTPS端口：50470
- NameNode的IPC端口：8020/9000
- 数据块的大小默认为128MB，文件大于128MB时会拆分，文件小于128MB时并不意味着这个数据块会占128MB的空间
  - 如何确定数据块大小：建议寻址时间是传输时间的1%。比如寻址时间是10ms，硬盘速度100MB/s，那么数据块大小建议为100MB
- 每个数据块都会有一个元数据，用来校验
- 支持流IO上传/下载，支持定位读取

#### 数据流

- 写数据
  - 客户端向NN发上传请求，NN返回允许
  - 客户端请求上传第一个数据块，NN根据DN的负载、DN和客户端的距离、数据副本数量等因素，返回多个DN地址
    - 距离的计算
      - 同一个节点上的进程，距离为0
      - 节点距离为两个节点到达共同祖先的距离之和，比如，同一个机架上的两个节点，共同祖先是机架，则距离为2
  - 假设副本数量为3。客户端向DN1发请求，DN1向DN2发请求，DN2向DN3发请求，然后逆向响应，最后响应到客户端，建立通道
  - 客户端流式上传数据，DN1/2/3之间按顺序流式复制
  - 客户端通知NN，传输完毕
- 读数据
  - 客户端向NN发读取请求，NN返回文件的元数据，包括数据在哪个DN上
  - 客户端从最近的DN节点**依次**请求数据的每个块（不是并行）
- 更新NN元数据
  - 客户端发起修改请求
  - NN修改Edits文件，然后修改元数据
  - 如果定时时间到了（默认1小时），或者Edits中的数据满了（2NN会不断请求NN以检查Edits是否满了，默认每分钟一次，默认100w条记录算满），2NN会请求执行CheckPoint，即帮助NN合并Edits & FsImage
  - NN滚动Edits文件（比如把文件后缀从001变为002）
  - 2NN把需要的FsImage & Edits文件拷贝到本机，然后加载到内存进行合并
  - 把合并结束的FsImage拷贝回NN

#### NN故障恢复

有如下几个方案：

- 把2NN的数据拷贝到NN
- 从2NN导入checkpoint

#### 安全模式

- NN启动时，先把FsImage加载到内存，然后根据Edits文件更新内存，然后创建新的FsImage & Edits，然后开始监听请求。这段时间内NN无法接收请求，处于安全模式，只读，不可写
  - 刚刚格式化的NN由于没有保存任何数据块，所以不会进入安全模式
- DN启动时会向NN发送自身的块列表信息，从而报告数据块保存的位置。这个期间处于安全模式
- 如果满足【最小副本条件】，则NN会在30秒内退出安全模式
  - 最小副本条件：整个文件系统中99.9%的块满足【最小副本级别】（默认为1）
- 可以手动进入安全模式，从而使HDFS只读

#### 访问控制

可以使用白名单/黑名单来控制哪些DN可以注册到NN，防止外界恶意注册到自己的内部集群，也可以用来退役某些节点

- 白名单
  - 配置NN的`/opt/module/hadoop-x.x.x/etc/hadoop/dfs.hosts`文件（文件名不一定是`dfs.hosts`，官方推荐`dfs.hosts`）
    - 每一行是一个主机名，表示允许访问的主机
  - 配置`hdfs-site.xml`，并同步到所有节点
    - `<name>dfs.hosts</name> <value>/opt/module/hadoop-x.x.x/etc/hadoop/dfs.hosts</value>`
  - 刷新NN：`hdfs dfsadmin -refreshNodes`
  - 更新ResourceManager节点：`yarn rmadmin -refreshNodes`
  - 数据重平衡：`start-balancer.sh`
- 黑名单
  - 配置NN的`dfs.hosts.exclude`文件
  - 更新`hdfs-site.xml`，并同步到所有节点
    - `<name>dfs.hosts.exclude</name> <value>/path/to/dfs.hosts.exclude</value>`
  - 刷新NN：`hdfs dfsadmin -refreshNodes`
  - 更新ResourceManager节点：`yarn rmadmin -refreshNodes`
  - 数据重平衡：`start-balancer.sh`

#### HDFS 2.x新特性

1. 跨集群数据拷贝：`hadoop distcp <src> <dst>`
2. 小文件存档：HAR类型文件，减少NN内存占用，且直接使用har协议就可以查看内容
   1. 存档：`hadoop archive -archiveName xxx.har -p <input-path> <output-path>`
   2. 查看存档：`hadoop fs -lsr har:///xxx.har`
   3. 解压：`hadoop fs -cp har:///xxx.har/* <path>`
3. 回收站，防止误删除。默认关闭
   1. `fs.trash.interval`默认值为0表示禁用，其他值表示文件存活时间（分钟）
   2. `fs.trash.checkpoint.interval`表示检查回收站的间隔。如果为0则此值和`fs.trash.interval`相等
   3. `fs.trash.checkpoint.interval`应不大于`fs.trash.interval`
   4. 默认回收站用户是`dr.who`
   5. 回收站路径：`/user/xxx/.Trash`，会按照日期把文件进行归类
   6. 使用命令行`rm`删除文件会自动进入回收站。编程调用API时，需要使用`moveToTrash`才会放入回收站
   7. 清空回收站：`hadoop fs -expunge`。建议看下文档
4. 快照管理
   1. 备份目录。增量快照，并不会复制所有文件
   2. `hdfs dfsadmin -allowSnapshot <path>`为指定目录启用快照（默认所有目录禁用）（启用后才可以创建快照）
   3. `hdfs dfsadmin -disallowSnapshot <path>`为指定目录禁用快照
   4. `hdfs dfs -createSnapshot <path>`对目录创建快照
   5. `hdfs dfs -createSnapshot <path> <name>`对目录创建带名称的快照
   6. `hdfs dfs -renameSnapshot <path> <old-name> <new-name>`重命名快照
   7. `hdfs lsSnapshottableDir`列出当前用户所有可以被创建快照的目录
   8. `hdfs snapshotDiff <path> <from> <to>`比较两个快照目录的不同。**重要**。快照就是用来进行文件比较的
   9. `hdfs dfs -deleteSnapshot <path> <snapshot-name>`删除快照

### MapReduce

#### MR概述

组件：

- Map
  - 并行处理数据
  - 读数据，默认按行进行处理，得到k-v对（行数-字符串）
  - 对每一对k-v数据执行map方法（MapTask进程），输出格式也是k-v，我们假设输出为ko-vo
- Reduce
  - reduce的输入数据类型对应map的输出数据类型
  - 对Map的结果进行汇总，得到`ko -> [vo1, vo2, ...]`
  - reducer对每一组相同的k-v调用一次reduce方法
- Driver
  - 配置MR任务信息

其他细节：

- 是一个分布式计算编程框架
- 优点
  - 易于编程，提供众多接口，程序是串行逻辑，MR负责并行执行
  - 易于扩展，增减节点
  - 高容错，节点down的时候自动把任务重新分配到新的节点
  - 适合PB级数据离线处理
- 缺点
  - 不擅长实时计算，返回结果速度很慢
  - 不擅长流式计算，只能输入静态数据
  - 不擅长DAG（有向无环图）计算，即，前一个运算输出作为后一个运算的输入。因为每个MR作业的输出结果会写入磁盘造成大量IO，导致性能低下

#### MR原理

##### 切片与MR并行度

- 数据块
  - HDFS从物理上把数据分成的块
- 数据切片
  - 逻辑分片，并不会根据切片把数据存储在磁盘上
  - 每个MapTask处理一个切片，可能需要跨数据块/跨DN读取数据分片
  - 每个文件都会单独进行切片（如果单个文件大小大于数据块大小）
  - 切片完毕后，把切片信息保存在一个切片规划文件中，并提交给YARN，由YARN的MrAppMaster开启对应的MapTask
  - 切片机制
    - FileInputFormat
      - 根据文件大小进行切片，不考虑文件内容
      - 默认切片大小等于数据块的大小，避免传输时对数据块进行拆分或跨数据块读取
        - 本地运行时切片默认32MB
        - 集群模式下（由YARN调度），1.x默认64MB，2.x默认128MB
        - 如果最后一个切片的长度不大于数据块大小的1.1倍，则不切片（尽量避免过小的切片）
          - 比如本地运行模式数据块大小为32MB
            - 一个33MB的文件会被保存在两个数据块，但是只会被标记为一个切片，因为`33`小于`32*1.1`
            - 一个65MB的文件会被保存在三个数据块，但是只会被切割为两个切片，一个32MB，一个33MB
            - 一个70MB的文件会被保存在三个数据块，也会被切割为三个切片，分别是32/32/6MB
    - TextInputFormat（默认）
      - 按行读取
      - key是该行在整个文件中的字节偏移量，LongWritable
      - value是这一行的内容，不包括回车换行，Text
    - KeyValueTextInputFormat
      - 按行读取，根据某个分隔符把行的内容处理为k-v对，默认分隔符为制表符
        - `conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, "\t");`
      - key和value都是Text类型
    - NLineInputFormat
      - 指定每个切片由多少行组成
      - k-v和TextInputFormat一样
    - CombineTextInputFormat
      - 如果小文件很多，使用FileInputFormat就会导致过多的切片
      - 使用CombineTextInputFormat可以合并小文件形成切片，从而解决此问题
    - 自定义InputFormat
      - 继承`FileInputFormat`
      - 改写`RecordReader`，
      - 可以用来实现小文件的合并，比如有很多小文件，文件名是key，文件内容是value，可以写一个MR程序把他们聚合成新的文件，输出为SequenceFile，以后用SequenceFileInputFormat读取
    - 类似地，还有OutputFormat，比如TextOutputFormat。此处略
- MR并行度
  - 决定了任务并发能力
  - 由于启动MapTask也需要消耗时间，所以并不是并行度越高越好
  - 切片数量决定了Map任务并行度。Reduce任务并行度需要我们手动设置（通常等于分区数量）

##### Shuffle

数据处理过程称之为Shuffle。包括分区、排序、合并、溢写、压缩

- 分区
  - HashPartitioner（默认分区策略）
    - 根据k的hash值对ReduceTask数量取模得到
      - 多个`k->[v]`会保存在同一个分区
    - 用户没法控制哪个key存储到哪个分区
  - 支持自定义Partitioner
    - 自定义类集成Partitioner即可
    - k-v对应map输出的k-v
    - 在Job驱动中设置自定义Partitioner: `job.setPartitionerClass(XXX.class)`
    - 根据自定义Partitioner的逻辑，设置相应的ReduceTask数量（默认值为1）
      - 如果ReduceTask数量为0，则没有Reduce阶段，输出文件个数和Map个数一致
      - 如果ReduceTask数量为1，它负责处理所有的分区
      - 如果ReduceTask数量大于分区数量，则多出来的ReduceTask不会处理分区，输出空文件
      - 如果ReduceTask数量大于1，小于分区数量，报错
      - 分区号从0开始
- 排序
  - map阶段和reduce阶段都有排序
    - map阶段，环形缓冲区达到80%，落盘之前会排序。快排实现字典序
    - 一个map任务结束后，使用归并排序
    - reduce拉数据之后，对不同map任务 数据进行归并排序
  - 排序需求的分类
    - 部分排序
      - 根据k对数据排序，保证输出的每个文件（分区）有序
    - 全排序
      - 把ReduceTask数量设置为1，最后输出的结果只有一个文件。处理大型文件时效率低，无法并行
    - 辅助排序(Grouping Comparator分组排序)
      - 在Reduce端对k进行分组，使相同的k进入同一个reduce方法
    - X次排序
      - 在自定义排序过程中，如果compareTo中的判断条件有X个，就是X次排序（希尔排序）
  - 重写k对象的WritableComparable接口即可实现排序
- 合并
  - 是一种优化手段。原理是局部汇总以减少网络IO
  - Combiner是Mapper/Reducer之外的组件（插件）
  - Combiner组件的父类的Reducer
  - Combiner和Reducer的区别是运行的位置
    - Reducer接收全局所有Mapper的输出结果后运行
    - Combiner是在每一个MapTask所在的节点运行

```java
public class HashPartitioner<K, V> extends Partitioner<K, V> {
  public int getPartition(K key, V value, int numReduceTasks) {
    return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;
  }
}
```

#### MR进程

MR运行时有三种进程

- MrAppMaster
  - 程序调度、状态协调，是一个job的主控
- MapTask
  - 整个map阶段的数据处理
- ReduceTask
  - 整个reduce阶段的数据处理

#### MR数据类型与序列化

序列化：把内存中的对象转换为字节序列，以便持久化存储与通信

Java自身的序列化是一个重量级的序列化框架，一个对象序列化之后会附加很多额外的信息（校验、header、继承体系等等）

Hadoop使用自己的数据类型进行轻量序列化

- 紧凑高效
- 读写开销小
- 支持多语言交互
- 可扩展，可以随着通信协议的升级而升级

| Java | Hadoop Writable |
| --- | --- |
| boolean | BooleanWritable |
| byte | ByteWritable |
| int | IntWritable |
| float | FloatWritable |
| long | LongWritable |
| double | DoubleWritable |
| map | MapWritable |
| array | ArrayWritable |
| String | Text |

除了上述已有的数据类型，也可以自己开发可序列化对象

```java
public class Test implements Writable {
  // 成员变量
  private long v1;
  private long v2;
  private long v3;

  // 默认构造函数，其他类通过反射创建此类的对象时会使用
  public Test(){
    super();
  }

  // 序列化
  @Override
  public void write(DataOutput out) throws IOException {
    out.writeLong(v1);
    out.writeLong(v2);
    out.writeLong(v3);
  }

  // 反序列化
  @Override
  public void readFields(DataInput in) throws IOException {
    // 顺序和序列化时的顺序必须相同
    v1 = in.readLong();
    v2 = in.readLong();
    v3 = in.readLong();
  }

  // 如果需要写到文件，需要实现toString方法
  @Override
  public String toString(){
    return "xxx";
  }
}
```


#### 压缩

##### 压缩概述

- 用来减少HDFS的IO，提升网络带宽&磁盘空间利用率。
- 压缩基本原则：IO密集型适合压缩，计算密集型不适合压缩

| 压缩格式 | hadoop自带 | 算法 | 扩展名 | 可切分 | 是否需要修改程序 |
| --- | --- | --- | --- | --- | --- |
| DEFLATE | Y | DEFLATE | .deflate | N | N |
| Gzip | Y | DEFLATE | .gz | N | N |
| bzip2 | Y | bzip2 | .bz2 | Y | N |
| LZO | N | LZO | .lzo | Y | Y |
| Snappy | N | Snappy | .snappy | N | N |

##### 如何选择压缩算法

- Gzip
  - 优点：压缩率比较高，速度比较快，Hadoop自带，不需要改代码，大多数Linux自带Gzip，方便
  - 缺点：不支持切分
  - 场景：文件压缩后在130MB以内（一个块大小），可以考虑用Gzip。比如一天或者一个小时的日志压缩成一个Gzip文件
- Bzip2
  - 优点：支持切片，压缩率很高，Hadoop自带
  - 缺点：压缩/解压慢
  - 场景：对速度要求不高，但是对压缩率要求高。或者长期存档
- Lzo
  - 优点：速度快，支持切分，是Hadoop中最流行的压缩格式。可以在Linux系统安装lzop命令
  - 缺点：压缩率低，Hadoop不自带，需要改代码
  - 场景：很大的文本文件，压缩之后仍然大于200MB时可以考虑。单个文件越大，Lzo优势越明显
- Snappy
  - 优点：高速
  - 缺点：不支持切分，压缩率低，Hadoop不自带
  - 场景：Map输出文件较大，在Map和Reduce之间，或者级联作业之间

##### 何时压缩

- 输入数据是压缩格式
  - Hadoop自动检查文件扩展名，然后进行压缩/解压
- Map之后，Reduce之前
  - 如果Map输出的数据量大，需要考虑使用压缩。比如LZO/Snappy
- Reduce输出时压缩
  - 通常用于级联作业

#### Join

类似于SQL里面的join

没有这个功能，但是通过巧妙的业务逻辑自己实现

可以在Reduce阶段进行join，也可以在Map阶段进行join

#### 工作流

##### 宏观工作流

1. 客户端在submit之前，根据job的配置，生成任务规划（切片信息）
2. 客户端通过submit提交job，包括切片信息、jar包等配置
3. YARN调用ResourceManager创建MrAppMaster，根据切片信息创建MapTask
4. MapTask读取数据并处理map任务
5. `context.write`会向一个环形缓冲区写数据（默认大小100MB）
   1. 半个环用来写元数据（比如数据的起始位置、长度、分区等），半个环用来写数据
   2. 写到80%容量的时候，对每个分区的数据根据k使用快排进行排序（字典序），然后把缓冲区数据落盘然后反向继续写。比如分区1有两个k，分别是1和2，分区2有两个k，分别是3和4，那么每次输出应该类似于`p1(1->[v], 2->[v]), p2(3->[v], 4->[v])`
   3. 归并排序单个task的所有输出，得到多个数据分区，类似于`p1(1->[v], 2->[v]), p2(3->[v], 4->[v])`
6. MrAppMaster等待所有map任务执行完毕后，根据所有map任务的不同分区数量启动ReduceTask
7. ReduceTask把MapTask的结果下载到本地，使用归并排序合并不同task相同分区的结果，然后交给Reducer处理


以下两个函数是`Mapper`类和`Reducer`类的`run`方法

```java
// Mapper workflow
public void run(Context context) throws IOException, InterruptedException {
  setup(context);
  try {
    while (context.netKeyValue()){
      map(context.getCurrentKey(), context.getCurrentValue(), context);
    }
  } finally {
    cleanup(context);
  }
}

// Reducer workflow
public void run(Context context) throws IOException, InterruptedException {
  setup(context);
  try {
    while (context.nextKey()) {
      reduce(context.getCurrentKey(), context.getValues(), context);
      Iterator<VALUEIN> iter = context.getValues().iterator();
      if (iter instanceof ReduceContext.ValueIterator) {
        ((ReduceContext.ValueIterator<VALUEIN>)iter).resetBackupStore();
      }
    }
  } finally {
    cleanup(context);
  }
}
```

##### 详细MapTask工作流

- Read阶段
  - 根据InputFormat读取数据
- Map阶段
  - 根据Mapper计算数据
- Collet阶段
  - `context.write`写到`outputCollector`，也就是环形缓冲区
- 溢写阶段
  - 环形缓冲区需要落盘的时候溢写阶段
- Combine阶段
  - 溢写完毕，把已经落盘的文件进行合并

##### 详细ReduceTask工作流

- Copy阶段
  - 把MapTask的结果文件拷贝到Reducer节点
- Merge & Sort阶段
  - 合并MapTask的结果
  - 分组，`GroupingComparator`
- Reduce阶段
  - 根据Reducer实现业务逻辑

#### 示例项目

输入文件有若干行，每一行有若干单词，单词之间由空格进行分割，输出以26个字母开头的单词的总字母数

示例输入文件：

```bash
once upon a time there was a bug in the browser
test suite that caused the browser to crash when the user clicks on the button to start
```

示例输出文件：

```bash
a 2 # len("a") + len("a")
b 23 # len("bug") + len("browser") + len("browser") + len("button")
...
```

```java
/*
Driver, 用来配置任务
*/
public class TestDriver {
  public static void main(String[] args) throws IOException {
    // 初始化配置 & job
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);
    
    // 基于反射获取Driver的jar包位置
    job.setJarByClass(TestDriver.class);
    
    // 基于反射获取Mapper/Reducer的jar包位置
    job.setMapperClass(TestMapper.class);
    job.setReducerClass(TestReducer.class);

    // 设置Mapper/Reducer输出格式
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(IntWritable.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);

    // 设置输入输出路径
    FileInputFormat.setInputPaths(job, new Path("/path/to/input")); // 输入通常是args[0]
    FileOutputFormat.setOutputPaths(job, new Path("/path/to/output")); // 输出通常是args[1]

    // 提交job
    boolean result = job.waitForCompletion(true); // 冗余输出
    // System.exit(result ? 0 : 1);
  }
}


/*
Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT>
*/
public class TestMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
  Text keyOut = new Text();
  IntWritable valueOut = new IntWritable();

  // key是偏移量，value是一行文本
  @Override
  protected void map(LongWritable key, Text value, Context context)
    throws IOException, InterruptedException {
    String[] words = value.toString().split(" "); // 根据空格拆分单词
    for (String word : words){
      keyOut.set(word[0]); // 以单词首字母作为输出的key
      valueOut.set(word.length()); // 以单词长度作为输出的value
      context.write(keyOut, valueOut); // output (首字母, 长度)
    }
  }
}

/*
Reducer<KEYIN, VALUEIN, KEYOUT, VALUEOUT>
*/
public class TestReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
  IntWritable valueOut = new IntWritable();

  // key是单词首字母，values是相同首字母的单词的长度
  @Override
  protected void reduce(Text key, Iterable<IntWritable> values, Context context)
    throws IOException, InterruptedException {
    int sum = 0;
    for (IntWritable value : values) {
      sum += value.get();
    }
    valueOut.set(sum);
    context.write(key, valueOut); // output (首字母, 总长度)
  }
}
```

#### 管理

- web ui
  - 在8088端口有web ui界面
  - 在19888端口有job history的web ui界面
- 计数器
  - Hadoop为每个job内置若干计数器
    - 已处理字节数、记录数、输入数据量、输出数据量
  - 可以自定义计数器，用来debug
    - `context.getCounter(XXX).increment(1);`
    - 还可以分组管理计数器

#### ETL

执行核心MR任务之前通常需要进行数据清洗。通常只需要map不需要reduce

### Yarn

#### 组成部分

- ResourceManager(RM)
  - 是整个集群的主控
  - 处理客户端请求
  - 监控NM
  - 启动或监控AM
  - 资源的分配和调度
- NodeManager(NM)
  - 管理单个节点上的资源（相当于一个agent）
  - 汇报状态给RM
  - 处理RM的命令
  - 处理AM的命令
- ApplicationMaster(AM)
  - 负责数据的切分
  - 为应用程序申请资源
  - 任务的监控和容错
- Container
  - 是Yarn中资源的抽象
  - 封装了某个节点上的多维度资源
    - 内存
    - CPU
    - 磁盘
    - 网络

#### 工作流

1. 客户端提交任务到ResourceManager。即使用YarnRunner之后执行`job.waitForCompletion()`
2. ResourceManager返回Application资源提交路径（HDFS路径，`hdfs://xxx/.staging`）和application_id
3. 客户端把数据提交到HDFS。包括切片信息`Job.split`，配置文件`Job.xml`和Hadoop程序的jar包`xxx.jar`。这些文件都是在客户端`job.submit()`之后生成的
4. 资源提交完毕，客户端向ResourceManager请求运行mrAppMaster
5. ResourceManager会把请求初始化为一个Task，进入一个调度器（详见下文）
6. 空闲节点的NodeManager从ResourceManager领取任务
7. NodeManager在节点上创建容器，分配CPU和内存，启动MRAppMaster
8. Container下载资源到本地
9. Container向ResourceManager申请运行MapTask Container
10. 其他空闲节点的NodeManager从ResourceManager领取MapTask任务，创建容器，分配CPU和内存
11. MRAppMaster节点发送启动脚本到YarnChild执行MapTask
12. MapTask执行完毕后，MRAppMaster根据分片数量，向ResourceManager申请运行ReduceTask
13. 其他空闲节点的NodeManager从ResourceManager领取ReduceTask，并从MapTask节点拷贝Map结果数据
14. 整个MR工作结束后，MRAppMaster会通知ResourceManager注销任务

#### 调度器

- 目前调度器有三种：
  - FIFO队列
    - 一旦有节点空闲，向ResourceManager领取任务，ResourceManager会按照FIFO顺序把第一个Job里面的一个MapTask或ReduceTask分配给节点
  - Capacity Scheduler容量调度器（Hadoop 2.7.2默认）
    - 多个队列。每个队列有一个资源比例。比如3个队列，资源比例是20%/30%/50%
    - 为了防止同一个用户独占所有资源，调度器会对同一个用户提交的作业所占资源量进行限定
    - 计算每个队列中正在执行的Job数量和队列资源的比值。比值最小的队列最闲
    - 结合作业优先级、提交时间、用户资源量限制、内存限制等因素，对队列内的任务进行排序
    - 多个队列并行处理，互不影响。单个队列同时只能处理一个Job
  - Fair Scheduler公平调度器
    - 多个队列，按比例分配资源
    - 单个队列里面的Job可以同时执行
    - 根据优先级给Job分配资源。雨露均沾，不存在没有资源的Job。每个Job需要的资源和实际获得的资源存在缺额
    - 同一个队列中，Job的资源缺额越大，越先获得资源
    - 并发能力极强。如果机器的性能很高，可以使用这种调度器

#### 推测任务

如果某个任务的执行时间明显高于平均值，一直不结束，如何判断任务是在执行，还是因为硬件/软件原因导致执行失败呢？（停机问题）

可以配置推测任务，即执行一个相同的任务。取先执行完的任务的结果作为结果

前提条件：

- 每个Task只能有一个备份任务/推测任务
- 当前Job已完成的Task数量不小于5%
- 可以在配置文件里面配置参数来启动
  - 在`mapred-site.xml`中
  - `<name>mapreduce.reduce.speculative</name>`
  - 默认启动

以下情况不要启用推测任务：

- 非幂等的任务，比如写数据库
- 任务存在严重负载倾斜，导致某个任务的负载特别高。启动推测任务可能进一步增加原任务的执行时间

```python
# 计算
estimatedRunTime = (currentTimestamp - taskStartTime) / progress # 推测执行时长
estimatedEndTime = estimatedRunTime + taskStartTime # 推测当前任务完成时间
backupEstimateEndTime = currentTimestamp + averageRunTime # 备份任务推测完成时间
```

- MR总是选择`estimatedEndTime - backupEstimateEndTime`最大的任务启动备份任务
- 为了防止大量的任务同时启动备份任务，MR为每个作业设置了同时启动的备份任务数量上限

### 目录结构

- `bin`存放二进制文件
  - `hadoop`
  - `hdfs`
- `sbin`存放hadoop集群管理脚本
  - `hadoop-daemon.sh`
  - `slaves.sh`
  - `start-*.sh`
- `etc`存放配置文件
  - `*-env.sh`
  - `*.xml`
- `include`存放C语言头文件`.h`
- `lib`本地库，包括`.a`/`.so`文件。未来可以用来添加功能
- `libexec`类似于`lib`
- `share`存放文档、示例程序
- `data`数据文件夹。格式化NameNode时产生
- `logs`日志文件夹

## 运行模式

- 本地模式
  - 学习与测试
- 伪分布式
  - 在单机按照分布式的配置进行搭建
  - 学习与测试
- 完全分布式
  - 生产环境

## 配置

### 配置文件

配置文件有两类：默认配置文件、自定义配置文件

- 默认配置文件，在jar包里面，保存了所有配置的默认值
  - core-default.xml
  - hdfs-default.xml
  - yarn-default.xml
  - mapreduce-default.xml
- 自定义配置文件，用来覆盖默认配置。可以存在于集群上，也可以存在于客户端。客户端的配置优先级高于集群上的配置
  - core-site.xml
    - NameNode的主机名称和端口号
    - 数据存储路径
  - hdfs-site.xml
    - 副本数量
  - yarn-site.xml
    - NodeManager配置
    - ResourceManager配置
    - 日志和日志聚集
  - mapreduce-site.xml
    - 配置MapReduce在YARN上执行程序

### 启动与停止集群

- 各个组件逐一启动/停止
  - `hadoop-daemon.sh start/stop namenode/datanode/secondarynamenode`
  - `yarn-daemon.sh start/stop resourcemanager/nodemanager`
- 整个模块一起启动（需要配置节点之间的SSH免密登录）。**常用**
  - `start-dfs.sh/stop-dfs.sh`
  - `start-yarn.sh/stop-yarn.sh`
- 全部启动（官方不建议使用）
  - `start-all.sh/stop-all.sh`

### 常见流程

0. 配置集群时间同步。配置定时任务，周期性使用NTP同步时间
1. 配置`hadoop-env.sh`，需要配置`JAVA_HOME`
   1. 可以直接执行`echo $JAVA_HOME`获取当前环境变量
   2. 修改`hadoop-env.sh`文件，配置`JAVA_HOME`
2. 修改`core-site.xml`

```xml
<!-- 配置HDFS中NameNode的地址 -->
<property>
  <name>fs.defaultFS</name>
  <value>hdfs://xxxx</value>
</property>

<!-- 配置Hadoop运行时产生的临时文件的存储目录 -->
<property>
  <name>hadoop.tmp.dir</name>
  <value>/xxx/xxx</value>
</property>
```

3. 修改`hdfs-site.xml`（可选）

```xml
<!-- 指定HDFS副本数量 -->
<property>
  <name>dfs.replication</name>
  <value>3</value>
</property>

<!-- Secondary NameNode -->
<property>
  <name>dfs.namenode.secondary.http_address</name>
  <value>xxx</value>
</property>
```

4. 配置`yarn-env.sh`中的`JAVA_HOME`
5. 修改`yarn-site.xml`

```xml
<!-- 配置reducer获取数据的方式 -->
<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>mapreduce_shuffle</value>
</property>

<!-- 配置ResourceManager在哪个主机上 -->
<property>
  <name>yarn.resourcemanager.hostname</name>
  <value>xxx</value>
</property>
```

6. 配置`mapred-env.sh`中的`JAVA_HOME`
7.  修改`mapred-site.xml`

```xml
<!-- 声明MapReduce运行在YARN上 -->
<property>
  <name>mapreduce.framework.name</name>
  <value>yarn</value>
</property>

<!-- 历史服务器地址 -->
<property>
  <name>mapreduce.jobhistory.address</name>
  <value>xxx.com:10020</value>
</property>

<!-- 历史服务器web端地址 -->
<property>
  <name>mapreduce.jobhistory.webapp.address</name>
  <value>xxx.com:19888</value>
</property>
```

8. 修改`etc/slaves`，添加从节点，每一行都是一个数据节点的域名，注意不要有空格和空行
7. 把上述配置同步到集群的所有节点
9. 检查所有节点的NameNode
   1. 执行`jps`，确保NameNode/DataNode没有处于启动状态
   2. 删除`data`目录和`logs`目录
10. 格式化NameNode: `bin/hdfs namenode -format`
   1. 会生成一个集群ID
11. 启动HDFS：`sbin/start-dfs.sh`，包括`core-site.xml`/`hdfs-site.xml`/`slaves`中指定的所有DN/NN/2NN。此脚本相当于在不同节点执行如下脚本
    1. 在本机启动NameNode: `sbin/hadoop-daemon.sh start namenode`
    2. 在本机启动DataNode: `sbin/hadoop-daemon.sh start datanode`
       1. 启动后会生成和NameNode一样的集群ID
       2. 此时如果再次格式化NameNode，就会得到新的集群ID，DataNode和NameNode就无法正常通信
          1. 解决方法：格式化NameNode之前先删除DataNode里面的数据
12. 启动YARN：`sbin/start-yarn.sh`。此命令会在`yarn-site.xml`指定的节点上执行如下命令
    1.  在本机启动ResourceManager: `yarn-daemon.sh start resourcemanager`
    2.  在本机启动NodeManager: `yarn-daemon.sh start nodemanager`。每个节点都会有NodeManager
13. 启动jobhistory服务器: `mr-jobhistory-daemon.sh start historyserver`

## 命令

### hadoop命令

- `hadoop jar <mr-jar-file> <params>`执行MR程序
- `hadoop fs`操作HDFS
  - 相当于`hdfs dfs`命令。dfs是fs的实现类，即，`hadoop fs`其实是在调用`hdfs dfs`

### hadoop-daemon命令

- `hadoop-daemon.sh start namenode`启动NameNode
- `hadoop-daemon.sh start datanode`启动DataNode

### hdfs命令

- `hdfs dfs -help <command>`查看帮助
  - 例：`hdfs dfs -help ls`
- `hdfs dfs -<command>`执行类似于linux自带的文件系统命令
  - `hdfs dfs -mkdir -p /123/123/123`创建目录
  - `hdfs dfs -ls <dir>`列出目录
  - `hdfs dfs -cat /xxx/xxx`输出文件
  - 类似的命令：`chgrp`/`chmod`/`chown`/`mv`/`cp`/`tail`/`rm`
- `hdfs dfs -put <src> <dest>`把本地文件上传到HDFS（复制）
  - 同样效果的命令：`copyFromLocal`
- `hdfs dfs -get <src> <dst>`把HDFS文件下载到本地（复制）
  - 同样效果的命令：`copyToLocal`
- `hdfs dfs -moveFromLocal <src> <dst>`把本地文件上传到HDFS（剪切）
- `hdfs dfs -appendToFile <src> <dst>`把本地文件内容追加到HDFS中的文件
- `hdfs dfs -getmerge <...src> <dst>`合并下载文件到本地。可以用来合并日志
- `hdfs dfs -du <dir>`统计文件夹下每个dirent的大小
  - `-h`易读模式输出
  - `-s`输入指定文件夹的总大小，而不是每个dirent的大小
- `hdfs dfs -setrep <num> <file>`设置单个文件的副本数量
- `hdfs namenode -format`格式化NameNode，生成`data`文件夹
- `hdfs oiv -p <processor> -i <input-file> -o <output-file>`离线查看fsimage
  - processor可以是XML
- `hdfs oev -p <processor> -i <input-file> -o <output-file>`离线查看edits
- `hdfs dfsadmin -safemode <command>`管理安全模式
  - `get`查看安全模式
  - `enter`进入安全模式
  - `leave`离开安全模式
  - `wait`阻塞shell，直到退出安全模式。通常用来写sh脚本

### JAVA相关命令

- `jps`查看JAVA进程。用来查看Hadoop集群是否正常运行

## 优化

- MapReduce跑得慢
  - 硬件性能瓶颈
    - CPU/内存/磁盘/网络
  - IO操作优化
    - 数据倾斜
    - Map/Reduce任务数设置不合理
    - Map运行时间太长，Reduce长时间等待
    - 小文件太多
    - 大量不可分块的超大文件
    - 溢写次数过多
    - 归并排序次数过多
- 优化
  - 数据输入
    - 合并小文件
    - 使用CombineTextInputFormat合并小文件
  - Map
    - 减少溢写次数，从而减少IO
      - 调整`io.sort.mb` & `sort.spill.percent`，增大触发溢写的内存上限
    - 减少合并的次数
      - 调整`io.sort.factor`，添加merge的文件数量，从而减少合并次数
    - 如果不影响业务逻辑，可以使用Combiner
  - Reduce
    - 合理设置Map和Reduce数量
    - 设置Map/Reduce共存
      - 调整`slowstart.completedmaps`，使map任务运行到一定程度后，reduce就开始执行，减少reduce等待时间
    - 不使用Reduce阶段，完全使用Map处理业务逻辑
    - 合理设置Reduce的buffer，减少IO
      - 调整`mapred.job.reduce.input.buffer.percent`
  - IO
    - 数据压缩，使用Snappy & LZO
    - 使用SequenceFile二进制文件作为中间文件
  - 数据倾斜
    - 分类
      - 频率倾斜：比如中国的外卖次数比梵蒂冈的多，且长期存在
      - 大小倾斜：比如只有双十一那几天存在异常记录
    - 解决方案
      - 抽样和范围分区
        - 根据抽样的数据得到结果而不是所有数据
        - 对数据进行手动分区后再处理
      - 自定义分区
        - 把特殊的数据用特殊的方法处理。比如大的分区手动打散
      - 使用Combine，在map阶段处理
      - 使用map join而不是reduce join

常用调优参数：详见视频p191


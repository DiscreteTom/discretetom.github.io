---
title: Big Data(Part 7)
description: Kafka
---

## 前言

本文是观看[此视频](https://www.bilibili.com/video/BV1a4411B7V9)时的笔记

## Kafka

### 概述

- 分布式集群
- 基于pub/sub模式的消息队列(MQ)
- Scala语言开发
- 需要ZooKeeper作为集中配置
  - 保存集群的配置，比如每个topic的副本数，topic的leader在哪个broker等
  - Kafka版本低于0.9的时候，ZK还要保存消费者的进度(offset)，以便故障恢复
    - 0.9之后进度保存在Kafka集群中（作为一个拥有50个分片的topic），避免消费者维护和ZK的连接，并高频请求ZK
- 消息队列的两种模式
  - 点对点模式（一对一）
    - 消息保存在Queue中，消费者取出消息，消费，然后删除Queue中的消息
    - 一个消息只能被消费一次
  - 发布/订阅模式(pub/sub)
    - 生产者把消息发送到topic中，所有消费者都可以订阅消息
    - 消费者可以使用轮询的模式拉取(pull)消息，或者直接接收topic的推送(push)
  - Kafka是拉取型pub/sub MQ
- 消息默认保存在磁盘，默认保存7天
- 监控：可以使用开源的Eagle

### 架构

- 一个Kafka集群包含多个Broker
- 一个Kafka集群可以设置多个Topic，横跨多个Broker。每个Broker上面都有Topic的不同分区
  - 每个分区都至少被保存一份(leader)，可以在其他Broker上面保存副本(follower)
  - 生产者和消费者都会访问leader。follower只是做高可用，不做负载均衡
- 多个消费者构成消费者组
  - 一个消费者组负责消费某个topic的所有分区
  - 一个分区只能被同一个消费者组里面的一个消费者消费，以确保每个消息仅被一个消费者组消费一次
  - 无法实现全局有序（跨broker），但是可以实现broker内的数据有序

### 关键配置

```conf
# file: server.properties
broker.id=0 # 每个broker唯一的ID，整数
delete.topic.enable=true # 允许删除topic
log.dirs=/tmp/kafka-logs # Kafka暂存数据的目录
  # 不是日志目录，包含日志
  # 日志文件是server.log
  # 也包含各个consumer的offset（版本>0.9）
log.retention.hours=168 # 数据保存时间，不是日志
zookeeper.connect=localhost:2181 # ZK地址
```

- Kafka不被视为每个节点都保存数据（因为是分布式存储），所以把节点上的数据视为append only logs，所以数据目录也叫log
- 默认情况下日志和数据都保存在`log.dirs`目录下。也可以配置日志和数据分离，需要修改配置、删除数据并关闭、重新启动集群。具体操作可以自行搜索

### 原理

#### 文件存储

- 在数据目录下（某个topic的某个分区下），包含如下文件
  - `xxx.log`
    - 保存具体数据。只存数据不存边界
    - append only logs
    - 会根据配置文件里面的配置`log.segment.bytes`对数据进行分块(segment)
      - 第一个文件是`00000000000000000000.log`
      - 假设第一个文件里面有6条消息(0-5)，第二个文件就是`00000000000000000006.log`
  - `xxx.index`
    - 保存索引，用来定位`xxx.log`里面的数据
    - 也就是`第几个消息 => offset`
    - 每一个索引的大小都是固定的，这样就可以直接用索引的偏移量快速查索引
    - `000.index`对应`000.log`，`006.index`对应`006.log`
    - 假设需要查找第X个事件，则首先使用二分查找，定位到index文件，然后在index文件中直接根据索引偏移量，得到log文件中的偏移量
  - `xxx.timeindex`
  - `xxx.snapshot`
  - `leader-epoch-checkpoint`

#### 生产者

- 通过SDK写数据的时候，有三种分区策略
  - 直接使用分区ID（一个整数）指定分区
  - 使用分区键，根据策略（哈希）分区
  - 不指定分区，轮询分区写数据。第一个数据随机分区
- 副本同步策略，通常有两种策略
  - 半数副本完成同步就视为写入成功，返回ACK
    - 低延迟
    - 选举新的Leader的时候，容忍n台节点的故障，需要2n+1个副本（需要有n+1个副本存活，才能保障数据是正确的）
  - 全部副本完成同步才视为写入成功，返回ACK
    - 延迟高
    - 选举新的leader的时候，容忍n台节点的故障，需要n+1个副本（只要存在一个副本存活，数据就是正确的）
    - **Kafka使用了此方案**，因为方案1的数据冗余太多，而网络延迟并不是Kafka的主要瓶颈
      - 如果一个节点故障/延迟高，Kafka就永远不会返回ACK了？Kafka使用了ISR进行优化
- 生产者ACKS配置
  - `0`：生产者不等待broker的ACK，实现最低延迟。broker接收到数据后，还没写入磁盘就返回OK。broker故障时丢失数据
  - `1`：生产者仅等待leader的ACK。如果在第一个follower同步完毕之前leader出现故障，则丢失数据
  - `all/-1`：生产者等待leader和follower全部写入完毕后的ACK。如果follower同步完毕后，leader发送ACK之前，leader故障，则数据可能重复

#### 消费者

- consumer使用pull的方式从broker拉数据
  - push模式很难适应不同消费速度的消费者，而pull模式可以根据消费者自己的消费速度拉取数据
  - 如果kafka里面没有数据，消费者可能会一直收到空数据
    - kafka的优化方式是，使用长请求，即消费者的消费请求中会带有一个timeout参数，如果当前集群没有数据，消费者会等待一段时间然后再返回空数据。避免一直收到空数据
- 分区分配策略（哪个分区由消费者组中的哪个消费者消费）
  - RoundRobin
    - 假设有两个topic分别为t1, t2，它们都有3个分区，即t1-1, t1-2, t1-3和t2-1, t2-2, t2-3
    - 由两个消费者A和B消费（在一个消费者组中）
    - 这两个topic会被视为一个整体，即`(t1-1, t1-2, t1-3, t2-1, t2-2, t2-3)`，然后分配给A和B（分配之前可能会先shuffle一下打乱分区数据）
    - A得到`(t1-1, t1-3, t2-2)`，B得到`(t1-2, t2-1, t2-3)`
    - 所有消费者需要消费的分区数量差值最大为1
  - Range（默认）
    - 对于每个topic，不打乱分区，把topic里面的分区尽可能平均地分发给消费者
    - 假设有两个topic分别为t1, t2，它们都有3个分区，即t1-1, t1-2, t1-3和t2-1, t2-2, t2-3
    - 由两个消费者A和B消费（在一个消费者组中）
    - t1会把t1-1 & t1-2交给A消费，把t1-3交给B消费
    - t2会把t2-1 & t2-2交给A消费，把t2-3交给B消费
    - 可能会导致消费者负载不均衡。比如上述例子，A消费4个分区，B消费2个分区
  - 消费者数量变更（比如消费者挂了）的时候，会重新分配分区
- 维护offset
  - 以`(消费者组, 主题, 分区)`为主键，维护offset
    - 这样即使消费者组被重新分区，也不会重复读数据
  - 版本大于0.9时，在Kafka集群中维护offset
    - 保存在`__consumer_offsets`主题中
    - 可以修改消费者配置文件`consumer.properties`里面的`exclude.internal.topics=false`实现读取内置主题
    - 0.11版本之前，需要`--formatter "kafka.coordinator.GroupMetadataManager\$OffsetsMessageFormatter"`作为`kafka-console-consumer.sh`的参数，格式化数据
    - 0.11版本之后（含0.11），需要`--formatter "kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter`作为`kafka-console-consumer.sh`的参数，格式化数据
    - 命令：`kafka-console-consumer.sh --topic __consumer_offsets --zookeeper ip:port --formatter xxx --consumer.config config/consumer.properties --from-beginning`
      - 为什么要连接ZK而不是Kafka保存offset？如果连接Kafka直接读取offset，则消费者会在读取offset的时候同时写入offset导致数据循环。所以在查询Kafka里面的offset的时候要让消费者把自己的offset保存在ZK
  - 版本小于0.9时，offset保存在ZK
    - ZK里面除了`/zookeeper`之外的其他节点都是Kafka创建的，包括`cluster`, `controller`, `brokers`, `admin`, `consumers`等
      - `/controller`里面可以看到Kafka集群的controller信息。通常情况是第一个启动的broker。controller负责向ZK写数据，并和其他broker同步数据
      - 可以在ZK的`/brokers`中看到broker信息
      - 在`/consumers`中保存消费者组的信息，里面包括offset
  - 默认情况下，每秒提交一次offset。ZK里面的数据延迟可能更高。如果没有流数据，消费者可能会使用长轮询，可能5秒提交一次offset

#### ISR(In-Sync Replica set)同步副本集

- 如果一个follower长时间没有向leader同步数据，则把follower踢出ISR。
- 这个时间长度由`replica.lag.time.max.ms`参数设定。默认10秒
- leader故障后，会从ISR中重新选举Leader
- ISR和leader信息都保存在ZK
- 老版本(低于0.9)时，还会根据事件落后的条数来判断是否把follower提出ISR。后来发现每次写入的事件比较多，可能会导致follower频繁出入ISR，给ZK造成压力，所以废弃

#### 数据一致性问题

- 如果leader挂了，不同的follower保存的事件也不一致，如何解决
- Log文件中有两个指针
  - LEO(Log End Offset)
    - 当前副本保存的最后一个事件的offset
  - HW(High Watermark)
    - 所有ISR中的副本的最小LEO
- 只有HW之前的数据对consumer可见。所以consumer只能消费到所有副本中都有的数据（实现读取一致性）
- 新的Leader被选举出来之后，所有follower需要删除自己log中HW之后的数据（超前数据），然后从Leader同步数据（实现存储一致性）
- 此机制不会解决丢数据/重复数据的问题

#### Exactly Once

- 如果把服务器ACK设置为1，则保障不会丢数据，但是可能出现数据重复，即at least once
- 如果把服务器的ACK设置为0，则保证生产者的每条消息都只会发送一次，但是不保证数据不丢，即at most once
- 0.11版本之前，Kafka做不到exactly once。只能由消费者做全局去重，性能影响很大
- 0.11版本的Kafka引入了幂等性，用来实现exactly once
  - at least once + 幂等性 = exactly once
  - 需要配置`enable.idompotence`为`true`
  - kafka会自己做去重，而不是由消费者去重
  - producer在初始化的时候会获得一个PID(producer id)，发往同一个partition的消息会附带sequence number
  - broker会把`(PID, Partition, SeqNumber)`作为主键进行缓存，实现去重
  - 重启producer的时候PID会变化，不同的partition也会导致主键不同，所以幂等性无法实现跨分区、跨会话的exactly once

#### 为什么Kafka读写高效

首先Kafka是分布式，所以可扩展。

其次，Kafka即使是单机，也会比其他的消息队列的速度更快。原因如下

- 顺序读写
  - Kafka写数据到log文件是顺序读写，只需要append到文件末尾
  - 官方数据，同样的磁盘，顺序读写速度可以达到600M/s，而随机读写只有100K/s
  - 因为省去了大量的磁头寻址时间
- 零拷贝技术
  - 一般的应用程序，实现网络传输文件，流程如下
    - 用户空间应用程序调用内核空间文件接口，拿到数据（副本）
      - `File -> Page Cache -> Application Cache`
    - 用户空间应用程序调用内核空间网络接口，发送数据（副本）
      - `Application Cache -> Socket Cache -> NIC`
  - 而Kafka使用的零拷贝技术可以实现：
    - 在内核空间，把文件接口拿到的数据直接发给内核空间的网络接口，不走用户空间
      - `File -> Page Cache -> NIC`

#### ZK的作用

- Kafka集群中会有一个broker被选举为controller，负责管理集群broker的上下线、所有topic的分区副本的分配，以及leader的选举
  - 在ZK可以查到当前controller的broker id
- controller的管理工作都是依赖ZK的
- Partition的Leader选举过程如下
  - controller监听ZK里面的`/brokers/ids`
  - 一旦出现变化，则根据ISR，协调选举出新的leader
  - controller更新ISR和leader信息

#### 事务

- 0.11版本引入事务
- 可以保证在Exactly Once的基础上，生产/消费可以跨分区/会话，且具有原子性（要么全部成功，要么全部失败）
  - 比如生产者可能希望向3个分区分别写入一个数据，且要么全部写入成功，要么全部写入失败
- 生产者事务
  - 为了实现跨分区跨会话的事务，需要引入一个全局唯一的Transaction ID，并把生产者的PID（producer ID）和Transaction ID绑定
    - 生产者重启时根据Transaction ID获取PID
  - Kafka引入了新的组件Transaction Coordinator用来管理事务
    - 生产者通过和Transaction Coordinator交互获取Transaction ID对应的任务状态
    - Transaction Coordinator负责把事务写入Kafka内部的Topic，实现故障恢复
- 消费者事务
  - 消费者事务相对较弱
  - 无法保证commit信息被精确消费
  - 因为消费者可以根据offset访问任意信息

### 命令

- `bin/kafka-server-start.sh -daemon config/server.properties` & `bin/kafka-server-stop.sh config/server.properties`
  - 后台启动服务端/关闭服务端
  - 可以用`jps`查看kafka进程
- `bin/kafka-console-consumer.sh` & `bin/kafka-console-producer.sh`
  - 控制台消费者/生产者
  - `--topic xxx --broker-list ip:port`创建生产者
    - `broker-list`可以写任意一个broker，因为broker之间会复制数据。但是推荐写上所有broker，避免某个节点故障
  - `--topic xxx --zookeeper ip:port`创建基于ZK的消费者（老版本）
    - `--from-beginning`从头读取
    - `zookeeper`可以写任意一个节点的地址，但是推荐写上所有ZK节点，避免单点故障
  - `--topic xxx --bootstrap-server ip:port`创建基于Kafka的消费者（新版本）
- `bin/kafka-consumer-perf-test.sh` & `bin/kafka-producer-perf-test.sh`
  - 压力测试工具
- `bin/kafka-topics.sh`
  - Topic相关的命令
  - `--zookeeper xxx`指定ZK。下面的命令都需要指定ZK地址
  - `--list`查看所有topic
  - `--create --replication-factor 3 --partitions 1 --topic topic-name`创建topic
    - 副本数不能超过Broker数量
    - 副本数包含了leader
  - `--delete --topic xxx`删除topic
  - `--describe --topic xxx`描述topic，比如分区数量，副本数量
  - `--alter --topic xxx --partitions 6`修改分区数量

### API

#### 生产者API

- 消息发送流程
  - 异步发送，批量写入
  - 两个线程
    - `main`和`sender`
    - 两个线程共享一个变量：`RecordAccumulator`
    - `main`不断把消息发给`RecordAccumulator`
    - `sender`不断从`RecordAccumulator`拉取数据发送到broker
  - 流程
    - 生产者调用send方法
    - 数据交给拦截器(Interceptors)进行数据增强或过滤
      - 拦截器可以自定义
    - 数据交给序列化器(Serializer)进行序列化，变为字节数组
      - 序列化器可以自定义
    - 数据交给分区器(Partitioner)进行分区
      - 分区器可以自定义
    - 数据被交给RecordAccumulator（中的多个分区）
    - 被sender线程拉取，并发送给broker
  - 配置
    - `batch.size`数据积累到此值之后，sender发送数据
    - `linger.ms`数据超过此时间还未发送时，sender发送数据
- API
  - `send`
    - 支持传入回调函数，就可以得到事件的offset等信息，或者进行奕常处理
    - 支持根据参数使用不同分区方式。详见上文
    - 返回一个`Future`。如果选择使用`Future.get()`则可以实现同步发送而非异步
      - 同步发送可以实现确保消息有序。到那时通常可能会用其他消息队列实现这个功能

```java
Properties props = new Properties();
props.put("bootstrap.servers", "xxx"); // use literal string as key
// props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "xxx"); // use enum const as key
props.put("acks", "all");
props.put("retries", 1);
props.put("batch.size", 16384); // 16KB
props.put("linger.ms", 1);
props.put("buffer.memory", 33554432); // RecordAccumulator buffer size, 32MB
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

KafkaProducer<String, String> producer = new KafkaProducer<>(properties);
producer.send(new ProducerRecord<String, String>("topic-name", "xxx"));

producer.close(); // flush current buffer to sender
```

> 如果仅调用了send而没有close，可能会丢数据，因为数据在buffer中，没有被close给清理掉

#### 消费者API

- 流程
  - 批量读出
  - 仅保证分区内数据有序。不保证分区之间数据有序
- 提交offset
  - 支持自动提交，即刷新集群里面保存的消费者的offset
  - 也可以手动提交，避免消费者故障导致的数据读取丢失。处理完数据之后再手动提交
    - `commitSync`
      - 阻塞，自动重试
    - `commitAsync`
      - 非阻塞，不重试，可能失败。支持回调函数
  - 手动提交仍然无法解决数据重复的问题
    - 因为存在分布式事务。比如读取kafka里面的数据并保存在MySQL，如果MySQL已经更新而offset没有更新，此时进程挂掉，就会数据重复
    - 自定义存储offset，可能可以解决上述问题
      - 比如把offset保存在MySQL。更新MySQL和更新offset就可以在同一个MySQL事务中执行。MySQL的更新和offset的更新具有原子性，可以避免数据重复
      - 但是需要自行解决消费者rebalance的问题（即消费者数量变化时，如何更新所有消费者的offset）
        - 可以使用`ConsumerRebalanceListener`监听消费者变化
        - 主要是实现两个方法
          - `getOffset`根据分区获取offset
          - `commitOffset`提交当前消费者对应的所有分区的offset

```java
Properties props = new Properties();
props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "xxx");
props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true); // 开启自动提交（刷新offset）
props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, 1000); // 自动提交延迟
props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, "xxx"); // 反序列化类
props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, "xxx"); // 反序列化类
props.put(ConsumerConfig.GROUP_ID_CONFIG, "xxx"); // 消费者组
props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); // 是否重置offset
  // "earliest" 或 "latest"，默认latest

KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
consumer.subscribe(Arrays.asList("topic-name", "topic-name2")); // 可以订阅多个主题
  // 或者用 Collections.singletonList("topic-name") 如果只有一个主题

while (true) {
  ConsumerRecords<String, String> records = consumer.poll(100); // 接收数据。如果没有数据，长轮询100毫秒
}

consumer.close();
```

#### 自定义拦截器

- 通常用来增强数据（比如添加时间戳）/过滤数据/统计数据
- 继承`ProducerInterceptor`
  - 主要实现以下方法
    - `configure(configs)`
      - 获取配置信息和初始化数据
    - `onSend(ProducerRecord)`
      - 在消息被序列化、分区之前调用此方法
      - 最好不要修改消息所属的topic和分区，否则会影响目标分区的计算
    - `onAcknowledgement(RecordMetadata, Exception)`
      - 不要执行很重的逻辑，会影响生产者效率
    - `close`
      - 清理资源
- 拦截器可能被运行在多个线程。需要自行确保线程安全
- 如果指定了多个拦截器，则会按顺序被调用

#### 自定义分区器

- 序列化之后才进行分区，所以key是字节数组
- 可以根据key分区，也可以根据value
- 在producer config里面指定分区器：`props.put("partitioner.class", "xxx")`

```java
public class MyPartitioner implements Partitioner {
  @Override
  public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
    // 以下是常用的函数
    List<PartitionInfo> partitions = cluster.partitionsForTopic(topic); // 获取topic的分区信息。主要用来获取分区数量
    List<PartitionInfo> availablePartitions = cluster.availablePartitionsForTopic(topic); // 获取可用的分区信息
    Integer count = cluster.partitionCountForTopic(topic); // 获取分区数量

    return 0; // 返回分区ID
  }

  @Override
  public void close() {
  }

  @Override
  public void configure(Map<String, ?> configs) {
  }
}
```

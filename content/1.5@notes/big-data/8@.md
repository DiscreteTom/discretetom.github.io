---
title: Big Data(Part 8)
description: Spark
---

## 前言

本文是观看[此视频](https://www.bilibili.com/video/BV11A411L7CK)时的笔记

## Spark概述

- **基于内存**、快速、通用、可扩展的大数据分析计算引擎
- 使用Scala语言开发，**支持流处理**
- 解决MapReduce处理慢的问题，被认为是MapReduce框架的升级版
  - MapReduce关注一次性数据计算。读数据-计算-写数据，只提供了map-reduce两种操作，所以复杂的业务逻辑需要手动串联map-reduce操作。而单次map-reduce会产生大量的磁盘IO，导致效率低下
  - Spark提供了更多的计算模型，且基于内存，不同任务串联时使用内存保存数据，所以更方便更快
  - 如果生产环境内存不足，那么还是要用MapReduce基于硬盘做数据处理
  - 总结：Spark和MapReduce的根本差异是**多个作业之间的数据通信方式**。Spark基于内存，MapReduce基于磁盘
- 模块
  - 核心模块:
    - Spark Core
      - 最基础、最核心的功能。高层模块都是基于Spark Core实现的
  - 高层模块：
    - Spark SQL
      - 操作结构化数据
      - 使用SQL或者HQL（Hive版本的SQL）
    - Spark Streaming
      - 操作流式数据
    - Spark MLlib
      - 机器学习
      - 提供算法库、模型评估、数据导入等功能
    - Spark GraphX
      - 图形挖掘计算

## Spark Core

### Spark环境和运行模式

- Spark框架是一个运行的环境
  - 就像我们连接到MySQL然后执行SQL命令一样，我们需要连接到Spark环境，然后调用Spark API
  - Spark目前提供了Scala/Java/Python的SDK
  - 代码逻辑：
    - 建立连接
    - 提交/执行任务
    - 断开连接
- Spark被设计为可以在所有常见集群环境中执行
  - 比如本地、容器、Yarn等。国内主流环境为Yarn，容器正在逐渐变得流行
  - 本地模式(Local)
    - `/bin/spark-shell`可以启动一个scala shell，在里面可以写scala代码操作spark
      - 启动shell的时候会在本地启动一个spark服务端
      - shell里面已经存在一个sc对象(SparkContext)和一个spark对象（Spark会话）
      - 可以使用4040端口访问spark web管理界面
    - 也可以直接在用SDK写代码的时候把`master`设置为`local`或`local[*]`
  - Standalone模式（独立部署模式）
    - master/slave模式。比如3台机器，一台master负责管理，两台slave。3台机器都有worker可以跑任务
    - 和MR一样，Spark也可以配置历史任务服务器(event log)。略
    - 可以配置高可用，实现主备master。通常使用zookeeper配置
    - `master`中设置多个endpoint
  - Yarn模式
    - Spark主要负责计算而不是资源调度。可以使用Yarn实现资源调度
    - `master`需要设置为`yarn`
    - 需要启动YARN/HDFS
  - 容器模式，略

### 端口号

- Spark Local: 4040
- Spark Master内部通信：7077
- Standalone模式下SparkMaster:8080
- 历史服务：18080
- 使用YARN查看任务：8088

### SparkCore核心概念

- Executor
  - 是worker中的一个JVM进程
  - 提交app的时候可以提供参数指定executor数量、每个executor的内存大小、每个executor的CPU数量
- 并行度
  - 整个集群并行执行任务的数量
  - 并行和并发的区别：并行(parallelism)使用的是独立的CPU核心，并发(concurrency)可以复用单个CPU核心
- 有向无环图(DAG)

> 大数据计算引擎的迭代：第一代是MapReduce，仅提供map和reduce两种阶段，那么上层应用就必须把算法拆分为map/reduce，且需要多个job串联。难以迭代计算。
> 
> 因为这个弊端，催生了DAG框架，被视为第二代计算引擎。比如Tez和更上层的Oozie。但是它们仍然只能执行批处理任务。
> 
> 接下来就是以Spark为代表的第三代计算引擎，主要特点是job内部支持DAG(DAG不会跨job)，以及支持准实时计算。

### SparkCore架构

- 使用master-slave架构
- 客户端有一个driver program，里面使用spark context连接欸到spark集群的cluster manager
  - driver负责
    - 把用户程序转换为job
    - 在executor之间调度task
    - 跟踪executor的执行情况
- 每个worker node上面都有一个cache和一个executor
- executor负责执行多个task，并把结果返回给driver
  - executor自带block manager，把RDD缓存在executor进程的内存中实现加速计算
- 如果使用了Yarn进行调度，driver会请求Yarn Application Master，AM会请求Spark Master

### 核心编程

#### 流程

以使用Yarn调度为例

1. 启动Yarn环境（ResourceManager, NodeManager, ApplicationManager）
2. Spark通过申请资源来创建调度节点和计算节点
   1. 有些Node上面跑Driver(调度节点)
   2. 有些Node上面跑Executor(计算节点)
3. Driver根据需求，把计算逻辑根据分区划分成不同的任务，放到任务池(Task Pool)中
4. 调度节点根据计算节点的状态，从任务池中取任务，发送到计算节点进行计算

#### 数据结构

为了实现高并发/高吞吐的数据处理，Spark封装了三大数据结构：

- RDD(Resilient Distributed Dataset) - 弹性分布式数据集
  - 是Spark中最基础的数据处理模型
  - 封装计算逻辑，并不保存数据
  - 是一个抽象类
  - 弹性，不可变，可分区，内部元素可以并行计算的集合
    - 存储的弹性：内存/磁盘自动切换
    - 容错的弹性：数据丢失可以自动恢复
    - 计算的弹性：出错重试
    - 分片的弹性：根据需求重新分片，以便Executor被充分利用
    - 不可变：对RDD的操作只能生成新的RDD，不会修改原RDD
  - 可以被拆解为Task之后被序列化，发送给Executor。Task中包含数据和操作
  - 通过对RDD操作得到新的RDD，其实是在使用装饰者设计模式，给原RDD添加功能。直到执行collect，才会真正执行任务，之前都是在定义操作逻辑
  - 执行操作的时候，为了可以把任务发到不同的Executor，RDD会对数据进行分区
  - 五大核心属性
    - 分区列表(a list of partitions)
    - 分区计算函数(a function for computing each split)
    - RDD之间的依赖关系(a list of dependencies on other RDDs)
    - 分区器(optionally, a Partitioner for k-v RDDs)
    - 计算每个分区的首选位置(optionally, a list of preferred locations to compute each split on)
      - 尽量把计算任务放到保存文件的节点上，以免产生网络传输
  - 创建方式
    - 从内存创建，即利用已有的数组`Seq`创建。
      - `val rdd: RDD[Int] = sc.parallelize(seq)`
      - `val rdd: RDD[Int] = sc.makeRDD(seq)`
      - 可以使用参数`numSlices`指定分区数量。默认分区数量从SparkConf中获取，如果没有，则等于totalCores（当前环境最大可用核数）。Spark会自动根据数据长度，尽可能地进行均匀分片
    - 从外部存储创建（比如HDFS, HBase，本地文件系统）
      - `val rdd: RDD[String] = sc.textFile(path)`
        - 以行为单位读取数据
        - path可以是单个文件也可以是目录，支持通配符
        - 也可以是HDFS路径：`hdfs://xxx:xxx/xxx`
        - 使用`minPartitions`指定最小分区数量
        - 读取HDFS的文件，底层使用的是Hadoop
      - `sc.wholeTextFiles(path)`以整个文件为单位读取文件
    - 从其他RDD创建，即为已有的RDD添加操作
    - 直接创建(new)，通常由Spark框架自身使用
  - 两大类方法（算子）
    - 转换
      - 对RDD进行操作，生成新的RDD
      - 比如：`map/flatMap/reduce/groupBy/reduceByKey`
      - 分类
        - 单Value类型，操作单个RDD
          - `map`逐条转换数据
          - `mapValues`仅转换k-v里面的v，不改变k
          - `mapPartitions`逐个分区操作数据，性能比`map`高
            - `rdd.mapPartitions(_.map(_*2))`
            - 会把整个分区的数据加载到内存
            - 处理完的数据不会被释放掉
            - 如果数据量大，可能会内存溢出。`map`慢一些，但是不会溢出
          - `mapPartitionsWithIndex`逐个分区操作数据，并且可以得到分区的编号
          - `flatMap`扁平映射
          - `glom`把一个分区的数据转换为数组。分区不变
          - `groupBy`分组
            - 分组和分区没有必然的关系
            - 数据会根据分组，被发送到不同的分区。这个过程称为shuffle
            - shuffle比较消耗性能。shuffle越多，性能越差。所以一个优化程序的思路就是通过改变算子，减少shuffle
          - `filter`过滤。分区数量不变，但是可能数据倾斜
          - `sample`抽样
          - `distinct`去重
          - `coalesce`修改分区数量，提升效率
            - 通常用来减少分区
            - 如果目标分区数量大于原分区，需要把第二个参数shuffle设置为true
          - `repartition`扩大分区，提升效率。底层使用的就是`coalesce`
          - `sortBy`排序
        - 双Value类型，操作两个RDD
          - `intersection`交集
          - `union`并集
          - `subtract`差集
          - `zip`拉链
            - 合并相同index的值
            - 要求两个RDD的分区数量一致、分区内元素数量一致
        - Key-Value类型，RDD的数据类型是有两个元素的tuple
          - `partitionBy`根据传入的Partitioner进行分区。默认分区器是`HashPartitioner`
            - 如果新的分区器和之前一样，则不会重新分区
          - `reduceByKey`把相同的key的数据进行聚合
            - 如果某个key对应的元素只有一个，则结果中不会出现这个key。因为reduceByKey是成对运算的
            - 执行速度比`groupByKey`快，因为`reduceByKey`会先在当前分区进行操作（预聚合），然后再落盘（因为shuffle），所以落盘数据量会少一些
          - `groupByKey`根据key进行分组
            - 和`groupBy`的区别：`groupBy`的值中会包含`key`，而`groupByKey`的值不会包含`key`
            - `groupByKey`会导致shuffle把数据重组。Spark在处理shuffle的时候会落盘以避免OOM，所以`groupByKey`会比较慢
          - `aggregateByKey`分区内聚合，然后分区间聚合
            - `reduceByKey`的分区内聚合函数，与分区间聚合函数相同
            - `aggregateByKey`可以像`reduceByKey`一样实现分区内聚合，然后再使用不同的函数分区间聚合
            - 比如：求所有分区的最大值，然后求和
          - `foldByKey`对分区内和分区间使用相同的函数聚合
            - 相比于`reduceByKey`，`foldByKey`可以设置初始值
          - `combineByKey`先对每个key对应的第一个元素进行转换，然后再进行分区内、分区间聚合
          - `reduceByKey` vs `aggregateByKey` vs `foldByKey` vs `combineByKey`
            - 底层使用了相同的逻辑
            - 初始值的取值不同、分区内和分区间的函数不同，导致上述函数的不同
          - `join`连接相同key的value
            - 如果没有匹配的key，则结果中不会包含这个key
            - 如果一个RDD中出现重复的key，则每个item都会join一次。这会导致笛卡尔乘积的存在，导致数据量暴增，导致OOM
          - `leftOuterJoin` & `rightOuterJoin`允许结果存在None
          - `cogroup` = group + connect
            - 先分别把两个RDD中相同key的value进行group，然后把两个RDD相同的key对应的两个group给连接起来，变成`(key, list, list)`
            - 可以一次连接最多3个RDD
    - 行动
      - 触发作业的执行
      - 底层调用`sc.runJob`创建`ActiveJob`并执行作业
      - 包括
        - `reduce`直接获得reduce的结果
        - `collect`把数据按照分区顺序采集到driver的内存，得到数组
        - `count`获取数据个数
        - `first`获取第一个数据
        - `take`取指定数量的数据
        - `takeOrdered`排序，再取指定数量的数据
          - 支持自定义排序逻辑。默认数值升序
        - `aggregate`数据进行分区内聚合（+初始值），然后进行分区间的聚合（再+初始值）
          - 所以初始值一共被使用了【分区数量+1】次
        - `fold`类似`aggregate`，只不过分区内和分区间使用相同的聚合逻辑
        - `countByKey`根据Key，计算数量。RDD的数据需要是tuple list，根据k-v的Key计算
        - `countByValue`根据值，计算数量。RDD的数据需要是一个list。不是根据k-v的value进行计算
        - `saveAsTextFile` & `saveAsObjectFile` & `saveAsSequenceFile`保存为文件
        - `foreach`分布式执行。可能无序
  - 可序列化。RDD会被分发到不同的Executor进行处理，所以里面定义的处理逻辑（算子，函数）都是可序列化的
    - 闭包检测：Spark会检测算子的参数（函数）里面的对象是否可序列化。所有需要被executor使用的对象都可以序列化，才可以成功执行函数。而在driver端使用的对象可以不支持序列化
    - Java自带的序列化可以序列化任何类，但是比较重。Spark 2.0开始支持使用Kryo序列化框架进行序列化，速度是Java Serializable的10倍。
      - RDD在shuffle数据的时候，简单数据类型、数组、字符串默认使用Kryo进行序列化
      - Kryo会忽略Java的transient关键字，即使是transient变量也会被序列化
  - RDD的依赖
    - 如果两个RDD直接依赖，则就称为【依赖关系】。如果两个RDD之间间接依赖，则称为【血缘关系】
    - 每个RDD都会保存依赖/血缘关系。如果某个计算出现异常，就可以根据血缘进行重新计算
    - `RDD.toDebugString`可以看到血缘关系和shuffle
    - `RDD.dependencies`可以看到依赖关系
    - `OneToOneDependency/NarrowDependency`一个分区对应一个分区的依赖/窄依赖，比如`map`
      - 上游RDD的每个分区最多被一个下游RDD使用。独生子女
    - `ShuffleDependency`打乱重组依赖/宽依赖。比如`groupBy`
      - 上游RDD的每个分区被多个下游RDD使用
    - 窄依赖只需要在父依赖执行完毕后就可以直接向下执行。宽依赖需要等待所有父任务执行完毕才能继续执行，需要划分阶段
    - 阶段划分
      - 每个job的最后一个task是一个阶段(result stage)
      - 宽依赖会导致阶段的生成
      - 阶段用来进行任务调度
    - 任务划分
      - 核心概念
        - Application
          - 初始化一个SparkContext就会生成一个Application
        - Job
          - 一个Action算子就会生成一个Job
        - Stage
          - 等于宽依赖的个数+1(result stage)
        - Task
          - 一个Stage阶段中，最后一个RDD的分区个数就是此stage的Task的个数
        - Application-Job-Stage-Task每一层都是一对多的关系
      - 根据RDD生成DAG，然后根据DAG划分任务，最后交给worker执行
      - Stage和Task基本上名字的对应的，比如`ShuffleMapStage`对应`ShuffleMapTask`
  - RDD持久化/缓存/复用
    - 假设多个任务共享了步骤，如果我们仅仅是重用了RDD对象，并不能实现RDD复用，因为RDD不保存数据。操作还是执行了两遍
    - `RDD.cache()`可以缓存对象在内存。速度快但是可能不安全
    - `RDD.persist()`同上
    - `RDD.persist(StorageLevel.XXX)`持久化到某个存储中
      - `MEMORY_ONLY`仅内存。默认。内存不够了丢数据
      - `DISK_ONLY`仅硬盘，不需要指定文件名，保存为临时文件。Application结束后丢失。速度比内存慢
      - `DISK_ONLY_2`仅硬盘，备份2份
      - `MEMORY_AND_DISK`内存不够了写磁盘
    - 持久化操作必须在action算子执行的时候才会缓存
    - 也可以用来缓存一些时间很长的操作的结果
    - 通过在血缘关系中添加一个阶段的方式，使后面的操作优先从cache中取数据，而不是重新计算
  - RDD检查点(checkpoint)
    - 落盘，必须指定路径/文件名，Application结束后也不会丢失
    - 通常保存在HDFS上
    - 为了数据安全，通常会重新执行一遍作业
      - 所以通常和cache结合使用，先cache再checkpoint，就不会重新执行作业了
      - `RDD.cache()`
      - `RDD.checkpoint()`
    - checkpoint会直接修改血缘关系，把之前的所有依赖/血缘改为自身
  - checkpoint vs cache vs persist
    - cache是临时存储
    - persist比cache的数据更安全，但是不能跨作业共享
    - checkpoint永久保存数据，可以跨作业
  - 分区器
    - 内置哈希分区器、范围分区器、python分区器，支持自定义
    - 自定义分区器需要集成`Partitioner`
    - 使用`RDD.partitionBy(xxx)`来应用自定义分区
  - 文件保存和读取
    - 保存
      - `saveAsTextFile`
      - `saveAsObjectFile`
      - `saveAsSequenceFile`仅支持键值数据
    - 读取
      - `RDD.textFile` & `RDD.objectFile` & `RDD.sequenceFile`
- 累加器 - 分布式共享**只写**变量
  - 思想
    - 正常情况下，driver端的变量在传送给executor的时候，每个executor都会收到一个全新的副本。如果executor对变量进行了修改，driver也无法获得这些变量
    - 累加器可以把driver端定义的变量，在executor端进行修改之后，传回driver进行merge
    - 一些简单的操作，不需要在executor端进行（不需要分片、shuffle等操作），就可以传回driver进行合并
  - 示例
    - `sumAcc = sc.longAccumulator("some-name")`
    - `rdd.foreach(num => sumAcc.add(num))`
  - 系统自带的累加器类型
    - `longAccumulator`
    - `doubleAccumulator`
    - `collectAccumulator`集合类型
  - 支持自定义累加器
    - 继承`AccumulatorV2`，指定输入类型和输出类型
    - 需要实现如下方法
      - `add`在executor端添加新的值
      - `isZero`判断累加器是否为初始状态
      - `copy`复制累加器
      - `reset`重置累加器
      - `merge`在driver端合并多个累加器
      - `value`在driver端求值
- 广播变量 - 分布式共享**只读**变量
  - 思想
    - 一个Executor里面可能执行多个任务，但是这些任务可能会共享一些变量
    - 从【每个任务传输一次变量】改为【每个Executor传输一次变量】
    - 使用广播变量降低网络传输
  - 示例
    - `bc = sc.broadcast(xx)`
    - `bc.value` => `xx`

### WordCount

```scala
// init connection
val sparkConf = new SparkConf().setMaster("local").setAppName("WordCount")
val sc = new SparkContext(sparkConf)

// read data by line
val lines: RDD[String] = sc.textFile("data") // `data` is a folder

// split each line to words
val words: RDD[String] = lines.flatMap(_.split(" "))

// transform `word` to `(word, 1)`
val wordToOne = words.map (
  word => (word, 1)
)

// group by word
// `(word, 1)` to `(word, (word, 1))`
val wordGroup: RDD[(String, Iterable[(String, Int)])] = wordToOne.groupBy(
  t => t._1
)

// calculate word count
val wordToCount = wordGroup.map {
  case (word, list) => {
    list.reduce(
      (t1, t2) => {
        (t1._1, t1._2 + t2._2)
      }
    )
  }
}

// or, use `reduceByKey`
// val wordToCount = wordToOne.reduceByKey(_+_)

// collect result
val array: Array[(String, Int)] = wordToCount.collect()

// print result
array.foreach(println)

// close spark connection
sc.stop()
```

上述代码也可以简化为一行：

```scala
sc.textFile("data").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
```

### 命令

- `bin/spark-shell`进入交互式命令行
  - 使用scala语言
  - 自动提供`sc`对象（spark context）
- `bin/spark-submit <jar> <params>`提交应用
  - `--class xxx`应用程序的主类名，比如`org.apache.spark.examples.SparkPi`
  - `--master xxx`指定主节点
    - 本地模式：`local`（单线程）或者`local[*]`（所有核）
    - Standalone，指定集群master：`spark://<host>:<port>`。默认端口7077
    - Yarn模式，填写`Yarn`
  - `--executor-memory 1G`每个executor可用的内存
  - `--total-executor-cores 2`所有executor一共使用2个CPU线程
  - `--executor-cores 1`每个executor使用的CPU线程数量为1
  - `jar`是主类jar包所在位置
  - `params`是主程序参数

### 工程化代码

Java程序三层架构：

- Controller - 控制层
  - 接受请求，调度服务
  - Controller类中保存很多Service的实例
- Service - 服务层
  - 处理业务逻辑，和持久化层交换数据
- DAO - 持久化层
  - 访问数据库，维护数据对象模型

> 如果程序有界面（客户端程序/网页），则通常使用MVC三层架构（model/view/controller）。服务端程序通常使用上述三层架构

> 可以把代码组织到不同的文件夹中：controller/service/dao/util/common/bean。其中util指所有包都可能调用的东西，比如字符串处理函数。common是把一些复用的代码抽象为函数。bean放置一些实体类

以下为使用python写的工程化伪代码。**仅供参考**

```python
# file: util/EnvUtil
class EnvUtil:
  scLocal = threading.local()

  def put(k, v):
    scLocal[k] = v

  def take(k):
    return scLocal[k]
  
  def clear(k):
    del scLocal[k]

envUtil = EnvUtil()

# file: common/TApplication
from EnvUtil import envUtil
class TApplication:
  def start(f, name, master='local[*]'):
    sparkConf = SparkConf().setMaster(master).setAppName(name)
    sc = SparkContext(sparkConf)
    envUtil.put('sc', sc)

    f()

    sc.stop()
    envUtil.clear('sc')

# file: common/TController
class TController:
  def dispatch():
    pass

# file: common/TService
class TService:
  def dataAnalysis():
    pass

# file: common/TDao
from EnvUtil import envUtil
class TDao:
  def readFile(path):
    sc = envUtil.take('sc')
    sc.textFile(path)

# file: application/WordCountApplication
def f():
  controller = WordCountController()
  controller.dispatch()

class WordCountApplication(TApplication):
  pass

WordCountApplication().start(f, 'wc')

# file: controller/WordCountController
class WordCountController(TController):
  wcService = WordCountService()

  def dispatch():
    result = wcService.dataAnalysis()
    print(result)

# file: service/WordCountService
class WordCountService(TService):
  def dataAnalysis():
    ...

# file: dao/WordCountDao
class WordCountDao(TDao):
  pass
```

> ThreadLocal负责实现线程之间数据的隔离。它并不能解决线程安全问题，因为它的数据根本不跨线程，不涉及多个线程修改数据的情况。ThreadLocal的主要目的是单个线程的数据共享

### 源码分析（略

## SparkSQL

### SparkSQL概述

- 用来处理结构化数据
- 前身是Shark
- 提供了新的数据模型：DataFrame & DataSet
- 兼容Hive/HQL
- 可以直接连接到各种数据库，也可以被JDBC/ODBC连接
- 支持处理各种类型的数据
  - CSV/format/JSON/JDBC/text/parquet/orc/...
  - 以JSON为例，每一行都要是一个合法的JSON，而不是整个文档是一个JSON
- 使用`import spark.implicits._`引入spark隐式类型转换，比如字符串转数字
  - 此处的`spark`不是包名，而是`SparkSession`的成员
  - 这条语句通常出现在初始化`SparkSession`之后

### SparkSQL数据模型

- DataFrame
  - 概述
    - 以RDD为基础的分布式数据集
    - 相比于RDD，DataFrame自身就有schema/元数据信息，每一列都有名称和数据类型
    - 懒执行（触发时执行），但是性能比RDD高（因为可以进行各种底层过程优化）
    - 支持使用DSL(domain-specific language)
      - 直接使用`df`的方法来访问数据，从而避免创建临时视图之后用`spark.sql`执行SQL命令
  - 创建
    - 从Spark数据源创建
      - `spark.read.xxx`
      - 从文件读取的时候，数值类型会默认视为`bigint`，可以和`long`转换但是不能转换为`int`
    - 从RDD转换
      - `rdd.toDF('name')`RDD变DF
      - `df.rdd`DF变RDD
    - 从Hive查询
  - API
    - `df.show`查看DF内容
    - `df.createOrReplaceTempView('xxx')`/`df.createTempView('xxx')`创建临时视图
      - 视图只能查询不能修改（就像SQL一样）
      - 生命周期为session会话。在session外查不到。比如`spark.newSession.sql()`就查不到
    - `spark.sql('select * from xxx')`执行SQL
    - `df.createOrReplaceGlobalTempView('xxx')`/`df.createGlobalTempView('xxx')`创建全局临时视图
      - 使用`global_temp.xxx`访问视图
    - `df.printSchema`查看表结构
    - `df.select('xxx').show()`查询某列
      - `df.select($"xxx" + 1).show`查询某列+1。使用此语法时，每列都要使用`$`（Scala）
      - `df.select('xx + 1).show`查询某列+1。使用此语法时，每列都要使用`'`（Scala）
      - `df.select('xxx + 1 as "xxx2")`列的别名
    - `df.filter($"xxx" > 30)`条件过滤（Scala)
      - `df.filter(df.xx > 30)`python语法
      - `df.filter('xx > 30')`python语法
    - `df.groupBy`分组
- DataSet
  - 概述
    - 是Spark 1.6添加的新的抽象
    - 是DataFrame的扩展
      - DataFrame是DataSet的特例
      - `type DataFrame = DataSet[Row]`
      - 所以所有DataFrame的API，DataSet都支持
    - 把一行数据视为一个对象
    - 提供了RDD的优势（强类型，支持lambda函数）
    - 未来可能会逐步取代RDD和DataFrame成为唯一接口
  - 创建
    - 从List直接创建
      - `list.toDS`
    - 从DataFrame创建（给DataFrame赋予类型）
      - `df.as[SomeType]`
      - `ds.toDF`还原为DataFrame
    - 从RDD创建，RDD里面的元素必须是一个类的对象
      - `rdd.toDS`
      - `ds.rdd`转换为RDD
  - API
    - `ds.show()`查看数据

### SparkSQL环境对象

- 老版本中
  - 使用SQLConext对象进行SparkSQL查询
  - 使用HiveContext进行Hive查询
- 新版本
  - 使用SparkSesion进行查询
  - 是SQLContext和HiveContext的组合，兼容二者的API
  - 底层是SparkContext(`sc`)
  - 默认环境对象为`spark`
  - 构造环境：`SparkSession.builder().config(sparkConf).getOrCreate()`

### UDF

- SQL里面的自定义函数
- `spark.udf.register("name", f)`
- 定义UDAF（自定义聚合函数）（老版本）
  - 需要定义一个类，继承`UserDefinedAggregateFunction`（此类在新版本已经不推荐使用）
  - 实现如下成员
    - `inputSchema`，即输入数据的数据类型
    - `bufferSchema`，即中间结果的数据类型
    - `dataType`，即输出/最终结果的数据类型
    - `deterministic`，函数的稳定性，即输入相同的参数，输出是否相同
    - `initialize`，如何初始化缓冲区
    - `update`，如何更新缓冲区
    - `merge`，如何合并缓冲区
    - `evaluate`，如何（聚合）得到最终结果
  - 使用`spark.udf.register("name", f)`进行注册，然后就可以在`spark.sql`的SQL语句中调用
- 定义UDAF（from Spark 3.0）
  - 定义一个类，实现`Aggregator`类
    - Aggregator是模板类，可以指定输入/输出/缓冲区的类型，就可以使用强类型的特性来指定数据，而不用像UserDefinedAggregateFunction一样使用索引来指定数据
  - 重写方法
    - `zero`，初始化缓冲区
    - `reduce`，更新缓冲区
    - `merge`，合并缓冲区
    - `finish`，计算最终结果
    - `bufferEncoder`，如何编码缓冲区以便网络传输
      - 如果是自定义类，则使用`Encoders.product`
      - 如果是scala内置类（比如long），则使用`Encoders.scalaLong`
    - `outputEncoder`，如何编码结果以便网络传输
  - 使用`spark.udf.register("name", functions.udaf(new MyAggregator()))`的方式注册
  - 早期spark版本中也有Aggregator，但是不能用在SQL中，只能用在DSL中
    - `udafColumn = new MyAggregator().toColumn`
    - `ds.select(udafColumn)`
  - DataFrame/DataSet自带一些聚合函数，比如`count/countDistinct/avt/max/min`
  - Aggregator和UserDefinedAggregateFunction的最大区别是：前者是强类型，后者是弱类型

### 数据的读取和保存

- 默认文件格式为Parquet（读取和写入）。可以根据`spark.sql.sources.default`修改默认数据格式
  - Parquet是一种能够有效存储嵌套数据的列式存储格式
  - 读取JSON的时候，每一行都要是一个合法的JSON，而不是整个文档是一个JSON
    - JSON会被解析为`Dataset[Row]`
- `spark.read.load`是通用的数据加载方法。默认加载Parquet
  - `spark.read.format("json").load("path")`加载其他格式的数据
  - `spark.read.json("path")`
  - ``spark.sql("select * from json.`path"`").show()``直接在SQL语句中读数据
  - `spark.read.format("csv").option("sep", ";").option("inferSchema", "true").option("header", "true").load("path")`
- `df.write.save('path')`写文件。默认写Parquet
  - `df.write.format("json").save("path")`写JSON
  - `df.write.mode("append").json("path")`设置模式
    - `error`如果文件已经存在则抛出异常（默认）
    - `append`如果文件已经存在则追加
    - `overwrite`如果文件已经存在则覆盖
    - `ignore`如果文件已经存在则忽略
- 接入Hive
  - SparkSQL原生就支持Hive。也可以在编译SparkSQL的时候就加上Hive的支持。可以使用Hive的元数据
  - 使用内置的Hive（不连接到Hive服务）
    - `spark.sql("show tables").show()`查看Hive里面的元数据
      - 数据保存在`/opt/module/spark-local/metastore_db`目录下
    - 使用`df.createOrReplaceTempView`会创建临时元数据
    - `spark.sql("create table user(id int)")`创建表
    - `spark.sql("load data local inpath 'path' into table user")`把数据加载到表
      - 数据加载在`/opt/module/spark-local/spark-warehouse/user`目录下
  - 使用外部Hive服务
    - 需要把`hive-site.xml`放到spark的`conf`目录下
    - 需要把mysql驱动拷贝到`jars`目录下
    - 如果访问不到hdfs，需要把`core-site.xml` & `hdfs-site.xml`拷贝到`conf`目录下
  - 使用代码访问Hive
    - 把`hive-site.xml`拷贝到classpath下
    - 创建支持Hive的SparkSession: `spark = SparkSession.builder().enableHiveSupport().config(sparkConf).getOrCreate()`
    - 添加依赖，比如MySQL的驱动
  - 使用命令行工具`spark-sql`可以直接访问到Hive，执行各种SQL命令。类似于Hive的CLI
  - 使用`beeline`
    - Spark Thrift Server是Spark社区实现的一个Thrift服务，目标是无缝兼容HiveServer2
    - 部署Spark Thrift Server后，可以直接使用hive的beeline访问Spark Thrift Server执行操作。也就是说Spark Thrift Server可以视为Hive服务端
    - Spark Thrift Server的目标是取代HiveServer2。它可以和HDFS里面的Hive Metastore交互，获取元数据

## SparkStreaming

### SparkStreaming概述

- 处理准实时流式数据
  - 通常把毫秒级处理视为实时处理，把小时/天级别处理视为离线处理
  - Spark的架构决定了它只能实现批处理。SparkStreaming只是使用微批(micro-batch)的概念来加速处理，但是仍然无法实现毫秒级别，只能实现准实时处理，秒级/分钟级延迟
  - 真正的实时处理，可以使用Flink/Storm
- 多种数据源，比如Kafka/Flume/Twitter/MQ/socket
- 使用Spark的算子进行计算，比如map/reduce/join/window
- 数据模型为【离散化流(discretized stream, DStream)】
  - 可以把DStream视为很多的RDD。根据DStream根据时间间隔从源获取数据形成RDD并发送给SparkCore进行逻辑处理
- 背压机制(Spark Streaming Backpressure)
  - 根据JobScheduler反馈的作业信息，动态调整Receiver接收数据的速率
  - 默认不启用。需要修改`spark.streaming.backpressure.enabled`

### Stream Word Count

> 使用netcat工具向9999端口写入数据：`netcat -lp 9999`

```scala
// 初始化环境
val sparkConf = new SparkConf().setMaster("local[*]").setAppName("SparkStreaming")
val ssc = new StreamingContext(sparkConf, Seconds(3)) // 采集周期为3秒

// 设置数据源
val lines = ssc.socketTextStream("localhost", 9999)

// 定义WC逻辑
lines.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_).print()

// 开始任务并持续等待
ssc.start()
ssc.awaitTermination()

// 关闭环境
ssc.stop()
```

### DStream数据源

- `ssc.queueStream(new mutable.Queue[RDD[Int]](), oneAtTime = false)`
  - 从RDD Queue创建DStream
  - 通常用来调试
- `ssc.socketTextStream("localhost", 9999)`
  - socket数据源
- 自定义数据源
  - 继承`Receiver`类
    - `class MyReceiver extends Receiver[String](StorageLevel.MEMORY_ONLY)`
  - 重写方法
    - `onStart`
      - 创建一个新线程，死循环采集数据并使用`store`方法保存数据到流
    - `onStop`
      - 停止`onStart`里面的死循环
  - `ssc.receiverStream(new MyReceiver())`创建自定义数据源DStream
- Kafka数据源
  - 老版本
    - 使用ReceiverAPI，需要一个专门的Executor收集数据，然后发给其他Executor进行计算。通常会导致接收数据的Receiver内存溢出（因为收集数据比处理数据快）。目前版本已经无法使用此方式
  - 当前版本
    - 使用DirectAPI，由负责计算的Executor直接消费Kafka的数据，速率由Executor自身控制
    - `KafkaUtils.createDirectStream[String, String](ssc, LocationStrategies.PreferConsistent, ConsumerStrategy.Subscribe[String, String](Set("xxx"), kafkaPara))`
      - `createDirectStream[String, String]`是在分别指定Kafka中k-v的数据类型
      - `LocationStrategies.PreferConsistent`由引擎决定从哪个Broker读取数据
      - `ConsumerStrategy`指定消费者的详情
        - `Set("xxx")`指定Topic
        - `KafkaPara`一个保存Kafka连接信息的变量
      - 返回一个InputDStream

### DStream转换操作

- 类似于RDD，DStream也是定义一系列的转换操作，然后run程序
- 支持有状态转换。用来在微批之间传递状态
- 无状态操作
  - `transform`允许DStream执行任何RDD-to-RDD的转换
  - `join`两个DStream之间的join
    - 其实就是join两个微批的RDD
- 有状态操作
  - `UpdateStateByKey`根据key更新状态
    - 首先根据key对数据进行分组
    - 然后根据key得到已有的数据`opt`和当前微批中的数据`seq`
    - 合并`seq`和`opt`得到新的`opt`
    - 需要提前设置checkpoint以防状态数据丢失
- 窗口操作
  - 把多个满足条件的微批当作一个微批，并且支持滑动，使一个微批被多次引用
  - 窗口的范围必须是采集周期的整数倍
  - 默认每个采集周期进行一次计算（滑动距离/步长为一个采集周期）
  - `stream.window(Seconds(6), Seconds(6)).reduceByKey(_+_)`
    - 第一个Seconds表示窗口范围，第二个Seconds表示滑动距离（步长）
  - `countByWindow`返回滑动窗口中的元素个数
  - `reduceByWindow`整合窗口中的流元素，得到一个新的单元素流
  - `reduceByKeyAndWindow`根据key和windows整合元素
    - 有两个版本。一个版本只需要传入一个reduce函数，另一个版本可以传入两个reduce函数，后者用来优化window中重复数据被重复计算的问题

```scala
stream.updateStateByKey(
  (seq: Seq[Int], opt: Option[Int] => {
    Option(opt.getOrElse(0) + seq.num)
  })
)
```

### DStream输出操作

- Spark Streaming如果没有输出操作，会报错（视为丢失流数据）
- `print`输出数据和时间戳
- `saveAsTextFiles/ObjectFiles/HadoopFiles`保存文件
- `foreachRDD`执行任意RDD操作，默认不输出时间戳

### 优雅结束流任务

- 流任务是一直执行的分布式的任务。一个一个关闭任务太不优雅了，而且可能有数据不一致的问题
- `ssc.stop(true, true)`优雅关闭
  - 第一个true表示停止SparkContext
  - 第二个true表示启用优雅关闭，即计算节点不再接收新的数据，但是会把当前数据处理完毕
  - 通常在另一个线程中调用此函数。主线程仍然是`ssc.start(); ssc.awaitTermination();`
  - 通常由一个外部存储保存状态（比如数据库、redis等）。线程会轮询，或者被事件触发，从而优雅关闭流任务
  - 停止之前通常会使用`ssc.getState()`获取当前状态，防止关闭已经被关闭的流

```scala
ssc.start()

new Thread(
  new Runnable {
    override def run(): Unit = {
      while (true) {
        if (someCondition) {
          val state = ssc.getState()
          if (state == StreamingContextState.ACTIVE) {
            ssc.stop(true, true)
            System.exit(0)
          }
        }
        Thread.sleep(5000)
      }
    }
  }
)

ssc.awaitTermination()
```

### 恢复数据

- 优雅结束之后，需要根据检查点恢复数据并重新启动流
- 关键是在`StreamingContext.getActiveOrCreate`的时候，传入检查点路径

```scala
ssc = StreamingContext.getActiveOrCreate("checkpoint-path", () => {
  // create ssc
  val sparkConf = new SparkConf().setMaster("xxx").setAppName("xxx")
  val ssc = new StreamingContext(sparkConf, Seconds(3))

  // define operations
  ssc.socketTextStream("localhost", 9999).map((_, 1)).print()

  ssc
})

ssc.checkpoint("checkpoint-path")

ssc.start()

// graceful stop
new Thread(...) 

ssc.awaitTermination()
```

### 示例流处理场景

- 把短时间点击某个广告超过100次的用户放入黑名单
  - 屏蔽恶意广告点击
- 实时统计某些数据，比如广告点击总流量

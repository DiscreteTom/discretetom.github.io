---
title: Big Data(Part 3)
description: Hive
tags:
  - Hadoop
---

## 前言

本文是观看[此视频](https://www.bilibili.com/video/BV1EZ4y1G7iL)时的笔记

## Hive

### 概述

- 基于Hadoop的数据仓库
- 使用HQL（Hive风格的SQL）进行操作
- 本质是把HQL转换为MapReduce程序，数据存在HDFS，调度用Yarn。所以Hive可以视为Hadoop的一个客户端，不需要集群
  - 把SQL语言中常见的select/where/insert/group等操作映射到MapReduce代码
  - 计算引擎是可以修改的，不一定是MapReduce，也可以是Spark、Tez之类的。在Hive 2中MR已经过时，建议使用Spark/tez
- 适合一次写入、多次查询的业务负载
- 最初由Facebook开源，用来解决海量结构化日志统计
- 优点
  - 类似SQL语法，简单容易上手，开发快速
  - 开发人员不需要学习MapReduce，降低学习成本
  - 底层是Hadoop，所以底层参数可以被调整，从而优化性能
  - 支持自定义函数
- 缺点
  - 基于MapReduce，执行延迟高，不适合实时任务和小数据量
  - HQL表达能力有限
    - 无法使用HQL表达迭代式算法（循环）
    - 不擅长数据挖掘，无法用MapReduce实现更高性能的算法
  - 效率低
    - 自动生成的MapReduce作业不够智能化
    - 从Hive角度很难调优，还是要基于Hadoop调优
- 日志默认保存在`/tmp/<username>`中。可以修改`hive-log4j2.properties`文件中的`hive.log.dir`来修改保存路径，比如`/opt/module/hive/logs`

### 架构

- Meta Store
  - 保存元数据，比如表名和HDFS文件的映射、创建的表(create table)的信息
    - 元数据会缓存表的行数、文件数等信息，从而加速`select count(*)`之类的查询
      - 使用`insert`之类的语句会刷新这些缓存
    - 默认映射关系：`tableName`在HDFS中的存储路径为`hdfs://xxx:xxx/user/hive/warehouse/tableName`
      - 这个路径是一个文件夹，Hive会访问文件夹下的所有文件
      - 所以`select * from tableName`会查询此文件夹下的所有文件。所以写数据可以不通过Hive，可以直接put到HDFS里面
  - 默认使用Apache Derby数据库（轻量，只支持单用户）。通常用改用MySQL
    - Derby数据库会在当前目录创建数据文件。如果在不同的目录启动Derby，是可以启动多个Derby的，但是这些进程不共享数据，所以还是单用户
  - metastore服务端
    - `bin/hive --service metastore`启动metastore服务器
      - 服务器是前台应用。可以使用`honup hive --service metastore 2>&1 &`把app放到后台
    - 修改配置文件里面的`hive.metastore.uris`以应用此metastore服务。值为`thrift://xxx:xxx`
  - 需要使用`bin/schematool -dbType mysql -initSchema`初始化元数据
- Client
  - Hive CLI
    - 默认情况是交互式CLI
    - `-e`通过命令行传入SQL命令
    - `-f`传入SQL文件
    - `quit/exit`推出交互式CLI
    - 可以执行HDFS命令，比如`dfs fs /;`
    - 命令历史保存在`~/.hivehistory`
  - JDBC
    - 需要启动`hiveserver2`和`metastore`服务
      - 修改配置文件
        - `hive.server2.thrift.bind.host`
        - `hive.server2.thrift.port`
      - 启动hiveserver2服务：`bin/hive --service hiveserver2`或`bin/hiveserver2`
      - 启动metastore服务见上文
    - java代码使用Hive JDBC连接到hiveserver2，hiveserver2连接到metastore，metastore使用MySQL JDBC连接到MySQL获取元数据
    - 也可以使用Hive JDBC CLI: `bin/beeline -u jdbc:hive2://xx.xxx -n <username>`
      - 用户名是系统用户名
      - 启动比较慢，建议等hiveserver2准备好了再操作
- SQL Parser（解析器）
  - 解析SQL
- Physical Plan（编译器）
  - 把解析后的SQL解释为MR任务
- Query Optimizer（优化器）
  - 优化SQL
- Execution（执行器）
  - 执行MR任务

### 数据仓库对比关系型数据库

- 二者通常都使用类SQL语言
- 数据仓库数据量更大（比如Hive的数据存在分布式的HDFS里面），不建议经常更新数据，适合查询。而关系型数据库比较注重小数据量、实时、ACID读写、在线业务
- 数据仓库延迟较高。适合大数据量、并行任务

### 配置

- `hive-default.xml`服务端默认配置文件。保存了默认值
- `hive-site.xml`服务端配置文件。通常修改此文件来修改服务端配置
- 命令行参数：`bin/hive --hiveconf k=v`。单个会话生效
- 在客户端执行`set;`可以查看配置和环境变量，执行`set k=v;`可以修改配置，仅在单个会话生效

### 数据类型

#### 基本数据类型

| Hive数据类型 | Java数据类型 | 细节 |
| --- | --- | --- |
| TINYINT | byte | 1字节有符号整数 |
| SMALINT | short | 2字节有符号整数 |
| **INT** | int | 4字节有符号整数 |
| **BIGINT** | long | 8字节有符号整数 |
| BOOLEAN | boolean | true/false |
| FLOAT | float | 单精度浮点 |
| **DOUBLE** | double | 双精度浮点 |
| **STRING** | string | 字符串。可以指定字符集。使用单引号或双引号 |
| TIMESTAMP | - | 时间 |
| BINARY | - | 二进制字节数组 |

hive的string相当于普通数据库的varchar，是可变字符串，但是不能声明最多可以保存多少字符。理论上最多2GB

#### 集合数据类型

- STRUCT
  - 类似C-like语言的struct，成员可以通过`.`来访问
  - 语法：`struct<street:string, city:string>`
- MAP
  - 键值对，使用`['key']`访问成员
  - 语法：`map<string, int>`
- ARRAY
  - 数组，起始编号为0
  - 语法：`array<string>`

集合数据类型支持嵌套

示例建表语句：

```sql
create table xxx (
	name string,
	friends array<string>,
	children map<string, int>, -- name=>age
	address struct<street:string, city:string>
)
row format delimited
	fields terminated by ','
	collection items terminated by '_' -- struct/map/array的分隔符
	map keys terminated by ':' -- map中k/v的分隔符
	lines terminated by '\n';
```

示例数据：

```csv
DiscreteTom,friend1_friend2,child1:18_child2:19,BUPT_Beijing
```

查询数据：

```sql
select children['child1'], address.street from xxx;
```

#### 类型转换

基本数据类型支持隐式转换，类似于java里面的类型转换。转换条件是不能丢失信息量，比如TINYINT可以转换为INT，但是INT不能隐式转换为TINYINT，除非使用CAST进行显式转换

- 所有整数类型都可以隐式转换为范围更广的整数类型
- 整数类型、FLOAT、**STRING**都可以隐式转换为DOUBLE
  - 如果STRING里面的内容正好是小数格式，比如`'123.456'`则可以转换为DOUBLE
- TINYINT/SMALLINT/INT可以转换为FLOAT
- BOOLEAN不能转换

强制类型转换：`cast(xxx as int)`

例：`select '1'+2, cast('1' as int) + 2;`，输出是`3.0 3`，即第一个结果是DOUBLE，第二个是INT

### 操作

#### DDL

##### 数据库级别

创建数据库：

```sql
create database [if not exists] <database name>
[COMMENT <comment>]
[LOCATION <hdfs path>]
[WITH DBPROPERTIES (k=v,...)];
```

- `show <DDL>`来查看某个语句实际执行的时候，默认参数是什么样子的，比如`show create table xxx;`
- `show databases;`查看数据库
- `use <database>;`切换数据库
- `show databases like 'xxx*';`查看名字符合条件的数据库
- `desc database <name>;`查看数据库信息
- `desc database extended <name>;`查看数据库详细信息
- `alter database xxx set dbproperties(k=v);`修改数据库
- `drop database [if exists] xxx;`删除数据库
  - 如果数据库不为空（存在表），可以使用`drop database xxx cascade;`进行强制删除。HDFS里面的文件也会被删除

##### 表级别

创建表：

```sql
create [external] table [if not exists] <table name>
[(<col name> <data type> [comment <comment>], ...)]
[comment <comment>]
[partitioned by (<col name> <data type> [comment <comment>], ...)] -- 分区
[clustered by (<col name>, ...)] -- 分桶
[sorted by (<col name> [ASC|DESC], ...) into <num> buckets] -- 分桶
[row format <row format>]
[stored as <file format>] -- 压缩格式。默认不压缩
[location <hdfs path>]
[tblproperties (<k=v, ...>)]
[as <select statement>]
```

- 内部表vs外部表
  - 内部表（管理表，managed），由hive控制数据生命周期。删除一个内部表时，hive除了会删除元数据，还会删除HDFS中的数据
  - 外部表：hive认为自己不拥有数据。删除表时仅删除元数据，不会删除HDFS中的数据
  - 修改表：`alter table xxx set tblproperties('EXTERNAL'='TRUE|FALSE');`
  - 查询表类型：`desc formatted xxx;`
- 重命名表：`alter table <name> rename to <name>;`
- 更新列：`alter table <name> change [column] <old name> <new name> <type> [comment <comment>] [first|after <col name>]`
- 增加和替换列：`alter table <name> add|replace columns (<name> <type> [commen <comment>], ...);`
  - ADD表示添加字段，在所有列后面（partition列前）
  - REPLACE表示替换表中所有字段

#### DML

##### 导入数据

- 导入已存在的表：`load data [local] inpath <path> [overwrite] into table <name> [partition (k=v, ...)];`
  - 如果使用了`local`则把本地数据加载到Hive表。否则从hdfs加载数据
    - 如果是从本地加载数据，则数据会被**复制**到HDFS中。使用HDFS API
    - 如果从hdfs加载数据，数据会被**移动**到目标位置，而不是复制。只需要修改HDFS的NameNode元数据，很快
  - 如果使用了`overwrite`则删除原表内数据。否则，追加数据
  - 相比于使用`hdfs put`的方法上传数据，使用`load data`会刷新原数据里面的文件数等信息
    - 但是不会刷新表的行数
    - 使用`insert`可以同时刷新文件数和行数
- 查询插入：`insert into|overwrite table <name> select xx, xx from xxx where xxx;`
  - `insert into`插入数据
  - `insert overwrite`覆盖数据
  - 可以插入回原表。使用此方式更新整个表可能比update语句更合适
- 分区插入：

```sql
from xxx
insert overwrite table xxx partition(k=v)
select xxx where condition1
insert overwrite table xxx partition(k=v)
select xxx where condition2;
```

- 建表并从查询加载数据：`create table if not exists xxx as select xxx from xxx;`
- 建表并从HDFS加载数据`create external table xx (xxx xxx) location '/xxx';`
- 使用import命令导入之前使用export命令导出的数据：`import table xxx from '/path';`

##### 导出数据

- 可以直接下载HDFS里面的文件
- 把查询结果保存到本地目录`insert overwrite [local] directory '/path' select xx from xxx;`
- export: `export table xxx to '/path';`
  - 使用export导出的数据会包含元数据
  - 主要用于Hive/Hadoop迁移
- CLI + 输出重定向
- 清空表：`truncate table xxx;`
  - 只能删除内部表，无法操作外部表
- Sqoop，用来把数据导出到MySQL，详见下文

##### 查询数据

```sql
select [all|distinct] xxx, select xxx, ...
from xxx
[where xxx]
[group by xxx]
[order by xxx]
[cluster by xxx | [distribute by xxx] [sort by xxx]]
[limit xxx];
```

和普通的SQL类似。略

### 分区

- Hive的分区就是在HDFS上分目录保存数据
  - 目录名就是分区的k/v值。所以用来分区的field不在文件中，而是文件名
  - 并行任务
  - 避免全表扫描
  - 分区信息会保存在元数据中
- 创建分区表：在`create table`的时候使用`partitioned by(<field name> <type>)`
- 向分区加载数据：`load data inpath '/path' into table xxx partition(k=v);`
  - 如果不指定partition，则partition的值为`__HIVE_DEFAULT_PARTITION__`
- 查询时，`where`条件中如果以分区键为条件，就会自动使用分区查询
- 查询时，会根据元数据中的分区进行查询。元数据中没有的分区不会被查询
  - `msck repair table xxx`根据HDFS中的实际文件来修复分区信息
- 添加分区：`alter table xxx add partition(k=v) partition(k=v) ... ;`
  - 注意没有逗号
- 删除分区：`alter table xxx drop partition(k=v), partition(k=v), ... ;`
  - 注意有逗号
- 查看分区：`show partitions <table name>;`
- 二级分区：有两个分区字段。嵌套分区
  - `create table xxx (xxx) partitioned by (xxx xxx, xxx xxx);`
  - `load data inpath 'xxx' into table xxx partition(k=v, k=v);`
- 动态分区：自动根据数据字段把数据放到对应的分区
  - 启用动态分区：`hive.exec.dynamic.partition=true`。默认开启
  - 动态分区模式：`hive.exec.dynamic.partition.mode=nonstrict|strict`
    - 默认为strict，表示必须至少指定一个分区为静态分区
    - nonstrict表示所有分区都可以是动态分区
  - 使用数据文件中已有的字段进行分区
  - 注意：建表的时候，表的fields里面是没有目标字段的，目标字段仅出现在`partitioned by`语句中
  - 其他配置
    - `hive.exec.max.dynamic.partitions=1000`所有节点上一共最多可以有多少动态分区。默认1000
    - `hive.exec.max.dynamic.partitions.pernode=100`每个执行MR的节点最大可以创建多少动态分区。默认100。需要根据业务逻辑进行设置。比如分区键是一年中的天，那么分区数量就要设置大于366
    - `hive.exec.max.created.files=100000`整个MR任务可以创建多少个HDFS文件。默认100000
    - `hive.error.on.empty.partition=false`空分区被创建时是否抛出异常。默认false

### 分桶

- 对于一个表或者一个分区，可以把数据拆分为文件，从而更细粒度地组织数据
  - 分区针对数据路径，分桶针对数据文件
- `create table xxx (xxx) clustered by (xxx) into x buckets`
  - 创建分桶表时，`clustered by`中的属性必须是表的一个字段。而分区表的`partitioned by`字段中的属性必须不属于表的字段
  - 数据会哈希取模，然后分散在桶中
- reduce的个数建议设置为-1，这样reducer的个数会等于桶的个数。或者设置大于桶的个数
- 抽样查询
  - `select xxx from xxx tablesample (bucket x out of y on id);`
    - 根据id把数据集分为y等份，取第x份到第y份。x不大于y
  - 适用于数据量非常大的场景

### 函数

- 内置函数
  - 查看函数：`show functions`
  - 解释函数：`desc function xxx`
  - 详细解释函数：`desc function extended xxx`
  - 常用内置函数
    - `nvl(xxx, yyy)`如果xxx的值为NULL则替换为yyy
      - yyy可以是字段、常量、NULL
    - `case when then else end`
      - 例：`select sum(case sex when 'male' then 1 else 0 end) as maleCount from xxx;`
    - `if (condition, true value, false value)`
      - 例：`select sum(if(sex='male', 1, 0)) as maleCount`
    - `concat(xxx, yyy, ...)`拼接字符串
    - `concat_ws(sep, xxx, yyy, ...)`拼接字符串，第一个参数为分隔符
      - 支持接收array作为参数
    - `collect_set(xxx)`把字段进行去重后汇总为一个array（行转列，多行变一行）
      - 可能和`concat_ws`一起使用，比如`concat_ws(',', collect_set(xxx))`
    - `explode(xxx)`炸裂。把复杂的array/map拆分为多行（列转行，一行变多行）
    - `lateral view`侧写。如果要保留炸裂前的关系，则需要使用lateral view
      - 如：`select xxx, zzz from ttt lateral ivew explode(yyy) kkk as zzz;`
    - 窗口函数`over`
      - `over()`根据`group by`设置窗口，计算每个`group by`得到的分组的数据
      - 比如：`select name, count(*) over () from xxx group by name;`，其中计算`count(*)`的时候会应用一个窗口，窗口大小为group的大小
      - `over (partition by xxx, yyy, ...)`会根据某个字段设置窗口
      - `over (partition by xxx order by xxx rows between unbounded preceding and current row)`指定窗口范围
        - `current row`当前行
        - `n preceding`前n行
        - `n following`后n行
        - `unbounded`起点
          - `unbounded preceding`从起点
          - `unbounded following`到终点
        - `lag(col, n, default)`往前第n行
        - `lead(col, n, default)`往后第n行
        - `ntile(n)`
    - `rank`排名。出现重复时，总数不变
      - 比如：`1,2,2,4`
    - `dense_rank`排名。出现重复时，总数减少
      - 比如：`1,2,2,3`
    - `row_number`根据顺序排名，不会重复，但是相同排名可能乱序
      - 比如：`1,2,3,4`
- 自定义函数
  - 分类：
    - UDF(User Defined Function)
      - 一行输入一行输出
      - 比如
        - upper
    - UDAF(User Defined Aggregate Function)
      - 多行输入一行输出
      - 比如
        - max
        - min
        - count
    - UDTF(User Defined Table Function)
      - 一行输入多行输出
      - 比如
        - explode
  - 需要编程实现，集成Hive提供的Java类，实现抽象方法即可

### 压缩和存储

- 执行过程中使用MapReduce的压缩。在MR里面配置。略
- 最终输出结果可以配置压缩
  - `hive.exec.compress.output=true`。默认值是false，以便查看输出

配置最终压缩过程（Hive客户端）：

1. 启动hive压缩：`set hive.exec.compress.output=true;`
2. 启动MR最终输出压缩：`set mapreduce.output.fileoutputformat.compress=true;`
3. 设置MR输出压缩格式：`set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;`
4. 设置MR压缩为块压缩：`set mapreduce.output.fileoutputformat.compress.type=BLOCK;`

### 文件存储格式

#### 行式存储和列式存储

假设有一个表格：

| name | age |
| --- | --- |
| aaa | 1 |
| bbb | 2 |
| ccc | 3 |

- 基于行存储：`aaa,1,bbb,2,ccc,3`
- 基于列存储：`aaa,bbb,ccc,1,2,3`

由于类SQL通常以列（字段）进行查询，所以生产环境通常使用列存储作为存储格式，性能更高

#### 文件格式

- `TEXTFILE`（默认）
  - 文本
  - 基于行存储
- `SEQUENCEFILE`
  - 二进制
  - 基于行存储
- `ORC`
  - Optimized Row Columnar
  - 由Hive 0.11引入的存储格式
  - 基于列存储
  - 先把整个数据按行分段(stripe)，然后对端进行列式存储
  - 每个stripe有自己的索引信息，以便加速查询
- `PARQUET`
  - 基于列存储，二进制，文件包含数据和元数据，可以自解析
  - 行组(Row Group)
    - 类似于ORC的Stripe，把整个数据分成若干行组，每个行组包含若干行。一个HDFS文件中至少保存一个行组
  - 列块(Column Chunk)
    - 单个行组中的每一列都保存在一个列块中
    - 行组的所有列，连续地存储在这个行组文件中
    - 一个列块中的所有值的类型相同
    - 不同的列块可以使用不同压缩算法
  - 页(Page)：每个列块分为若干页
    - 页是Parquet最小编码单位
    - 同一个列块的不同页可能使用不同编码方式

使用场景：

- Hive环境下，不同存储格式的查询效率差不多，ORC存储效率高，所以通常使用ORC
- Spark环境默认使用Parquet格式文件，对Parquet格式有优化，性能更高

不同的存储格式可以和压缩方案结合，实现更高的性能

### 优化

- 执行计划：查看优化后的查询语句
  - 语法：`explain [extended | dependency | authorization] xxx`
- Fetch抓取
  - 某些查询可能不走MR任务比走MR任务更快
  - `hive.fetch.task.conversion=more`
    - 老版本是`minimal`
    - 设置为`more`后，全局查找、字段查找、limit查找等任务不走MR
- 本地模式
  - 数据量小的时候，在单台机器上面执行所有任务，可能更快（因为不需要网络IO）
  - `hive.exec.mode.local.auto=true`启动自动本地模式
  - `hive.exec.mode.local.auto.inputbytes.max=134217728`启动本地模式的最大数据量，默认128M
  - `hive.exec.mode.local.auto.input.files.max=4`启动本地模式的最大文件数。默认4.
  - 如果单个节点性能优秀，那么增加上述上限可以提升任务执行效率
- 表的优化，略，详见视频（偷懒）